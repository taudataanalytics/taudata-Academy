{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"black\">https://bit.ly/wfh-2021-taudata</font></center>\n",
    "    \n",
    "<img alt=\"\" src=\"images/IDBigData/Cover_idBigData-2021.jpg\" /> \n",
    "    \n",
    "## <center><font color=\"blue\">tau-data Indonesia</font><br>(C) Taufik Sutanto - 2021</center>\n",
    "<center><a href=\"https://tau-data.id\">https://tau-data.id</a> ~ <a href=\"mailto:taufik@tau-data.id\">taufik@tau-data.id</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Tak Kenal, Maka Tak Sayang</font></center>\n",
    "\n",
    "<center><img src=\"images/bio-about/bio_TS.png\" /></center>\n",
    "\n",
    "<font color=\"green\">“*I am not what you see. I am what time and effort and interaction slowly unveil*.” ― Richelle E. Goodrich</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">WFH 2021: Pengenalan Natural Language Processing (NLP) ~ Sentiment Analysis</font></center>\n",
    "\n",
    "* Pendahuluan NLP & Textmining\n",
    "* **Basic Document Representation**\n",
    " - Vector Space Model: tf-idf ~ BM25\n",
    " - normalization, Stopwords, & n-gram\n",
    " - *Discussion*: on Strength & weakness of VSM and-or tf-idf\n",
    "* **Simple Text Preprocessing**\n",
    " - lemma, Slang & Abbreviation\n",
    "* **Unsupervised Sentiment Analysis: Lexicon Based**\n",
    " - *Discussion*: Strength & weakness, Error & Model Analysis + Interpretation\n",
    "* **Supervised Approach VSM + SVM**\n",
    " - *Discussion*: Strength & weakness, Error & Model Analysis + Interpretation\n",
    "* **Penutup**\n",
    " - Deep Learning & Improvements\n",
    "\n",
    "<font color=\"green\">\"*I always have a basic plot outline, but I like to leave some things to be decided while I write*.\" ~ J. K. Rowling</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Asumsi Workshop WFH 2021 - IDBigData</font></center>\n",
    "\n",
    "* <font color=\"green\">**Tantangan**</font>: \n",
    " - Peserta dari berbagai latar belakang keilmuan dan tingkat kemampuan.\n",
    " - Hanya 3 jam (termasuk diskusi & rehat)\n",
    " - Daring (online)\n",
    "* <font color=\"green\">**Asumsi & Strategi**</font>:\n",
    " - Peserta diasumsikan mengenal sedikit tentang Python.\n",
    " - Hanya menggunakan kasus teks bahasa Indonesia\n",
    " - Fokus ke **Aplikasi Dasar** NLP/Textmining Sentimen Analisis.\n",
    " - **Code** diberikan/tunjukkan hanya sebagai demonstrasi (bukan kompetensi utama yang didapatkan dari kegiatan ini).\n",
    "\n",
    "<center><img src=\"images/vector-icon/quiz-requirement.jpeg\" width=\"399\" height=\"249\" /></center>\n",
    "\n",
    "><font color=\"green\">\"*Minimum requirements & Maximum adjustments are two steps for Happy & Successful Life*\" ~ Maulik Jadav</font>\n",
    "\n",
    "image source: https://medium.com/@nipunithisarangi/level-up-your-requirement-gathering-game-cb1c42e9ffbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Referensi & Resources:</font></center>\n",
    "\n",
    "### <font color=\"green\">Python:</font>\n",
    "* Pengenalan Python: https://tau-data.id/adsp/ & https://tau-data.id/hpds/\n",
    "* Python basic: https://www.python-course.eu/python3_history_and_philosophy.php \n",
    "* Data Science Basic: https://tau-data.id/dsbd/ & https://scikit-learn.org/stable/tutorial/index.html\n",
    "* Advanced Python: http://andy.terrel.us/blog/2012/09/27/starting-with-python/\n",
    "* Visualisasi di Python: https://matplotlib.org/gallery.html\n",
    "\n",
    "<img alt=\"\" src=\"images/tau-data_banner_large.jpg\" style=\"width: 600px;\" />\n",
    "\n",
    "### <font color=\"green\">NLP~Text Mining:</font>\n",
    "* https://tau-data.id/nlptm/ , https://tau-data.id/sma/ https://tau-data.id/sna/\n",
    "* https://github.com/ailabtelkom/id-NLP-resources\n",
    "* https://github.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia\n",
    "* https://github.com/fajri91/InSet\n",
    "* https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Instruksi Workshop:</font></center>\n",
    "\n",
    "* **Side-by-side**: Atur jendela antar muka layar berdampingan.\n",
    "* **Expand Sections**: Di Google Colab, klik \"View\", lalu pilih \"Expand Sections\"\n",
    "* **Shortcuts**: Selama Workshop berlangsung, tekan tombol \"ctrl+Enter\" untuk menjalankan cell.\n",
    "* **Run Anyway**: Saat pertama kali menjalankan cell akan ada warning box pop-up, silahkan pilih \"Run Anyway\"\n",
    "* **Left Panel**: Disebelah kiri Google Colab, silahkan tekan \"icon directory\", di panel ini kita dapat unduh atau unggah file/folder.\n",
    "* **Terurut (Sequential)**: Semua cell harus dijalankan terurut dari atas ke bawah, tanpa ada cell yang terlewati.\n",
    "\n",
    "<center><img src=\"images/side-by-side.jpg\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk, warnings; warnings.simplefilter('ignore')\n",
    "import logging; logging.captureWarnings(True)\n",
    "# Pilih \"Run Anyway\" di Pop-Up dialog yang muncul jika cell ini dijalankan di Google Colab\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.dic\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/s-pos.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/s-neg.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/s-negasi.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/ind_SA.csv\n",
    "    !pip install python-crfsuite unidecode textblob sklearn-pycrfsuite sastrawi gensim\n",
    "    nltk.download('popular')\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Importing Modules untuk Notebook ini\n",
    "import re, matplotlib.pyplot as plt, pandas as pd, seaborn as sns, json\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "random_state = 99\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Apakah Perbedaan antara NLP dan Text Mining (TM)?</font></center>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/1_jarvis.jpg\" style=\"height: 450px; width: 600px;\" /></p>\n",
    "\n",
    "<p><a href=\"https://www.turn-on.de/lifestyle/topliste/zehn-film-gadgets-die-wir-uns-im-wahren-leben-wuenschen-4413\" target=\"_blank\"><strong>[Image Source]: https://www.turn-on.de/primetime/topliste/zehn-film-gadgets-die-wir-uns-im-wahren-leben-wuenschen-4413</strong></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">NLP dan Text Mining</font></center>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/1_Text_Analytics.jpg\" style=\"height: 451px; width: 600px;\" /></p>\n",
    "\n",
    "<p>[Image Source: <a href=\"http://www.pearson.com.au/products/S-Z-Turban-Sharda/Business-Intelligence-and-Analytics-Systems-for-Decision-Support-Global-Edition/9781292009209?R=9781292009209\" target=\"_blank\">Efraim T. &quot;Business Intelligence and Analytics: Systems for Decision Support, Global Edition (10e)</a>&quot;]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Aplikasi NLP dan Text Mining</font></center>\n",
    "\n",
    "* Sentiment Analysis\n",
    "* Speech Recognition dan Classification\n",
    "* Machine Translation (Misal&nbsp;https://translate.google.com/ )\n",
    "* Information Retrieval (IR)&nbsp;(misal www.google.com, bing, elasticsearch, etc.)\n",
    "* Man-Machine Interface (misal Chatbot, Siri, cortana, atau Alexa)\n",
    "* Named Entity Recognition (NER)\n",
    "* Word Sense Disambiguation\n",
    "* Topic Modelling, dsb\n",
    "\n",
    "<p><img alt=\"\" src=\"images/nlp-textmining-applications.jpg\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Sentimen Analysis</font></center>\n",
    "\n",
    "<p><strong>Apakah sentiment analysis?</strong></p>\n",
    "\n",
    "* Sentiment analysis, also called opinion mining, is the field of study that analyzes people’s opinions, sentiments, appraisals, attitudes, and emotions toward entities and their attributes expressed in written text [Bing Liu 2014].\n",
    "\n",
    "<p>Terkadang disebut juga sebagai&nbsp;<strong>opinion mining.</strong> (walau technically sebenarnya berbeda)</p>\n",
    "\n",
    "For proper definition see:\n",
    "* Liu, B., 2015. Sentiment analysis: Mining opinions, sentiments, and emotions. Cambridge University Press.\n",
    "\n",
    "<p><img style=\"undefined: undefined;\" src=\"images/sentiment-analysis.jpg\" width=\"589\" height=\"231\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Aplikasi Sentiment analysis</font></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/sentimen_analysis_interface.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/9_SA_techniques.jpg\" style=\"height:300px; width:536px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Preprocessing</font></center>\n",
    "\n",
    "* Data Media Sosial sangat \"noisy\".\n",
    "* Sama seperti data terstruktur: **Garbage-in~Garbage-out**\n",
    "* Sebagian besar waktu akan habis untuk preprocessing.\n",
    "* Di workshop ini hanya disinggung sekilas sebagian proses preprocessing data teks. Silahkan akses resources di cell sebelum ini untuk detail lebih jauh.\n",
    "\n",
    "<img alt=\"\" src=\"images/Data-preprocessing-steps-for-extractive-text-summarization.png\" style=\"height:400px; width:487px\" />\n",
    "\n",
    "[<a href=\"https://www.researchgate.net/publication/317610956_Evaluation_of_Unsupervised_Learning_based_Extractive_Text_Summarization_Technique_for_Large_Scale_Review_and_Feedback_Data/figures?lo=1&utm_source=google&utm_medium=organic\" target=\"_blank\"><strong>Image Source</strong></a>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Text Case Normalization (Huruf BESAR/kecil)</font></center>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menganalisa makna (<em>semantic</em>) dari suatu (frase) kata dan mencari informasi dalam proses textmining, seringnya (*) kita tidak membutuhkan informasi huruf besar/kecil dari kata&nbsp;tersebut.</li>\n",
    "\t<li><em>Text case normalization</em> dapat dilakukan pada string secara efisien.</li>\n",
    "\t<li>Namun, bergantung pada analisa teks yang akan digunakan pengguna harus berhati-hati dengan urutan proses (pipelining) dalam preprocessing. Mengapa dan apa contohnya?</li>\n",
    "</ul>\n",
    "\n",
    "<p>(*) Coba temukan minimal 2 pengecualian dimana&nbsp; huruf kapital/kecil (case) mempengaruhi makna/pemrosesan teks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore case (huruf besar/kecil)\n",
    "T = \"Permisi Mas, kalau mau ikut workshop WFH-2021 daftarnya kemana ya? ...@@... \"\n",
    "print(T.lower())\n",
    "print(T.upper())\n",
    "# Perintah ini sangat efisien karena hanya merubah satu bit di setiap (awal) bytes dari setiap karakter\n",
    "# Sehingga tetap efisien jika ingin dilakukan sebelum tokenisasi dan-atau pada data yang besar sekalipun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Tokenisasi</font></center>\n",
    "\n",
    "<p>Tokenisasi adalah pemisahan kata, simbol, frase, dan entitas penting lainnya (yang disebut sebagai token) dari sebuah teks untuk kemudian di analisa lebih lanjut. Token dalam NLP sering dimaknai dengan &quot;sebuah kata&quot;, walau tokenisasi juga bisa dilakukan ke kalimat, paragraf, atau entitas penting lainnya (misal suatu pola string DNA di Bioinformatika).</p>\n",
    "\n",
    "<p><strong>Mengapa perlu tokenisasi?</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Langkah penting dalam preprocessing, menghindari kompleksitas mengolah langsung pada string asal.</li>\n",
    "\t<li>Menghindari masalah (semantic) saat pemrosesan model-model natural language.</li>\n",
    "\t<li>Suatu tahapan sistematis dalam merubah unstructured (text) data ke bentuk terstruktur yang lebih mudah di olah.</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images\\2_Pipeline_Tokenization.png\" style=\"height:300px; width:768px\" /><br />\n",
    "[<a href=\"https://www.softwareadvice.com/resources/what-is-text-analytics/\" target=\"_blank\"><strong>Image Source</strong></a>]: https://www.softwareadvice.com/resources/what-is-text-analytics/</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Tokenisasi Berbagai Bahasa</font></center>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/2_Tokenization_Complexity.jpg\" style=\"height:500px; width:686px\" /><br />\n",
    "[<a href=\"http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contoh Tokenisasi dengan Module <font color=\"blue\"> TextBlob</font>\n",
    "\n",
    "<p>*Blob : Binary large Object</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizing di TextBlob\n",
    "T = \"Permisi Mas, kalau mau ikut workshop WFH-2021 daftarnya kemana ya? ...@@... \"\n",
    "print(TextBlob(T).words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = TextBlob(T).words\n",
    "z[0]=='Permisi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">NLP Bahasa Indonesia</font></center>\n",
    "\n",
    "<p>NLTK belum support Bahasa Indonesia, bahkan module NLP Python yang support bahasa Indonesia secara umum masih sangat langka. Beberapa <u><strong>resources </strong></u>yang dapat digunakan:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li><strong><a href=\"https://github.com/kirralabs/indonesian-NLP-resources\" target=\"_blank\">KirraLabs</a></strong>: Mix of NLP-TextMining resources</li>\n",
    "\t<li><strong><a href=\"https://pypi.python.org/pypi/Sastrawi/1.0.1\" target=\"_blank\">Sastrawi 1.0.1</a>:</strong>&nbsp;untuk &quot;stemming&quot; &amp;&nbsp;<strong><a href=\"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\" target=\"_blank\">stopwords&nbsp;</a></strong>bahasa Indonesia.</li>\n",
    "\t<li><strong><a href=\"http://stop-words-list-bahasa-indonesia.blogspot.co.id/2012/09/daftar-kata-dasar-bahasa-indonesia.html\" target=\"_blank\">Daftar Kata Dasar Indonesia</a></strong>:&nbsp;Bisa di load sebagai dictionary di Python</li>\n",
    "\t<li><strong><a href=\"https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata\" target=\"_blank\">Wiktionary</a></strong>: ProyekWiki bahasa Indonesia [termasuk Lexicon]</li>\n",
    "\t<li><a href=\"http://wn-msa.sourceforge.net/\" target=\"_blank\"><strong>WordNet Bahasa Indonesia</strong></a>: Bisa di load&nbsp;sebagai dictionary (atau NLTK<em>*</em>) di Python.</li>\n",
    "\t<li><strong><a href=\"http://kakakpintar.com/daftar-kata-baku-dan-tidak-baku-a-z-dalam-bahasa-indonesia/\" target=\"_blank\">Daftar Kata Baku-Tidak Baku</a></strong>: Bisa di load sebagai dictionary di Python.</li>\n",
    "\t<li><strong><a href=\"https://spacy.io/\" target=\"_blank\">Spacy</a></strong>: Cepat/efisien, MIT License, tapi language model Indonesia masih terbatas.</li>\n",
    "\t<li><a href=\"http://ufal.mff.cuni.cz/udpipe\" target=\"_blank\"><strong>UdPipe</strong></a>: Online request &amp; restricted license (support berbagai bahasa -&nbsp;pemrograman).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Morphological-Linguistic Normalization: Stemming & Lemmatization</font></center>\n",
    "\n",
    "### (Canonical Representation)\n",
    "<img alt=\"\" src=\"images/meme-cartoon/2_yoda.jpg\" style=\"height:400px; width:400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Stemming dan Lemma</font>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan konteks kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung konteks.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
    "\t</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Mengapa melakukan Stemming &amp; Lemmatisasi</strong>?</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Sering digunakan di IR (Information Retrieval) agar ketika seseorang mencari kata tertentu, maka seluruh kata yang terkait juga diikutsertakan.<br />\n",
    "\tMisal:&nbsp;<em>organize</em>,&nbsp;<em>organizes</em>, and&nbsp;<em>organizing&nbsp;</em>&nbsp;dan&nbsp;<em>democracy</em>,&nbsp;<em>democratic</em>, and&nbsp;<em>democratization</em>.</li>\n",
    "\t<li>Di Text Mining Stemming dan Lemmatisasi akan mengurangi dimensi (mengurangi variasi morphologi), yang terkadang akan meningkatkan akurasi.</li>\n",
    "\t<li>Tapi di IR efeknya malah berkebalikan: <strong><font color=\"blue\">meningkatkan recall, tapi menurunkan akurasi&nbsp;</font></strong>[<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\"><strong>Link</strong></a>]. Contoh: kata&nbsp;<em>operate, operating, operates, operation, operative, operatives, dan operational</em>&nbsp;jika di stem menjadi <em>operate</em>, maka ketika seseorang mencari &quot;<em>operating system</em>&quot;, maka entry seperti&nbsp;<em>operational and research</em> dan&nbsp;<em>operative and dentistry</em>&nbsp;akan muncul sebagai entry dengan relevansi yang cukup tinggi.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lemmatizer dengan Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "print(stemmer.stem(\"Test1: perayaan itu Berbarengan dengan saat kita bepergian ke Depok\"))\n",
    "print(stemmer.stem(\"Test2: Perayaan, Bepergian, Menyuarakan\"))\n",
    "# Amati dan analisa hasilnya dengan seksama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Text Level Normalization: StopWords</font></center>\n",
    "\n",
    "<p><u>Di Text Mining</u> kata-kata yang <strong>sering muncul </strong>(dan jarang sekali muncul) memiliki sedikit sekali informasi (signifikansi) terhadap model (machine learning) yang digunakan. Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan klasifikasi) atau di semua cluster (di permasalahan pengelompokan/clustering). Kata-kata yang sering muncul ini biasa disebut &quot;StopWords&quot;. Stopwords berbeda-beda bergantung dari Bahasa dan Environment (aplikasi)-nya.<br />\n",
    "<strong>Contoh</strong>:<br />\n",
    "\n",
    "<ul>\n",
    "\t<li>Stopwords bahasa Inggris: am, is, are, do, the, of, etc.</li>\n",
    "\t<li>Stopwords bahasa Indonesia: adalah, dengan, yang, di, ke, dsb</li>\n",
    "\t<li>Stopwords twitter: RT, ...<br />\n",
    "\t<img alt=\"\" src=\"images/2_StopWords.png\" style=\"height:250px; width:419px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "stop_en = stopwords.words('english')\n",
    "stop_id = StopWordRemoverFactory().get_stop_words()\n",
    "\n",
    "print(stop_en[:10]); print(stop_id[:10])\n",
    "print(len(stop_id), len(stop_en))\n",
    "stop_en = set(stop_en); stop_id = set(stop_id) # Tips: selalu rubah list stopwords ke bentuk set, karena di Python jauh lebih cepat untuk cek existence di set ketimbang list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "'tidak' in stop_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Contoh cara menggunakan stopwords\n",
    "T = \"untuk kamu yang disana, sudah makan belum?\"\n",
    "Tokens = TextBlob(T.lower()).words # Tokenisasi \n",
    "' '.join([t for t in Tokens if t not in stop_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Menangani Slang atau Singkatan di Data Teks</font></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Cara me-load daftar singkatan/slang\n",
    "with open('data/slang.dic') as f:\n",
    "    slang = json.load(f)\n",
    "\n",
    "str(slang)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "slang['7an']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Contoh Penggunaan\n",
    "T = 'jangan ragu gan, langsung saja di order pajangan yg diatas. sesok sdh mehong!'\n",
    "T = TextBlob(T).words\n",
    "\n",
    "for i, t in enumerate(T):\n",
    "    if t in slang.keys():\n",
    "        T[i] = slang[t]\n",
    "        \n",
    "print(' '.join(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 'jangan ragu gan, langsung saja di order pajangan yg diatas. sesok sdh mehong!'\n",
    "\n",
    "T.replace(\"gan\", \"juragan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Encoding-Decoding</font></center> \n",
    "\n",
    "<ul>\n",
    "\t<li>Hal berikutnya yang perlu diperhatikan dalam memproses data teks adalah encoding-decoding.</li>\n",
    "\t<li>Contoh Encoding: ASCII, utf, latin, dsb.</li>\n",
    "\t<li>saya membahas lebih jauh tetang encoding disini:&nbsp;<br />\n",
    "\t<a href=\"https://tau-data.id/memahami-string-python/\" target=\"_blank\">https://tau-data.id/memahami-string-python/</a></li>\n",
    "\t<li>Berikut adalah sebuah contoh sederhana tantangan proses encoding-decoding ketika kita hendak memproses data yang berasal dari internet atau media sosial.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# kita bisa menggunakan modul unidecode untuk mendapatkan representasi ASCII terdekat\n",
    "from unidecode import unidecode\n",
    "\n",
    "T = \"ḊḕḀṙ ₲ØĐ, p̾l̾e̾a̾s̾e ḧḕḶṖ ṁḕ\"\n",
    "print(unidecode(T).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Kita juga bisa membersihkan posting media sosial/website dengan entitas html menggunakan fungsi \"unescape\" di modul \"html\"\n",
    "from html import unescape\n",
    "\n",
    "print(unescape('Satu &lt; Tiga&nbsp;&amp; &#169; adalah simbol Copyright'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Loading Data</font></center>\n",
    "\n",
    "* Menggunakan Data Sentimen Twitter dari: https://github.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia\n",
    "* Silahkan kunjungi tautan diatas untuk mengakses dataset sentimen lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/ind_SA.csv\")\n",
    "print(\"Banyak tweet di data = {} tweet\".format(data.shape[0]))\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Di python index dimulai dari \"0\"\n",
    "print('Tweet pertama = \"{}\"'.format(data.Tweet.loc[0]))\n",
    "data = data.sample(3000) # Agar komputasi tidak terlalu lama, kita sample datanya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Eksplorasi Data</font></center> \n",
    "\n",
    "* Preprocessing sederhana\n",
    "* Memisahkan yang positif dan negatif\n",
    "* Simpan dalam bentuk Teks\n",
    "* Visualisasi & Eksplorasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot = data.sentimen.value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cleanTxt(t, lemma=True, stopword=False, slang_=True):\n",
    "    T = unescape(unidecode(str(t).lower()))\n",
    "    if lemma:\n",
    "        T = stemmer.stem(t) # ingat struktur data sastrawi\n",
    "    T = TextBlob(T).words # Tokenisasi \n",
    "    if stopword: #butuh \"stop_id\" sudah di load terlebih dahulu\n",
    "        T = [t for t in T if t not in stop_id]\n",
    "    if slang_: #butuh \"slang\" sudah di load terlebih dahulu\n",
    "        for i, t in enumerate(T):\n",
    "            if t in slang.keys():\n",
    "                T[i] = slang[t]\n",
    "    return ' '.join(T) # output string lagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Test Fungsi CleanText sederhana\n",
    "t = \"utk kamu yg disana, sdh makan belum?\"\n",
    "cleanTxt(t)\n",
    "# Analisa hasilnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing ke seluruh Tweet dan simpan dalam kolom yang baru\n",
    "data['dataCleaned'] = [cleanTxt(d.Tweet) for i,d in tqdm(data.iterrows())]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pos = [d.dataCleaned for i,d in data.iterrows() if d.sentimen==1]\n",
    "neg = [d.dataCleaned for i,d in data.iterrows() if d.sentimen==-1]\n",
    "pos[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def simpan(Teks, namafile): \n",
    "    with open(namafile, 'w') as f:\n",
    "        for t in Teks:\n",
    "            f.write(t+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "simpan(pos, 'tweet-positif.txt')\n",
    "simpan(neg, 'tweet-negatif.txt')\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Text Analytics - Voyant Tools</font></center> \n",
    "\n",
    "### https://voyant-tools.org/\n",
    "\n",
    "* WordCloud, Word Links, Word Tree\n",
    "\n",
    "<img alt=\"\" src=\"images/11_voyant.png\" style=\"height:118px; width:426px\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Unsupervised Sentiment Analysis</font></center> \n",
    "\n",
    "Algoritma:\n",
    "1. Load Lexicon Positif dan Negatif\n",
    "2. Hitung jumlah kata positif dan negatif di data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Fungsi Load Lexicon\n",
    "def loadLexicon(file):\n",
    "    df=open(file,\"r\",encoding=\"utf-8\", errors='replace')\n",
    "    data=df.readlines();df.close()\n",
    "    return [d.strip().lower() for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fpos, fneg, fnegasi = 'data/s-pos.txt', 'data/s-neg.txt', 'data/s-negasi.txt'\n",
    "positif, negatif, negasi = loadLexicon(fpos), loadLexicon(fneg), loadLexicon(fnegasi)\n",
    "print(positif[:10])\n",
    "print(negatif[:10])\n",
    "print(negasi[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def prediksiSentiment(kalimat, positif, negatif, negasi):\n",
    "    # Naive Approach, nanti akan kita diskusikan bagaimana improvisasi fungsi sederhana ini\n",
    "    posWords = []\n",
    "    negWords = [w for w in negatif if w in kalimat]\n",
    "    for w in positif:\n",
    "        if w in kalimat:\n",
    "            negated = False\n",
    "            for n in negasi:\n",
    "                if n+' '+w in kalimat:\n",
    "                    negWords.append(n+' '+w)\n",
    "                    negated = True\n",
    "                    break\n",
    "            if not negated:\n",
    "                posWords.append(w)\n",
    "    nPos, nNeg = len(posWords), len(negWords)\n",
    "    if nPos>nNeg:\n",
    "        return 1\n",
    "    if nPos<nNeg:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "T = \"mie ayam ini tidak enak\"\n",
    "prediksiSentiment(T, positif, negatif, negasi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Discussion: Strength & weakness, Error & Model Analysis + Interpretation</font></center> \n",
    "\n",
    "1. Apa kelebihan dan kekurangannya?\n",
    "2. Apa yang bisa dilakukan untuk mencoba memperbaikinya?\n",
    "3. Interpretasi hasil Positif dan Negatifnya.\n",
    "4. Mari analisa data-data yang salah diprediksi.\n",
    "\n",
    "## <center><font color=\"blue\">Metode unsupervised yang lain</font></center>\n",
    "\n",
    "* Bing Liu’s, MPQA subjectivity, AFINN, SentiWordNet, VADER, dll\n",
    "* Contoh penggunaan Vader & Afinn: https://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project03%20-%20Sentiment%20Analysis%20Unsupervised%20Lexical%20Models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi ke seluruh data\n",
    "data['unsupervised'] = [prediksiSentiment(t.Tweet, positif, negatif, negasi) for i,t in tqdm(data.iterrows())]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(data.sentimen, data.unsupervised))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"red\">Hasilnya jelek sekali, apakah pendekatan ini seburuk itu?</font></center> \n",
    "\n",
    "## Mari kita analisa lebih lanjut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betul = [d.Tweet for i,d in data.iterrows() if d.sentimen==d.unsupervised]\n",
    "salah = [d.Tweet for i,d in data.iterrows() if d.sentimen!=d.unsupervised]\n",
    "simpan(betul, 'unsupervised-betul.txt')\n",
    "simpan(salah, 'unsupervised-salah.txt')\n",
    "\"Done\" # Lanjut ke Voyant Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Representasi Dokumen/Teks</font></center> \n",
    "\n",
    "<img alt=\"\" src=\"images/3_Bentuk umum representasi dokumen.JPG\" style=\"height: 294px ; width: 620px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">tf-idf: Term Frequency - Inverse Document Frequency</font></center> \n",
    "\n",
    "<img alt=\"\" src=\"images/toydata_vsm.png\" />\n",
    "\n",
    "* Menurut http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "* default formula tf-idf yang digunakan sk-learn adalah:\n",
    "\n",
    "* $tfidf = tf * log(\\frac{N}{df+1})$ ==> linear_tf, Smooth IDF\n",
    "* namun kita merubahnya menjadi:\n",
    "* $tfidf = tf * log(\\frac{N}{df})$ ==> linear_tf, Non Smooth IDF\n",
    "* $tfidf = (1+log(tf)) * log(\\frac{N}{df+1})$ ==> sublinear_tf, Smooth IDF\n",
    "* $tfidf = (1+log(tf)) * log(\\frac{N}{df})$ ==> sublinear_tf, Non Smooth IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Best-Match-Formula-:-BM25\">Best-Match Formula : BM25</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_bm25_simple.png\" style=\"height: 123px; width: 300px;\" /></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>di IR nilai b dan k yang optimal adalah :&nbsp;<strong> <em>b</em> = 0.75&nbsp; dan k = [1.2 - 2.0]&nbsp; &nbsp;</strong><br />\n",
    "\tref:&nbsp;<em>Christopher, D. M., Prabhakar, R., &amp; Hinrich, S. C. H. &Uuml;. T. Z. E. (2008). Introduction to information retrieval.&nbsp;An Introduction To Information Retrieval,&nbsp;151, 177.</em></li>\n",
    "\t<li>Tapi kalau untuk TextMining (clustering) nilai <strong>k optimal adalah 20, nilai b = sembarang (boleh = 0.75)</strong><br />\n",
    "\tref:&nbsp;<em>Whissell, J. S., &amp; Clarke, C. L. (2011). Improving document clustering using Okapi BM25 feature weighting.&nbsp;Information retrieval,&nbsp;14(5), 466-487.</em></li>\n",
    "\t<li><strong>avgDL </strong>adalah rata-rata panjang dokumen di seluruh dataset dan <strong>DL </strong>adalah panjang dokumen D.<br />\n",
    "\thati-hati, ini berbeda dengan &nbsp;tf-idf MySQL diatas.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan modul SciKit untuk merubah data tidak terstruktur ke VSM\n",
    "# Scikit implementation http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Parameter: lowercase=True, smooth_idf= True, sublinear_tf=True,  ngram_range=(1, 2), max_df=0.90, min_df=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variasi pembentukan matriks VSM:\n",
    "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
    "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat'\n",
    "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
    "\n",
    "D = [d1, d2, d3, d4]\n",
    "# Jika kita menggunakan cara biasa: \n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True)\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Grams VSM\n",
    "# Bermanfaat untuk menangkap frase kata, misal: \"ga banget\", \"pisang goreng\", dsb\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary based VSM\n",
    "# Bermanfaat untuk menghasilkan hasil analisa yang \"bersih\"\n",
    "# variasi 2\n",
    "D = [d1,d2,d3,d4]\n",
    "Vocab = {'seru banget':0, 'seru':1, 'the best':2, 'lama':3, 'text mining':4, 'nlp':5, 'ayam':6}\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=Vocab)\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(vsm.toarray())\n",
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    \n",
    "# <center><strong><font color=\"blue\">Supervised Sentiment Analysis</font></strong></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bentuk VSM-nya\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True,smooth_idf= True, sublinear_tf=True, \n",
    "                                   ngram_range=(1, 2), max_df=0.95, min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 99\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['dataCleaned'], data['sentimen'], test_size=0.2, random_state=seed)\n",
    "x_train = tfidf_vectorizer.fit_transform(x_train) # \"Fit_Transform\"\n",
    "x_test = tfidf_vectorizer.transform(x_test) # Perhatikan disini hanya \"Transform\"\n",
    "\n",
    "print(x_train.shape, x_test.shape) # Jumlah kolom Sama ==> ini penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM: http://scikit-learn.org/stable/modules/svm.html\n",
    "from sklearn import svm\n",
    "\n",
    "dSVM = svm.SVC(kernel='linear', decision_function_shape='ovo') # oneversus one SVM\n",
    "dSVM.fit(x_train, y_train)\n",
    "\n",
    "y_SVM = dSVM.predict(x_test)\n",
    "print(classification_report(y_test, y_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hasilnya rendah? ... Hhmmmm ... Mari coba optimalkan model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['dataCleaned'], data['sentimen'], test_size=0.2, random_state=seed)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "pipeSVM = make_pipeline(TfidfVectorizer(), svm.SVC())\n",
    "#print(sorted(pipeSVM.get_params().keys()))\n",
    "\n",
    "paramsSVM = {}\n",
    "paramsSVM['tfidfvectorizer__min_df'] = [5, 10, 30]\n",
    "paramsSVM['tfidfvectorizer__max_df'] = [0.5, 0.75, 0.95]\n",
    "paramsSVM['tfidfvectorizer__smooth_idf'] = [True] # [True, False]\n",
    "paramsSVM['tfidfvectorizer__sublinear_tf'] = [True] # [True, False]\n",
    "paramsSVM['tfidfvectorizer__ngram_range'] = [(1, 1), (1, 2), (1,3)]\n",
    "paramsSVM['svc__C'] = [0.1, 10, 100] #sp.stats.uniform(scale=1)\n",
    "paramsSVM['svc__gamma'] = [1.0, 0.1, 0.001]\n",
    "paramsSVM['svc__kernel'] = ['rbf', 'poly', 'sigmoid', 'linear']\n",
    "paramsSVM['svc__decision_function_shape'] = ['ovo', 'ovr']\n",
    "\n",
    "gridsvmCV = GridSearchCV(pipeSVM, paramsSVM, cv=5, scoring='accuracy', verbose=1, n_jobs=-3)\n",
    "gridsvmCV.fit(x_train, y_train) # hati-hati disini x_train harus Text\n",
    "print(gridsvmCV.best_score_)\n",
    "print(gridsvmCV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mungkin Modelnya yang tidak mampu mengklasifikasikan?\n",
    "\n",
    "## Kita coba Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Dengan cara yang sama seperti tadi, coba analisa data yang salah di prediksi</font></center> \n",
    "\n",
    "### Coba juga ke seluruh data dan variasi preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Di kesempatan lain kita bisa coba dengan model Deep Learning</font></center> \n",
    "\n",
    "<p><img alt=\"\" src=\"images/5_DeepLearning.png\" style=\"width: 690px ; height: 777px\" /></p>\n",
    "\n",
    "<p><big>Yang menjadi pembeda utama DL dengan ML adalah DL &quot;<em>Learning representations from data</em>&quot;. Misal Word Embedding (bandingkan dengan VSM di Machine Learning).<br />\n",
    "Makna &quot;Deep&quot; di DL sendiri bermakna &quot;successive layers of representations&quot; biasa juga disebut sebagai&nbsp;<em>layered representations learning</em> atau <em>hierarchical representations learning</em>.</big></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM: Long Short Term memory\n",
    "\n",
    "### https://tau-data.id/lstm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Better Unsupervised Sentiment Analysis</font></center>\n",
    "\n",
    "<p><img src=\"images/satriadata2021/unsupervised_sentimen_analysis.png\" /></p>\n",
    "\n",
    "* Feature Engineering, Lexicon Based, & Bayesian Model\n",
    "* image source: \n",
    " - http://pasaentuciudad.com.mx/sentiment-analysis%E2%80%8A-%E2%80%8Acomparing-3-common-approaches-naive-bayes-lstm-and-vader/\n",
    " - https://www.emerald.com/insight/content/doi/10.1016/j.aci.2019.11.003/full/html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Improvement Implementasi di Industri</font></center>\n",
    "\n",
    "* **Hierarchical Classification**: Aspect-Based Sentiment Analysis\n",
    "* **Hybrid Model**: Sentimen Analysis ==> Topic Modelling\n",
    "\n",
    "<p><img src=\"images/Aspect-based-sentiment.png\" alt=\"\" width=\"399\" height=\"187\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Supplementary\">Supplementary</h2>\n",
    "\n",
    "<p>* Negasi suatu kata bukan berarti memiliki sentimen kebalikannya. Misal &quot;jelek&quot; dan &quot;tidak jelek&quot; (terrible vs not terrible).</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/negation_sentiments.png\" /></p>\n",
    "\n",
    "<p>[*]. Zhu, X., Guo, H., Mohammad, S., &amp; Kiritchenko, S. (2014). An empirical study on the effect of negation words on sentiment. In&nbsp;<i>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>&nbsp;(Vol. 1, pp. 304-313).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Makna positive/negative atau pro/kontra subjective (bias) terhadap user.\n",
    "* StopWords removal in general is a bad idea\n",
    "* learn the lingo in your topic, sentiment expressions are different across fields, languages, and regions.\n",
    "* Sarcasm perlu konteks untuk di deteksi dengan tepat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Feature-Engineering/Extraction\">Feature Engineering/Extraction</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Ketimbang pemilihan model yang optimal, beberapa literature sudah melaporkan bahwa feature engineering/extraction lebih efektif [1].</li>\n",
    "\t<li>Selain itu, pendekatan semantic dalam FE juga lebih plausible untuk dilakukan.</li>\n",
    "\t<li>Tabel berikut adalah contoh FE yang bisa dilakukan spesifik terhadap model SA.</li>\n",
    "\t<li><img alt=\"\" src=\"images/SA_Analysis_Features.png\" style=\"width: 544px; height: 425px;\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/Roda_Emosi.jpeg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> End of Module.\n",
    "\n",
    "<hr />\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
