{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"green\"> https://bit.ly/unp-2021-C</font><br><font color=\"blue\">Module C: Preprocessing &  Visualisasi Teks Media Sosial</font></center>\n",
    "\n",
    "<center><img alt=\"\" src=\"images/unp-2021/Cover_UNP_2021_SMA_Vis.jpg\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">tau-data Indonesia</font><br>(C) Taufik Sutanto - 2021</center>\n",
    "<center><a href=\"https://tau-data.id\">https://tau-data.id</a> ~ <a href=\"mailto:taufik@tau-data.id\">taufik@tau-data.id</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Outline Module C: Preprocessing & Visualisasi Teks Media Sosial</font></center>\n",
    "\n",
    "## <font color=\"green\">Text Preprocessing</font>\n",
    "* Tokenisasi\n",
    "* Normalisasi - Lemma\n",
    "* Stopword Filtering\n",
    "* Export Preprocessed Text\n",
    "\n",
    "## <font color=\"green\">Text Visualization using Voyant Tools</font>\n",
    "* Word Clouds\n",
    "* Word Links\n",
    "* Word Context for Interpretation\n",
    "* Word Tree\n",
    "* Word Trends\n",
    "\n",
    "><font color=\"red\">\"*I always have a basic plot outline, but I like to leave some things to be decided while I write*.\" ~ J. K. Rowling</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/unp.py\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.dic\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/kata_dasar.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-ind-def.tab\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-msa-all.tab\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/ind_SA.csv\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
    "\n",
    "    !pip install --upgrade spacy python-crfsuite unidecode textblob sastrawi sklearn-pycrfsuite\n",
    "    !pip install --upgrade unidecode twython tweepy beautifulsoup4\n",
    "    !python -m spacy download en\n",
    "    !python -m spacy download xx\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    nltk.download('popular')\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unp\n",
    "import json, urllib.request, requests\n",
    "from urllib.request import Request, urlopen\n",
    "from twython import TwythonStreamer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contoh API Keys (Sesuaikan dengan API keys masing-masing)\n",
    "Ck = 'QCgrPpBwTo8OJeV' # consumer_key\n",
    "Cs = '3FRZuNT4KgWVIAqe3wvYDf2aqIISgeMweP6zOIT' # consumer_secret\n",
    "At = '2214118411-3V6Tci5uxAbBZAQqM3CkNfWFDn7' # access_token\n",
    "As = 'THDDQsNeeOT1DzfufobW580UKySM6MH' # access_secret\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome \"Taufik Sutanto\" you are now connected to twitter server\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.auth.OAuthHandler(Ck, Cs)\n",
    "auth.set_access_token(At, As)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, timeout=180, retry_count=5, retry_delay=3)\n",
    "usr_ = api.verify_credentials()\n",
    "print('Welcome \"{}\" you are now connected to twitter server'.format(usr_.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [01:25, 23.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Selesai mengambil 2000 tweets tentang 'banjir'\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry = 'banjir' # Keyword topik SMA/SNA yang ingin diteliti\n",
    "N = 2000 # jumlah N tweet \"terkini\" yang ingin diambil (7 hari ke belakang)\n",
    "Tweets = []\n",
    "for post in tqdm(tweepy.Cursor(api.search_tweets, q=qry, lang='id', tweet_mode='extended').items(N)):\n",
    "    Tweets.append(post)\n",
    "    \n",
    "\"Selesai mengambil {} tweets tentang '{}'\".format(len(Tweets), qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@DNoertjahjo terjadi banjir semalam di bandung tgl 19 /11/2021 ...sampai mlm lum kunjung ada bantuan....salahkan bpk anies ya...yg selalu bilang kadrun itu golongan PKI https://t.co/coNeBTCxPU',\n",
       " 'RT @jkrtudmsubang18: Sesi Penampilan Audit oleh pihak BADSA Kem Pertahanan Malaysia, bagi Projek Menaiktaraf Sistem BFI dan Projek Tebatan…',\n",
       " 'RT @aiek__: BANSOS jd Bancakan PDIP\\nBisnis PCR menteri terbongkar\\nSintang Banjir akibat deferostasi\\nHutang bengkak Rp6 ribu trilyun\\nKilang…',\n",
       " 'RT @arfaahmad13: Ada jgak ibu mengandung yg tgh duduk kat belakg.. confirm kena hjn, dia ada bawa anak umur dlm 2,3 tahun demam... Ada makc…',\n",
       " '@ekoSrandyanto Halo, saya Bencana Bot. Untuk melaporkan banjir di sekitarmu, silakan balas dengan #banjir. https://t.co/APypuuE0Mr']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waktu = [t._json['created_at'] for t in Tweets]\n",
    "usernames = [t._json['user']['screen_name'] for t in Tweets]\n",
    "tweets = [t._json['full_text'] for t in Tweets]\n",
    "\n",
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Tweet Json Saved to Tweets_unp2021.json\n"
     ]
    }
   ],
   "source": [
    "def saveTweets(tweets, file='Tweets.json'): #in Json Format\n",
    "    with open(file, 'w') as f:\n",
    "        for t in tweets:\n",
    "            try:\n",
    "                f.write(json.dumps(t)+'\\n')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "Tweets = [t._json for t in Tweets]\n",
    "# Menyimpan hasil crawling twitter\n",
    "fileName = 'Tweets_unp2021.json'\n",
    "saveTweets(Tweets, file=fileName)\n",
    "print('Data Tweet Json Saved to '+fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi\n",
    "\n",
    "<p>Tokenisasi adalah pemisahan kata, simbol, frase, dan entitas penting lainnya (yang disebut sebagai token) dari sebuah teks untuk kemudian di analisa lebih lanjut. Token dalam NLP sering dimaknai dengan &quot;sebuah kata&quot;, walau tokenisasi juga bisa dilakukan ke kalimat, paragraf, atau entitas penting lainnya (misal suatu pola string DNA di Bioinformatika).</p>\n",
    "\n",
    "<p><strong>Mengapa perlu tokenisasi?</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Langkah penting dalam preprocessing, menghindari kompleksitas mengolah langsung pada string asal.</li>\n",
    "\t<li>Menghindari masalah (semantic) saat pemrosesan model-model natural language.</li>\n",
    "\t<li>Suatu tahapan sistematis dalam merubah unstructured (text) data ke bentuk terstruktur yang lebih mudah di olah.</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images\\2_Pipeline_Tokenization.png\" style=\"height:300px; width:768px\" /><br />\n",
    "[<a href=\"https://www.softwareadvice.com/resources/what-is-text-analytics/\" target=\"_blank\"><strong>Image Source</strong></a>]: https://www.softwareadvice.com/resources/what-is-text-analytics/</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi tidak hanya language dependent, tapi juga environment dependent\n",
    "\n",
    "<p>Tokenization sebenarnya tidak sesederhana memisahkan berdasarkan spasi dan removing symbol. Sebagai contoh dalam bahasa Jepang/Cina/Arab suatu kata bisa terdiri dari beberapa karakter.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/2_Tokenization_Complexity.jpg\" style=\"height:500px; width:686px\" /><br />\n",
    "[<a href=\"http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Mr.', 'Man', '.', 'He', 'smiled', '!', '!', 'This', ',', 'i.e.', 'that', ',', 'is', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "# Contoh tokenisasi menggunakan Spacy\n",
    "from spacy.lang.en import English\n",
    "nlp_en = English()\n",
    "\n",
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "nlp = nlp_en(T)\n",
    "Tokens = [tok.text for tok in nlp]\n",
    "print(Tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi (NLP) Bahasa Indonesia:\n",
    "\n",
    "<p>NLTK belum support Bahasa Indonesia, bahkan module NLP Python yang support bahasa Indonesia secara umum masih sangat langka. Beberapa <u><strong>resources </strong></u>yang dapat digunakan:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li><strong><a href=\"https://github.com/kirralabs/indonesian-NLP-resources\" target=\"_blank\">KirraLabs</a></strong>: Mix of NLP-TextMining resources</li>\n",
    "\t<li><strong><a href=\"https://pypi.python.org/pypi/Sastrawi/1.0.1\" target=\"_blank\">Sastrawi 1.0.1</a>:</strong>&nbsp;untuk &quot;stemming&quot; &amp;&nbsp;<strong><a href=\"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\" target=\"_blank\">stopwords&nbsp;</a></strong>bahasa Indonesia.</li>\n",
    "\t<li><strong><a href=\"http://stop-words-list-bahasa-indonesia.blogspot.co.id/2012/09/daftar-kata-dasar-bahasa-indonesia.html\" target=\"_blank\">Daftar Kata Dasar Indonesia</a></strong>:&nbsp;Bisa di load sebagai dictionary di Python</li>\n",
    "\t<li><strong><a href=\"https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata\" target=\"_blank\">Wiktionary</a></strong>: ProyekWiki bahasa Indonesia [termasuk Lexicon]</li>\n",
    "\t<li><a href=\"http://wn-msa.sourceforge.net/\" target=\"_blank\"><strong>WordNet Bahasa Indonesia</strong></a>: Bisa di load&nbsp;sebagai dictionary (atau NLTK<em>*</em>) di Python.</li>\n",
    "\t<li><strong><a href=\"http://kakakpintar.com/daftar-kata-baku-dan-tidak-baku-a-z-dalam-bahasa-indonesia/\" target=\"_blank\">Daftar Kata Baku-Tidak Baku</a></strong>: Bisa di load sebagai dictionary di Python.</li>\n",
    "\t<li><strong><a href=\"https://spacy.io/\" target=\"_blank\">Spacy</a></strong>: Cepat/efisien, MIT License, tapi language model Indonesia masih terbatas.</li>\n",
    "\t<li><a href=\"http://ufal.mff.cuni.cz/udpipe\" target=\"_blank\"><strong>UdPipe</strong></a>: Online request &amp; restricted license (support berbagai bahasa -&nbsp;pemrograman).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu-kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh-oleh', 'di', 'pasar']\n"
     ]
    }
   ],
   "source": [
    "# Contoh Tokenisasi dalam bahasa Indonesia dengan Spacy\n",
    "from spacy.lang.id import Indonesian\n",
    "nlp_id = Indonesian()  # Language Model\n",
    "\n",
    "teks = 'Sore itu, Hamzah melihat kupu-kupu di taman. Ibu membeli oleh-oleh di pasar'\n",
    "nlp = nlp_id(teks)\n",
    "Tokens = [tok.text for tok in nlp]\n",
    "\n",
    "print(Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sore', 'itu', ',', 'Hamzah', 'melihat', 'kupu', '-', 'kupu', 'di', 'taman', '.', 'Ibu', 'membeli', 'oleh', '-', 'oleh', 'di', 'pasar']\n"
     ]
    }
   ],
   "source": [
    "# Jika menggunakan Language model English:\n",
    "nlp = nlp_en(teks)\n",
    "print([tok.text for tok in nlp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><u><big><strong>Word Case</strong></big></u><big> (Huruf BESAR/kecil):</big></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menganalisa makna (<em>semantic</em>) dari suatu (frase) kata dan mencari informasi dalam proses textmining, seringnya (*) kita tidak membutuhkan informasi huruf besar/kecil dari kata&nbsp;tersebut.</li>\n",
    "\t<li><em>Text case normaliation</em> dapat dilakukan pada string secara efisien tanpa melalui tokenisasi (mengapa?).</li>\n",
    "\t<li>Namun, bergantung pada analisa teks yang akan digunakan pengguna harus berhati-hati dengan urutan proses (pipelining) dalam preprocessing. Mengapa dan apa contohnya?</li>\n",
    "</ul>\n",
    "\n",
    "<p>(*) Coba temukan minimal 2 pengecualian dimana&nbsp; huruf kapital/kecil (case) mempengaruhi makna/pemrosesan teks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there!, i am a student. nice to meet you :)\n",
      "HI THERE!, I AM A STUDENT. NICE TO MEET YOU :)\n"
     ]
    }
   ],
   "source": [
    "# Ignore case (huruf besar/kecil)\n",
    "T = \"Hi there!, I am a student. Nice to meet you :)\"\n",
    "print(T.lower())\n",
    "print(T.upper())\n",
    "# Perintah ini sangat efisien karena hanya merubah satu bit di setiap (awal) bytes dari setiap karakter\n",
    "# Sehingga tetap efisien jika ingin dilakukan sebelum tokenisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Morphological-Linguistic Normalization: Stemming &amp; Lemmatization\n",
    "(Canonical Representation)\n",
    "<p><img alt=\"\" src=\"images/2_yoda.jpg\" style=\"height:400px; width:400px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Stemming dan Lemma</font>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan konteks kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung konteks.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Mengapa melakukan Stemming &amp; Lemmatisasi</strong>?</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Sering digunakan di IR (Information Retrieval) agar ketika seseorang mencari kata tertentu, maka seluruh kata yang terkait juga diikutsertakan.<br />\n",
    "\tMisal:&nbsp;<em>organize</em>,&nbsp;<em>organizes</em>, and&nbsp;<em>organizing&nbsp;</em>&nbsp;dan&nbsp;<em>democracy</em>,&nbsp;<em>democratic</em>, and&nbsp;<em>democratization</em>.</li>\n",
    "\t<li>Di Text Mining Stemming dan Lemmatisasi akan mengurangi dimensi (mengurangi variasi morphologi), yang terkadang akan meningkatkan akurasi.</li>\n",
    "\t<li>Tapi di IR efeknya malah berkebalikan: <strong><font color=\"blue\">meningkatkan recall, tapi menurunkan akurasi&nbsp;</font></strong>[<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\"><strong>Link</strong></a>]. Contoh: kata&nbsp;<em>operate, operating, operates, operation, operative, operatives, dan operational</em>&nbsp;jika di stem menjadi <em>operate</em>, maka ketika seseorang mencari &quot;<em>operating system</em>&quot;, maka entry seperti&nbsp;<em>operational and research</em> dan&nbsp;<em>operative and dentistry</em>&nbsp;akan muncul sebagai entry dengan relevansi yang cukup tinggi.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Stemming tidak perlu &quot;benar&quot;, hanya perlu konsisten. Sehingga memiliki berbagai variansi, (sebagian) contoh di NLTK:</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token =  I , Lemmanya =  I\n",
      "Token =  am , Lemmanya =  be\n",
      "Token =  sure , Lemmanya =  sure\n",
      "Token =  apples , Lemmanya =  apple\n",
      "Token =  and , Lemmanya =  and\n",
      "Token =  oranges , Lemmanya =  orange\n",
      "Token =  are , Lemmanya =  be\n",
      "Token =  similar , Lemmanya =  similar\n"
     ]
    }
   ],
   "source": [
    "# Spacy Lemmatizer English\n",
    "# Perhatikan/hati-hati huruf besar/kecil-nya\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "E = \"I am sure apples and oranges are similar\"\n",
    "doc = nlp(E)\n",
    "\n",
    "for token in doc:\n",
    "    print(\"Token = \", token.text, \", Lemmanya = \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raya itu bareng dengan saat kita pergi ke padang\n",
      "raya pergi suara\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer dengan Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "I = \"perayaan itu Berbarengan dengan saat kita bepergian ke Padang\"\n",
    "print(stemmer.stem(I))\n",
    "print(stemmer.stem(\"Perayaan Bepergian Menyuarakan\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Level Normalization: StopWords\n",
    "<p><u>Di Text Mining</u> kata-kata yang <strong>sering muncul </strong>(dan jarang sekali muncul) memiliki sedikit sekali informasi (signifikansi) terhadap model (machine learning) yang digunakan. Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan klasifikasi) atau di semua cluster (di permasalahan pengelompokan/clustering). Kata-kata yang sering muncul ini biasa disebut &quot;StopWords&quot;. Stopwords berbeda-beda bergantung dari Bahasa dan Environment (aplikasi)-nya.<br />\n",
    "<strong>Contoh</strong>:<br />\n",
    "\n",
    "<ul>\n",
    "\t<li>Stopwords bahasa Inggris: am, is, are, do, the, of, etc.</li>\n",
    "\t<li>Stopwords bahasa Indonesia: adalah, dengan, yang, di, ke, dsb</li>\n",
    "\t<li>Stopwords twitter: RT, ...<br />\n",
    "\t<img alt=\"\" src=\"images/2_StopWords.png\" style=\"height:250px; width:419px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['&gt', '&lt', '&nbsp', 'a', 'able', 'about', 'above']\n",
      "['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar']\n"
     ]
    }
   ],
   "source": [
    "df=open('data/stopwords_en.txt',\"r\",encoding=\"utf-8\", errors='replace')\n",
    "en_stop = df.readlines()\n",
    "df.close()\n",
    "en_stop = [t.strip().lower() for t in en_stop]\n",
    "\n",
    "df=open('data/stopwords_id.txt',\"r\",encoding=\"utf-8\", errors='replace')\n",
    "id_stop = df.readlines()\n",
    "df.close()\n",
    "id_stop = [t.strip().lower() for t in id_stop]\n",
    "\n",
    "print(en_stop[:7])\n",
    "print(id_stop[:7])\n",
    "id_stop = set(id_stop) # Merubah ke set agar komputasi jauh lebih cepat\n",
    "en_stop = set(en_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am doing nlp at tau-data indonesia belajar nlp tau-data indonesia\n"
     ]
    }
   ],
   "source": [
    "# Contoh Cara menggunakan stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "T = \"I am doing NLP at tau-data Indonesia,... \\\n",
    "    adapun saya anu sedang belajar NLP di tau-data Indonesia\"\n",
    "T = T.lower()\n",
    "id_stop.add('anu')\n",
    "\n",
    "Tokens = TextBlob(T).words # Tokenisasi \n",
    "T2 = [t for t in Tokens if t not in id_stop] # Sastrawi_StopWords_id Personal_StopWords_en Personal_StopWords_id\n",
    "#T2 = [t for t in T2 if t not in en_stop] # Sastrawi_StopWords_id Personal_StopWords_en Personal_StopWords_id\n",
    "print(' '.join(T2))\n",
    "# Catatan: Selalu lakukan Stopword filtering setelah tokenisasi (dan normalisasi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text Menggunakan fungsi dari tau-data Indonesia ~ Module \"unp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████▌                                                    | 672/2000 [04:02<04:53,  4.52it/s]"
     ]
    }
   ],
   "source": [
    "stopId, lemmaId = unp.LoadStopWords(lang='id')\n",
    "slangFixId = unp.loadCorpus(file = 'data/slang.dic', sep=':')\n",
    "\n",
    "cleaned_data = []\n",
    "for tweet in tqdm(tweets):\n",
    "    cleaned_data.append(unp.cleanText(tweet, lemma=lemmaId, lan='id', stops = stopId, fix=slangFixId))\n",
    "    \n",
    "cleaned_data[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menyimpan Hasil Preprocessing untuk di Visualisasi\n",
    "\n",
    "### Kita simpan juga data yang tidak di preprocessing untuk module D (SNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ke txt ... lalu nanti akan di upload ke Voyant Tools\n",
    "filename = 'tweets_unp.txt'\n",
    "with open(filename, 'w') as f:\n",
    "    for data in cleaned_data:\n",
    "        f.write(data+'\\n')\n",
    "        \n",
    "\"Tweets Saved!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Raw Tweet Untuk Module D - SNA\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(zip(usernames, tweets), columns=['user', 'tweet'])\n",
    "df.to_csv('RawTweets_unp.csv', encoding='utf8', index=False)\n",
    "\n",
    "\"Raw Tweets Saved!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Analytics</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Tidak seperti data terstruktur, data tidak terstruktur seperti teks termasuk salah satu data yang cukup sulit untuk divisualisasikan.<br />\n",
    "\t<img alt=\"\" src=\"images/11_charts.jpg\" style=\"height:150px; width:276px\" /></li>\n",
    "\t<li>Namun terdapat Tools seperti Voyant yang dapat membantu dalam visualisasi sekaligus analisis.<br />\n",
    "\t<img alt=\"\" src=\"images/11_voyant.png\" style=\"height:118px; width:426px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Voyant-dapat-digunakan-dalam-2-cara:\">Voyant dapat digunakan dalam 2 cara:</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Online</strong>:&nbsp;<a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a><br />\n",
    "\t<u>Kelebihan</u>: Sederhana &amp; portable, tanpa harus install di komputer kita.<br />\n",
    "\t<u>Kekurangan</u>: butuh koneksi internet, tidak cocok untuk data teks yang besar, privacy.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Offline </strong>di komputer kita [Java Based]</p>\n",
    "\t</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2]. Jalankan Voyant secara offline atau online di URL&nbsp;<a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a>\n",
    "\n",
    "[3]. Upload file yang baru saja kita simpan ( **tweets_unp.txt** )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-1:-WordClouds\">Penggunaan Voyant 1: WordClouds</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.</li>\n",
    "\t<li>slider terms: mengkontrol banyaknya terms yang disertakan.</li>\n",
    "\t<li><strong>Summary </strong>(statistics)</li>\n",
    "\t<li><strong>Documents </strong>==&gt; add more</li>\n",
    "\t<li><strong>Phrases </strong>(n-grams like)</li>\n",
    "\t<li><strong>Export </strong>Visualisasi (kanan atas - pertama)</li>\n",
    "\t<li><strong>Options </strong>(kanan atas ke-3): Font, size, stopwords, whitelist</li>\n",
    "\t<li>&quot;?&quot; ==&gt; More Help</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Penggunaan Voyant 2: Word Links</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Visualization Tools ==&gt; Links</li>\n",
    "\t<li>Klik sembarang terms untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-3:-Word-Tree\">Penggunaan Voyant 3: Word Tree</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Klik branch untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Penggunaan Voyant 4: Trends</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Document Tools ==&gt; Trends</li>\n",
    "\t<li>.. Butuh preprocessing ...&nbsp;</li>\n",
    "\t<li>Data harus terurut waktu</li>\n",
    "\t<li>Berikut contohnya</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Latihan :&nbsp;</strong></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Crawl twitter dengan salah satu topik yang sedang trending saat ini di Indonesia.&nbsp;</li>\n",
    "\t<li>Lakukan Preprocessing data</li>\n",
    "\t<li>Visualisasikan</li>\n",
    "\t<li>Diskusikan insight/informasi yang didapat</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>End of Module C</h1>\n",
    "<hr />\n",
    "<img alt=\"\" src=\"images/meme-cartoon/2_Linguistic_Joke.jpg\" style=\"height:400px; width:608px\" />"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
