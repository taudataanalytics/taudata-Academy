{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><strong>Natural Language Processing and Text Mining (NLPTM)<br> Unstructured Data Analysis (UDA)*</strong></center>\n",
    "    \n",
    "## <center><strong><font color=\"blue\">01 - Pendahuluan Natural Language Processing dan Text Processing</font></strong></center>\n",
    "<center><img alt=\"\" src=\"images/SocMed.png\"/> </center>\n",
    "    \n",
    "## <center>(C) Taufik Sutanto - 2020 <br><strong><font color=\"blue\"> taudata Analytics ~ https://taudata.blogspot.com/2020/04/nlptm-01.html</font></strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline :\n",
    "\n",
    "* Pendahuluan\n",
    "* Review String di Python\n",
    "* Filtering (stopwords)\n",
    "* Replace slang/typos/singkatan\n",
    "* Spell Check\n",
    "* Machine translation\n",
    "* Pengenalan Reguler expression \n",
    "* Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aplikasi Umum NLP:\n",
    "\n",
    "* Speech Recognition dan Classification\n",
    "* Machine Translation (Misal&nbsp;https://translate.google.com/ )\n",
    "* Information Retrieval (IR)&nbsp;(misal www.google.com, bing, elasticsearch, etc.)\n",
    "* Man-Machine Interface (misal Chatbot, Siri, cortana, atau Alexa)\n",
    "* Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apakah Perbedaan antara NLP dan Text Mining (TM)?\n",
    "\n",
    "<p>TM (terkadang disebut Text Analytics) adalah sebuah pemrosesan teks (biasanya dalam skala besar) untuk menghasilkan (generate) informasi atau insights. Untuk menghasilkan informasi TM menggunakan beberapa metode, termasuk NLP. TM mengolah teks secara eksplisit, sementara NLP mencoba mencari makna latent (tersembunyi) lewat aturan bahasa (e.g. grammar/idioms/Semantics).<br /></p><p>\n",
    "    \n",
    "<strong>Contoh aplikasi TM</strong> : Social Media Analytics (SMA), Insights from customer's review, Sentiment Analysis, Topic Modelling, dsb.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modul-modul yang digunakan di Lesson ini \n",
    "## Silahkan install melalui terminal jika dijalankan secara lokal (PC/Laptop)\n",
    "\n",
    "### Perhatian: Anda harus menjalankan setiap cell secara berurutan dari paling atas, tanpa terlewat satu cell-pun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting environment:  Running the code locally.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Installing Modules & getting the necessary files for \"Google Colab\"\n",
    "Jika dijalankan di komputer lokal (PC/Laptop) silahkan unduh secara manual dan \n",
    "lakukan instalasi module di terminal/command prompt \n",
    "\"\"\"\n",
    "print(\"Detecting environment: \", end=' ')\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running the code in Google Colab. Installing and downloading dependencies.\\nPlease wait...\")\n",
    "    import nltk\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/lib/taudataNlpTm.py\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/corpus_sederhana.txt\n",
    "    !pip install unidecode textblob sastrawi\n",
    "    nltk.download('popular')\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review Tipe Variabel di Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = 2.3 # Floating Point (tidak sama dengan bilangan Real)\n",
    "b = 3.0   # Integer\n",
    "c = True # T/F Boolean\n",
    "d = 'python' # String\n",
    "e = [a,b,c,d] # List\n",
    "f = (a,b,c,d) # Tuple\n",
    "g = set([a,b,c,d]) # Set\n",
    "h = {'a':1, 'b':2, 7:'abc'} # Dictionary : keys, values, items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n"
     ]
    }
   ],
   "source": [
    "# Sehingga kita bisa melakukan preprocessing dasar dengan string manipulation seperti berikut:\n",
    "S = 'cob762a '\n",
    "print(dir(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = '84 '\n",
    "X.isdigit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function isdigit:\n",
      "\n",
      "isdigit() method of builtins.str instance\n",
      "    Return True if the string is a digit string, False otherwise.\n",
      "    \n",
      "    A string is a digit string if all characters in the string are digits and there\n",
      "    is at least one character in the string.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(X.isdigit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apa kabar'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# String adalah Tuple, sehingga\n",
    "S1 = 'apa '\n",
    "S2 = 'kabar'\n",
    "S1+S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 character pertama:  NLP dan\n",
      "7 character terakhir:   Python\n"
     ]
    }
   ],
   "source": [
    "# String adalah Tuple sehingga bisa di akses seperti List\n",
    "S = 'NLP dan TextMining di Python'\n",
    "print('7 character pertama: ', S[:7])\n",
    "print('7 character terakhir: ', S[-7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error, tuple tidak bisa dirubah nilainya (inplace)\n"
     ]
    }
   ],
   "source": [
    "# Tapi hati-hati karena ia tuple maka:\n",
    "try:\n",
    "    S[5] = 'o'\n",
    "except:\n",
    "    print('Error, tuple tidak bisa dirubah nilainya (inplace)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua']\n",
      "['&gt', '&lt', '&nbsp', 'a', 'able', 'about', 'above', 'abst', 'accordance', 'according']\n",
      "['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir']\n",
      "126 755 179 2659\n"
     ]
    }
   ],
   "source": [
    "# Loading Stopwords: Ada beberapa cara\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "factory = StopWordRemoverFactory()\n",
    "\n",
    "NLTK_StopWords = stopwords.words('english')\n",
    "Sastrawi_StopWords_id = factory.get_stop_words()\n",
    "\n",
    "df=open('data/stopwords_en.txt',\"r\",encoding=\"utf-8\", errors='replace')\n",
    "en_stop = df.readlines()\n",
    "df.close()\n",
    "en_stop = [t.strip().lower() for t in en_stop]\n",
    "\n",
    "df=open('data/stopwords_id.txt',\"r\",encoding=\"utf-8\", errors='replace')\n",
    "id_stop = df.readlines()\n",
    "df.close()\n",
    "id_stop = [t.strip().lower() for t in id_stop]\n",
    "\n",
    "N = 10\n",
    "print(NLTK_StopWords[:N])\n",
    "print(Sastrawi_StopWords_id[:N])\n",
    "print(en_stop[:N])\n",
    "print(id_stop[:N])\n",
    "print(len(Sastrawi_StopWords_id), len(id_stop), len(NLTK_StopWords), len(en_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Diskusi: Apakah sebaiknya kita menggunakan daftar stopwords bawaan modul atau custom milik kita sendiri?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Tipe variabel memiliki aplikasi optimal yang berbeda-beda, misal\n",
    "L = list(range(10**7))\n",
    "S = set(range(10**7)) # selain unik dan tidak memiliki keterurutan, set memiliki fungsi lain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.1 ms ± 1.15 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "99000000 in L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.1 ns ± 0.5 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "99000000 in S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Tips: selalu rubah list stopwords ke bentuk set, karena di Python jauh lebih cepat untuk cek existence di set ketimbang list\n",
    "NLTK_StopWords = set(NLTK_StopWords)\n",
    "Sastrawi_StopWords_id = set(Sastrawi_StopWords_id)\n",
    "en_stop = set(en_stop)\n",
    "id_stop = set(id_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'adalah' in id_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp tau-data indonesia belajar nlp tau-data indonesia\n"
     ]
    }
   ],
   "source": [
    "# Cara menggunakan stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "T = \"I am doing NLP at tau-data Indonesia,... \\\n",
    "    adapun saya anu sedang belajar NLP di tau-data Indonesia\"\n",
    "T = T.lower()\n",
    "id_stop.add('anu')\n",
    "Tokens = TextBlob(T).words # Tokenisasi \n",
    "T2 = [t for t in Tokens if t not in id_stop] # Sastrawi_StopWords_id Personal_StopWords_en Personal_StopWords_id\n",
    "T2 = [t for t in T2 if t not in en_stop] # Sastrawi_StopWords_id Personal_StopWords_en Personal_StopWords_id\n",
    "print(' '.join(T2))\n",
    "# Catatan: Selalu lakukan Stopword filtering setelah tokenisasi (dan normalisasi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Menangani Slang atau Singkatan di Data Teks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "janjuragan ragu juragan, langsung saja di order pajanjuragannya.\n"
     ]
    }
   ],
   "source": [
    "# Sebuah contoh sederhana \n",
    "T = 'jangan ragu gan, langsung saja di order pajangannya.'\n",
    "# Misal kita hendak mengganti setiap singkatan (slang) dengan bentuk penuhnya. \n",
    "# Dalam hal ini kita hendak mengganti 'gan' dengan 'juragan'\n",
    "H = T.replace('gan','juragan')\n",
    "print(H)\n",
    "# Kita tidak bisa melakukan ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yang'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = {'yg':'yang', 'gan':'juragan'}\n",
    "D['yg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['jangan', 'ragu', 'gan', 'langsung', 'saja', 'di', 'order', 'pajangan', 'yg', 'diatas'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dengan tokenisasi\n",
    "slangs = {'gan':'juragan', 'yg':'yang', 'dgn':'dengan'} #dictionary sederhana berisi daftar singkatan dan kepanjangannya\n",
    "\n",
    "T = 'jangan ragu gan, langsung saja di order pajangan yg diatas.'\n",
    "T = TextBlob(T).words\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jangan ragu juragan langsung saja di order pajangan yang diatas\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(T):\n",
    "    if t in slangs.keys():\n",
    "        T[i] = slangs[t]\n",
    "print(' '.join(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['& : dan\\n',\n",
       " '1pun : satupun\\n',\n",
       " '7an : tujuan\\n',\n",
       " '@ : di\\n',\n",
       " 'Dr : dokter\\n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Slang dan Singkatan dari File\n",
    "# Contoh memuat word fix melalui import file. \n",
    "df=open('data/slang.txt',\"r\",encoding=\"utf-8\", errors='replace')\n",
    "slangS = df.readlines(); df.close()\n",
    "slangS[:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['& : dan', '1pun : satupun', '7an : tujuan', '@ : di', 'Dr : dokter']\n"
     ]
    }
   ],
   "source": [
    "slangS = [t.strip('\\n').strip() for t in slangS]\n",
    "print(slangS[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['luv', 'love']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = 'luv:love'\n",
    "B = A.split(':')\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apa'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A ='  apa  '\n",
    "A.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['&', 'dan'], ['1pun', 'satupun'], ['7an', 'tujuan']]\n",
      "tujuan\n"
     ]
    }
   ],
   "source": [
    "# pisahkan berdasarkan ':'\n",
    "slangS = [t.split(\":\") for t in slangS]\n",
    "slangS = [[k.strip(), v.strip()] for k,v in slangS]\n",
    "print(slangS[:3])\n",
    "slangS = {k:v for k,v in slangS}\n",
    "print(slangS['7an'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love untuk sayang serius juragan tapi tidak tahu kalau besok\n"
     ]
    }
   ],
   "source": [
    "# Test it!\n",
    "tweet = 'I luv u say. serius gan!, tapi ndak tau kalau sesok.'\n",
    "T = TextBlob(tweet).words\n",
    "\n",
    "for i,t in enumerate(T):\n",
    "    if t in slangS.keys():\n",
    "        T[i] = slangS[t]\n",
    "        \n",
    "print(' '.join(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Tujuan Spellcheck:</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>\tCleaning Data\t</li>\n",
    "\t<li>Word suggestions</li>\n",
    "\t<li>OCR/hand writing (Image) recognition</li>\n",
    "\t<li>Speech Recognition</li>\n",
    "\t<li>Machine Translation</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('industry', 1.0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aplikasi spell check di textBlob\n",
    "from textblob import Word\n",
    "\n",
    "w = Word('industri')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('jang4n', 0.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('jang4n')\n",
    "w.spellcheck()\n",
    "# Kendalanya kalau Bahasa Indonesia ==> perlu pendekatan umum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import taudataNlpTm as tau\n",
    "# http://norvig.com/spell-correct.html\n",
    "# corpus = 'data/kata_dasar.txt'\n",
    "print(tau.correction('jang4n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "kombinasi = tau.edits1('benul')\n",
    "print(list(kombinasi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tau.known(tau.edits1('benul'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(tau.known(tau.edits2('benul')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(tau.WORDS.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Language Detection (TextBlob)\n",
    "from textblob import TextBlob\n",
    "T = \"Aku ingin mengerti NLP dalam bahasa Inggris\"\n",
    "U = \"jarene iki boso jowo\"\n",
    "\n",
    "\"\"\"\n",
    "# DEPRECATED\n",
    "print(TextBlob(T).detect_language())\n",
    "print(TextBlob(U).detect_language())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Machine Translation (TextBlob)\n",
    "# Butuh koneksi internet, limited calls. Error otherwise. Need \"try\" and \"catch\".\n",
    "T = \"Aku ingin mengerti NLP dalam bahasa Inggris. I love you\"\n",
    "\"\"\"\n",
    "# DEPRECATED\n",
    "print(TextBlob(T).translate(to='en'))\n",
    "print(TextBlob(T).translate(to='ar-sa'))\n",
    "print(TextBlob(T).translate(to='ja'))\n",
    "\"\"\"\n",
    "# daftar kode bahasa : http://www.cardinalpath.com/resources/tools/google-analytics-language-codes/\n",
    "# Perhatikan TextBlob secara automatis akan mendeteksi bahasa asal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Kalau secara spesifik ingin translate dari suatu bahasa ke bahasa lain:\n",
    "\"\"\"\n",
    "# Deprecated\n",
    "T = \"Aku hanya ingin mengatakan ... saya lapar ... hungry bro\"\n",
    "print(TextBlob(T).translate(from_lang ='id', to='en'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Translation\n",
    "# ini pengganti Fungsi TextBlob diatas\n",
    "import requests\n",
    "\n",
    "def translate(txt_='', from_='en', to_='id'):\n",
    "    headers = {\"Connection\": \"keep-alive\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\"}\n",
    "    url = 'http://translate.google.com/translate_a/t'\n",
    "    params = {\"text\": txt_, \"sl\": from_, \"tl\": to_, \"client\": \"p\"}\n",
    "    terjemahan = requests.get(url, params=params, headers=headers).content\n",
    "    return terjemahan.decode('utf8').replace('\"','').replace(\"'\",\"\").strip()\n",
    "\n",
    "try:\n",
    "    txt = \"mendung tanpo udan\"\n",
    "    f = 'jv'\n",
    "    t = 'id'\n",
    "    translate(txt_=txt, from_=f, to_=t)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Beberapa Reguler Expression yang sering digunakan di NLP/Text Mining</strong></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Menghilangkan/extract email</li>\n",
    "\t<li>Menghilangkan/extract nomer telephone</li>\n",
    "\t<li>Menghilangkan/extract URL di string.</li>\n",
    "\t<li>Alpha Numeric filtering</li>\n",
    "\t<li>Wild Card Search</li>\n",
    "\t<li>Cleaning hashTags di Media Sosial</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting atau replacing eMail.\n",
    "import re\n",
    "emailPattern = re.compile(r'[\\w._%+-]+@[\\w\\.-]+\\.[a-zA-Z]{2,4}')\n",
    "\n",
    "txt = 'Contact kami di admin@nlpindonesia.org, nlp.indonesia@sci.yahoo.co.id, atau nlp_nusantara@internet.net'\n",
    "\n",
    "print( re.sub(emailPattern, ' ', txt) )# clean email\n",
    "eMailS = re.findall( emailPattern, txt )\n",
    "print( 'email yang ditemukan: ', str(eMailS) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Pola telephone : \\d penanda angka di reguler Expression, \\s spasi, dan \"|\" adalah \"atau\"\n",
    "# \"?\" menyatakan pilihan (optional): colou?r sesuai dengan colour atau color.\n",
    "\n",
    "phonePattern = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "txt = 'Contact kami di 021-7634562 atau 021-763-4562 atau 021 763 4562 atau 0822959020 atau +628199258688'\n",
    "print(re.sub(phonePattern,'***',txt))# clean phone\n",
    "    \n",
    "noTelp = re.findall(phonePattern,txt)\n",
    "print('Nomer telephone yang ditemukan: ',str(noTelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Pola telephone 2: untuk setiap angka 8-14 digits dipisahkan oleh \"spasi\", \",\" atau \".\"\n",
    "phonePattern = re.compile(r'\\b\\d{8,14}\\b')\n",
    "txt = 'Contact kami di 082295203040 atau +6282295203040'\n",
    "print(re.sub(phonePattern,'***',txt))# clean phone\n",
    "    \n",
    "noTelp = re.findall(phonePattern,txt)\n",
    "\n",
    "for no in noTelp:\n",
    "    if no[0]!='0':\n",
    "        print('+' + no)\n",
    "    else:\n",
    "        print(no)\n",
    "#print('Nomer telephone yang ditemukan: ',str(noTelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Website URLS http(s) .... untuk ftp trivial\n",
    "urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "txt = 'website reguler expression & my site : https://www.regular-expressions.info/ & https://tau-data.id'\n",
    "print(re.sub(urlPattern,' ',txt))# clean urls\n",
    "\n",
    "URLs = re.findall(urlPattern,txt)# get URLs\n",
    "print('URL yang ditemukan: ',str(URLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# cleaning non alpha-numeric\n",
    "txt = 'Hi! @Mukidi, apa kabar? #sapa_Pagi.'\n",
    "print(re.sub(r'[^\\w]',' ',txt))\n",
    "# atau jika ingin exclude titik dan koma \n",
    "# re.sub(r'[^.,a-zA-Z0-9 \\n\\.]','',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# alternative 2:\n",
    "print(''.join([t for t in txt if t.isalnum() or t==' ' or t=='_']))\n",
    "# ada perbedaan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning hashTags dalam posting media sosial\n",
    "tweet = 'oh IoT, #AndaiSajaIaTahu #ApaYangAkuRasah... #AlayersTweet #d2d'\n",
    "\n",
    "getHashtags = re.compile(r\"#(\\w+)\")\n",
    "print(\"Tags = {0}\".format(re.findall(getHashtags, tweet)))\n",
    "# temukan hanya tags ... perhatikan IoT bukan Tags walau ada huruf besar & kecil dalam satu kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pisahtags = re.compile(r'[A-Z][^A-Z]*')\n",
    "\n",
    "for tags in re.findall(getHashtags, tweet):\n",
    "    print(re.findall(pisahtags, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Mengganti hashtags dengan kata dasar pembentuknya\n",
    "tweet = 'oh IoT, #AndaiSajaIaTahu #ApaYangAkuRasah... #AlayersTweet'\n",
    "tagS = re.findall(getHashtags, tweet)\n",
    "for tag in tagS:\n",
    "    proper_words = ' '.join(re.findall(pisahtags, tag))\n",
    "    tweet = tweet.replace('#'+tag,proper_words)\n",
    "\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "s = 'The25XYZ3abc'\n",
    "re.split('(\\d+)',s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoding-Decoding:\n",
    "\n",
    "<ul>\n",
    "\t<li>Hal berikutnya yang perlu diperhatikan dalam memproses data teks adalah encoding-decoding.</li>\n",
    "\t<li>Contoh Encoding: ASCII, utf, latin, dsb.</li>\n",
    "\t<li>saya membahas lebih jauh tetang encoding disini:&nbsp;<br />\n",
    "\t<a href=\"https://tau-data.id/memahami-string-python/\" target=\"_blank\">https://tau-data.id/memahami-string-python/</a></li>\n",
    "\t<li>Berikut adalah sebuah contoh sederhana tantangan proses encoding-decoding ketika kita hendak memproses data yang berasal dari internet atau media sosial.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# kita bisa menggunakan modul unidecode untuk mendapatkan representasi ASCII terdekat\n",
    "from unidecode import unidecode\n",
    "\n",
    "T = \"ḊḕḀṙ ₲ØĐ, p̾l̾e̾a̾s̾e ḧḕḶṖ ṁḕ\"\n",
    "print(unidecode(T).lower())\n",
    "# Bahasa Indonesia dan Inggris secara umum mampu direpresentasikan dalam encoding ASCII: \n",
    "# https://en.wikipedia.org/wiki/ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Kita juga bisa membersihkan posting media sosial/website dengan entitas html menggunakan fungsi \"unescape\" di modul \"html\"\n",
    "from html import unescape\n",
    "\n",
    "print(unescape('Satu &lt; Tiga&nbsp;&amp; &#169; adalah simbol Copyright'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Latihan 1</strong>:</p>\n",
    "\n",
    "<p>Diberikan&nbsp;tweet berikut:<br />\n",
    "<strong>tweet&nbsp;</strong>=&nbsp; &quot;<em><strong>The #OctopiPower is &amp;gt; Sharks! &amp;amp; they&#39;re awsm! So happy to see them here <a href=\"http://www.octopusVSshark.com\" target=\"_blank\">http://www.octopusVSshark.com</a> !</strong></em>&quot;</p>\n",
    "\n",
    "<p>preprocess tweet diatas sehingga didapatkan tweet seperti ini (Gunakan sembarang modul yang mendukung):<br />\n",
    "<strong>tweet</strong>= &quot;<em><strong>the octopus power is shark they are awesome so happy to see them here</strong></em>&quot;</p>\n",
    "\n",
    "<p><u><strong>Petunjuk/Hints</strong></u>:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Buat satu atau lebih fungsi untuk memudahkan, misal fungsi <em>fixTags </em>dan <em>cleanText</em>.</li>\n",
    "\t<li>fix kata &quot;<em>they&#39;re</em>&quot; dan <em>awsm </em>dengan teknik sederhana <em>dictionary fix</em>.</li>\n",
    "\t<li>Hati-hati terhadap <u><strong>urutan </strong></u>aksi di preprocessing karena akan mengakibatkan hasil yang berbeda.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Coba jawaban Latihan [1] di cell ini: \n",
    "# Salah satu contoh jawaban latihan ini ada di cell paling bawah. \n",
    "# Namun coba untuk tidak melihat jawaban tersebut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Latihan 2</strong>:</p>\n",
    "\n",
    "Filter kata-kata (token) yang terdiri dari huruf dan angka (misal <em>b29nf, _24x_,&nbsp;</em>dsb) pada string berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "T = 'pesawat b29 dan mig276 adalah kepunyaan fhg347x dan _24x_'\n",
    "# Harapan jawaban T = 'pesawat dan adalah kepunyaan dan '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>End of Module NLPTM-01</h1>\n",
    "<hr />\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
