{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><strong><font color=\"blue\">EVDA - Visualisasi Text</font></strong></h1></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/covers/cover_ui-n-taudata.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Modules untuk Notebook ini\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import itertools, re, nltk, pickle\n",
    "import time, numpy as np, matplotlib.pyplot as plt, pandas as pd, seaborn as sns \n",
    "from matplotlib.colors import ListedColormap\n",
    "from textblob import TextBlob\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from unidecode import unidecode\n",
    "from nltk.tag import CRFTagger\n",
    "from html import unescape\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from tqdm import tqdm\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "random_state = 170\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detail Materi:\n",
    "\n",
    "* https://taudata.blogspot.com/2020/04/nlptm-01.html\n",
    "* https://taudata.blogspot.com/2020/04/nlptm-02.html\n",
    "* https://taudata.blogspot.com/2022/04/nlptm-03.html\n",
    "* https://taudata.blogspot.com/2022/04/sma-01.html\n",
    "* https://taudata.blogspot.com/2022/05/EDA-02C-Text-Visualizations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Mining dan NLP?\n",
    "<p>Natural Language Processing (NLP) - Pemrosesan Bahasa Alami (PBA):&nbsp;</p>\n",
    "\n",
    "<p>\n",
    "&quot;<big><em>Sebuah cabang ilmu&nbsp;(AI/Computational Linguistik) yang mempelajari bagaimana&nbsp;bahasa (alami) manusia (terucap/tertulis) dapat dipahami dengan baik oleh komputer dan komputer dapat merespon dengan cara yang serupa ke manusia</em></big>&quot;.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><img alt=\"\" src=\"images/meme-cartoon/jarvis.jpg\" style=\"height: 450px; width: 600px;\" /></p>\n",
    "\n",
    "<p><a href=\"https://www.turn-on.de/lifestyle/topliste/zehn-film-gadgets-die-wir-uns-im-wahren-leben-wuenschen-4413\" target=\"_blank\"><strong>[Image Source]</strong></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Aplikasi Umum NLP:</strong></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Machine Translation (Misal&nbsp;https://translate.google.com/ )</li>\n",
    "\t<li>Information Retrieval (IR)&nbsp;(misal www.google.com, bing, elasticsearch, etc.)</li>\n",
    "\t<li>Man-Machine Interface (misal Siri, cortana, atau Alexa)</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Apakah Perbedaan antara NLP dan Text Mining (TM)?</strong></p>\n",
    "\n",
    "<p>TM (terkadang disebut Text Analytics) adalah sebuah pemrosesan teks (biasanya dalam skala besar) untuk menghasilkan (generate) informasi atau insights. Untuk menghasilkan informasi TM menggunakan beberapa metode, termasuk NLP. TM mengolah teks secara eksplisit, sementara NLP mencoba mencari makna latent (tersembunyi) lewat aturan bahasa (e.g. grammar/idioms/Semantics).<br />\n",
    "<strong>Contoh aplikasi TM</strong> : Clustering, Klasifikasi, Social Media Analytics (SMA).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><img alt=\"https://www.kdnuggets.com/2017/11/framework-approaching-textual-data-tasks.html\" src=\"images/0_NLP_TextMining.jpg\" style=\"height: 470px; width: 600px;\" /></p>\n",
    "\n",
    "<p>[image source: <a href=\"https://www.elsevier.com/books/practical-text-mining-and-statistical-analysis-for-non-structured-text-data-applications/miner/978-0-12-386979-1\" target=\"_blank\">Gary M.:&quot;Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications&quot;</a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><img alt=\"\" src=\"images/Text_Analytics_Field.jpg\" style=\"height: 451px; width: 600px;\" /></p>\n",
    "\n",
    "<p>[Image Source: <a href=\"http://www.pearson.com.au/products/S-Z-Turban-Sharda/Business-Intelligence-and-Analytics-Systems-for-Decision-Support-Global-Edition/9781292009209?R=9781292009209\" target=\"_blank\">Efraim T. &quot;Business Intelligence and Analytics: Systems for Decision Support, Global Edition (10e)</a>&quot;]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi\n",
    "\n",
    "<p>Tokenisasi adalah pemisahan kata, simbol, frase, dan entitas penting lainnya (yang disebut sebagai token) dari sebuah teks untuk kemudian di analisa lebih lanjut. Token dalam NLP sering dimaknai dengan &quot;sebuah kata&quot;, walau tokenisasi juga bisa dilakukan ke kalimat, paragraf, atau entitas penting lainnya (misal suatu pola string DNA di Bioinformatika).</p>\n",
    "\n",
    "<p><strong>Mengapa perlu tokenisasi?</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Langkah penting dalam preprocessing, menghindari kompleksitas mengolah langsung pada string asal.</li>\n",
    "\t<li>Menghindari masalah (semantic) saat pemrosesan model-model natural language.</li>\n",
    "\t<li>Suatu tahapan sistematis dalam merubah unstructured (text) data ke bentuk terstruktur yang lebih mudah di olah.</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images\\2_Pipeline_Tokenization.png\" style=\"height:300px; width:768px\" /><br />\n",
    "[<a href=\"https://www.softwareadvice.com/resources/what-is-text-analytics/\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi dengan <font color=\"blue\"> TextBlob</font>\n",
    "<strong>Kelebihan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Sederhana &amp; mudah untuk digunakan/pelajari.</li>\n",
    "\t<li>Textblob objects punya behaviour/properties yang sama dengan string di Python.</li>\n",
    "\t<li>TextBlob dibangun dari kombinasi modul NLTK dan (Clips) Pattern</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Tidak secepat Spacy dan NLTK</li>\n",
    "\t<li>Language Model terbatas: English, German, French</li>\n",
    "</ol>\n",
    "\n",
    "<p>*Blob : Binary large Object</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr', 'Man', 'He', 'smiled', 'This', 'i.e', 'that', 'is', 'it']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing di TextBlob\n",
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it :-) .\"\n",
    "print(TextBlob(T).words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,',\n",
       " 'Mr.',\n",
       " 'Man.',\n",
       " 'He',\n",
       " 'smiled!!',\n",
       " 'This,',\n",
       " 'i.e.',\n",
       " 'that,',\n",
       " 'is',\n",
       " 'it',\n",
       " ':-)',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, Mr. Man.', 'He smiled!!', 'This, i.e.', 'that, is it :-) .']\n"
     ]
    }
   ],
   "source": [
    "kalimatS = TextBlob(T).sentences\n",
    "print([str(kalimat) for kalimat in kalimatS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saat melakukan coding di Python, selalu perhatikan \"tipe data\" yang dihasilkan oleh modul.\n",
    "A = TextBlob(T).sentences\n",
    "B = TextBlob(T).words\n",
    "print(A[0], type(A[0]))\n",
    "print(B[0], type(B[0]))\n",
    "# Apakah bedanya dengan tipe data str biasa di python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenisasi tidak hanya language dependent, tapi juga environment dependent\n",
    "\n",
    "<p>Tokenization sebenarnya tidak sesederhana memisahkan berdasarkan spasi dan removing symbol. Sebagai contoh dalam bahasa Jepang/Cina/Arab suatu kata bisa terdiri dari beberapa karakter.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/2_Tokenization_Complexity.jpg\" style=\"height:500px; width:686px\" /><br />\n",
    "[<a href=\"http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenisasi (NLP) Bahasa Indonesia:\n",
    "\n",
    "<p>NLTK belum support Bahasa Indonesia, bahkan module NLP Python yang support bahasa Indonesia secara umum masih sangat langka. Beberapa <u><strong>resources </strong></u>yang dapat digunakan:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li><strong><a href=\"https://github.com/kirralabs/indonesian-NLP-resources\" target=\"_blank\">KirraLabs</a></strong>: Mix of NLP-TextMining resources</li>\n",
    "\t<li><strong><a href=\"https://pypi.python.org/pypi/Sastrawi/1.0.1\" target=\"_blank\">Sastrawi 1.0.1</a>:</strong>&nbsp;untuk &quot;stemming&quot; &amp;&nbsp;<strong><a href=\"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\" target=\"_blank\">stopwords&nbsp;</a></strong>bahasa Indonesia.</li>\n",
    "\t<li><strong><a href=\"http://stop-words-list-bahasa-indonesia.blogspot.co.id/2012/09/daftar-kata-dasar-bahasa-indonesia.html\" target=\"_blank\">Daftar Kata Dasar Indonesia</a></strong>:&nbsp;Bisa di load sebagai dictionary di Python</li>\n",
    "\t<li><strong><a href=\"https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata\" target=\"_blank\">Wiktionary</a></strong>: ProyekWiki bahasa Indonesia [termasuk Lexicon]</li>\n",
    "\t<li><a href=\"http://wn-msa.sourceforge.net/\" target=\"_blank\"><strong>WordNet Bahasa Indonesia</strong></a>: Bisa di load&nbsp;sebagai dictionary (atau NLTK<em>*</em>) di Python.</li>\n",
    "\t<li><strong><a href=\"http://kakakpintar.com/daftar-kata-baku-dan-tidak-baku-a-z-dalam-bahasa-indonesia/\" target=\"_blank\">Daftar Kata Baku-Tidak Baku</a></strong>: Bisa di load sebagai dictionary di Python.</li>\n",
    "\t<li><strong><a href=\"https://spacy.io/\" target=\"_blank\">Spacy</a></strong>: Cepat/efisien, MIT License, tapi language model Indonesia masih terbatas.</li>\n",
    "\t<li><a href=\"http://ufal.mff.cuni.cz/udpipe\" target=\"_blank\"><strong>UdPipe</strong></a>: Online request &amp; restricted license (support berbagai bahasa -&nbsp;pemrograman).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><u><big><strong>Word Case</strong></big></u><big> (Huruf BESAR/kecil):</big></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menganalisa makna (<em>semantic</em>) dari suatu (frase) kata dan mencari informasi dalam proses textmining, seringnya (*) kita tidak membutuhkan informasi huruf besar/kecil dari kata&nbsp;tersebut.</li>\n",
    "\t<li><em>Text case normaliation</em> dapat dilakukan pada string secara efisien tanpa melalui tokenisasi (mengapa?).</li>\n",
    "\t<li>Namun, bergantung pada analisa teks yang akan digunakan pengguna harus berhati-hati dengan urutan proses (pipelining) dalam preprocessing. Mengapa dan apa contohnya?</li>\n",
    "</ul>\n",
    "\n",
    "<p>(*) Coba temukan minimal 2 pengecualian dimana&nbsp; huruf kapital/kecil (case) mempengaruhi makna/pemrosesan teks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apa kabar?\n",
      "APA KABAR?\n"
     ]
    }
   ],
   "source": [
    "# Ignore case (huruf besar/kecil)\n",
    "T = \"Apa Kabar?\"\n",
    "print(T.lower())\n",
    "print(T.upper())\n",
    "# Perintah ini sangat efisien karena hanya merubah satu bit di setiap (awal) bytes dari setiap karakter\n",
    "# Sehingga tetap efisien jika ingin dilakukan sebelum tokenisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Morphological-Linguistic Normalization: Stemming &amp; Lemmatization\n",
    "(Canonical Representation)\n",
    "<p><img alt=\"\" src=\"images/meme-cartoon/2_yoda.jpg\" style=\"height:400px; width:400px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Stemming dan Lemma</font>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan konteks kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung konteks.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Mengapa melakukan Stemming &amp; Lemmatisasi</strong>?</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Sering digunakan di IR (Information Retrieval) agar ketika seseorang mencari kata tertentu, maka seluruh kata yang terkait juga diikutsertakan.<br />\n",
    "\tMisal:&nbsp;<em>organize</em>,&nbsp;<em>organizes</em>, and&nbsp;<em>organizing&nbsp;</em>&nbsp;dan&nbsp;<em>democracy</em>,&nbsp;<em>democratic</em>, and&nbsp;<em>democratization</em>.</li>\n",
    "\t<li>Di Text Mining Stemming dan Lemmatisasi akan mengurangi dimensi (mengurangi variasi morphologi), yang terkadang akan meningkatkan akurasi.</li>\n",
    "\t<li>Tapi di IR efeknya malah berkebalikan: <strong><font color=\"blue\">meningkatkan recall, tapi menurunkan akurasi&nbsp;</font></strong>[<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\"><strong>Link</strong></a>]. Contoh: kata&nbsp;<em>operate, operating, operates, operation, operative, operatives, dan operational</em>&nbsp;jika di stem menjadi <em>operate</em>, maka ketika seseorang mencari &quot;<em>operating system</em>&quot;, maka entry seperti&nbsp;<em>operational and research</em> dan&nbsp;<em>operative and dentistry</em>&nbsp;akan muncul sebagai entry dengan relevansi yang cukup tinggi.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raya itu bareng dengan saat kita pergi ke makassar\n",
      "raya pergi suara\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer dengan Sastrawi\n",
    "\n",
    "I = \"perayaan itu Berbarengan dengan saat kita bepergian ke Makassar\"\n",
    "print(stemmer.stem(I))\n",
    "print(stemmer.stem(\"Perayaan Bepergian Menyuarakan\"))\n",
    "# Ada beberapa hal yang berbeda antara Sastrawi dan modul-modul diatas.\n",
    "# Apa sajakah?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Level Normalization: StopWords\n",
    "<p><u>Di Text Mining</u> kata-kata yang <strong>sering muncul </strong>(dan jarang sekali muncul) memiliki sedikit sekali informasi (signifikansi) terhadap model (machine learning) yang digunakan. Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan klasifikasi) atau di semua cluster (di permasalahan pengelompokan/clustering). Kata-kata yang sering muncul ini biasa disebut &quot;StopWords&quot;. Stopwords berbeda-beda bergantung dari Bahasa dan Environment (aplikasi)-nya.<br />\n",
    "<strong>Contoh</strong>:<br />\n",
    "\n",
    "<ul>\n",
    "\t<li>Stopwords bahasa Inggris: am, is, are, do, the, of, etc.</li>\n",
    "\t<li>Stopwords bahasa Indonesia: adalah, dengan, yang, di, ke, dsb</li>\n",
    "\t<li>Stopwords twitter: RT, ...<br />\n",
    "\t<img alt=\"\" src=\"images/2_StopWords.png\" style=\"height:250px; width:419px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'dia', 'dua', 'ia', 'seperti', 'jika', 'jika', 'sehingga', 'kembali', 'dan', 'tidak', 'ini', 'karena', 'kepada', 'oleh', 'saat', 'harus', 'sementara', 'setelah', 'belum', 'kami', 'sekitar', 'bagi', 'serta', 'di', 'dari', 'telah', 'sebagai', 'masih', 'hal', 'ketika', 'adalah', 'itu', 'dalam', 'bisa', 'bahwa', 'atau', 'hanya', 'kita', 'dengan', 'akan', 'juga', 'ada', 'mereka', 'sudah', 'saya', 'terhadap', 'secara', 'agar', 'lain', 'anda', 'begitu', 'mengapa', 'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', 'lagi', 'maka', 'tentang', 'demi', 'dimana', 'kemana', 'pula', 'sambil', 'sebelum', 'sesudah', 'supaya', 'guna', 'kah', 'pun', 'sampai', 'sedangkan', 'selagi', 'sementara', 'tetapi', 'apakah', 'kecuali', 'sebab', 'selain', 'seolah', 'seraya', 'seterusnya', 'tanpa', 'agak', 'boleh', 'dapat', 'dsb', 'dst', 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'tapi', 'ingin', 'juga', 'nggak', 'mari', 'nanti', 'melainkan', 'oh', 'ok', 'seharusnya', 'sebetulnya', 'setiap', 'setidaknya', 'sesuatu', 'pasti', 'saja', 'toh', 'ya', 'walau', 'tolong', 'tentu', 'amat', 'apalagi', 'bagaimanapun']\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "# Loading Stopwords: Ada beberapa cara\n",
    "\n",
    "Sastrawi_StopWords_id = factory.get_stop_words()\n",
    "\n",
    "print(Sastrawi_StopWords_id)\n",
    "print(len(Sastrawi_StopWords_id))\n",
    "\n",
    "Sastrawi_StopWords_id = set(Sastrawi_StopWords_id) # Biasakan hal ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sedang bosan silahkan lari pagi mau sehat\n"
     ]
    }
   ],
   "source": [
    "# Cara menggunakan stopwords\n",
    "\n",
    "T = \"Yang sedang bosan, silahkan lari pagi jika mau sehat\"\n",
    "T = T.lower()\n",
    "Tokens = TextBlob(T).words # Tokenisasi \n",
    "T2 = [t for t in Tokens if t not in Sastrawi_StopWords_id] # Sastrawi_StopWords_id Personal_StopWords_en Personal_StopWords_id\n",
    "print(' '.join(T2))\n",
    "# Catatan: Selalu lakukan Stopword filtering setelah tokenisasi (dan normalisasi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menangani Slang atau Singkatan di Data Teks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "janjuragan ragu juragan, langsung saja di order pajanjuragannya.\n"
     ]
    }
   ],
   "source": [
    "# Sebuah contoh sederhana \n",
    "T = 'jangan ragu gan, langsung saja di order pajangannya.'\n",
    "# Misal kita hendak mengganti setiap singkatan (slang) dengan bentuk penuhnya. \n",
    "# Dalam hal ini kita hendak mengganti 'gan' dengan 'juragan'\n",
    "H = T.replace('gan','juragan')\n",
    "print(H)\n",
    "# Kita tidak bisa melakukan ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saya'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = {'yg':'yang', 'gan':'juragan', 'gue':'saya'}\n",
    "D['gue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['jangan', 'ragu', 'gan', 'langsung', 'saja', 'di', 'order', 'pajangan', 'yg', 'diatas'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dengan tokenisasi\n",
    "slangS = {'gan':'juragan', 'yg':'yang', 'dgn':'dengan', 'u': 'you', 'ndak':'tidak', 'sesok':'besok'} #dictionary sederhana berisi daftar singkatan dan kepanjangannya\n",
    "\n",
    "T = 'jangan ragu gan, langsung saja di order pajangan yg diatas.'\n",
    "T = TextBlob(T.lower()).words\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jangan ragu juragan langsung saja di order pajangan yang diatas\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(T):\n",
    "    if t in slangS.keys():\n",
    "        T[i] = slangS[t]\n",
    "print(' '.join(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I luv you say serius juragan tapi tidak tau kalau besok\n"
     ]
    }
   ],
   "source": [
    "# Test it!\n",
    "tweet = 'I luv u say. serius gan!, tapi ndak tau kalau sesok.'\n",
    "T = TextBlob(tweet).words\n",
    "\n",
    "for i,t in enumerate(T):\n",
    "    if t in slangS.keys():\n",
    "        T[i] = slangS[t]\n",
    "        \n",
    "print(' '.join(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/meme-cartoon/2_regex_meme.jpg\" style=\"height:397px; width:599px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Beberapa Reguler Expression yang sering digunakan di NLP/Text Mining</strong></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Menghilangkan/extract email</li>\n",
    "\t<li>Menghilangkan/extract nomer telephone</li>\n",
    "\t<li>Menghilangkan/extract URL di string.</li>\n",
    "\t<li>Alpha Numeric filtering</li>\n",
    "\t<li>Wild Card Search</li>\n",
    "\t<li>Cleaning hashTags di Media Sosial</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact kami di ****, ****, atau ****\n",
      "email yang ditemukan:  ['admin@nlpindonesia.org', 'nlp.indonesia@sci.yahoo.co.id', 'nlp_nusantara@internet.net']\n"
     ]
    }
   ],
   "source": [
    "# Extracting atau replacing eMail.\n",
    "emailPattern = re.compile(r'[\\w._%+-]+@[\\w\\.-]+\\.[a-zA-Z]{2,4}')\n",
    "\n",
    "txt = 'Contact kami di admin@nlpindonesia.org, nlp.indonesia@sci.yahoo.co.id, atau nlp_nusantara@internet.net'\n",
    "\n",
    "print( re.sub(emailPattern, '****', txt) )# clean email\n",
    "eMailS = re.findall( emailPattern, txt )\n",
    "print( 'email yang ditemukan: ', str(eMailS) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/2_email_regex.gif\" style=\"height:403px; width:687px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact kami di   atau   atau   atau   atau + 88\n",
      "Nomer telephone yang ditemukan:  ['021-7634562', '021-763-4562', '021 763 4562', '0822959020', '6281992586']\n"
     ]
    }
   ],
   "source": [
    "# Pola telephone : \\d penanda angka di reguler Expression, \\s spasi, dan \"|\" adalah \"atau\"\n",
    "# \"?\" menyatakan pilihan (optional): colou?r sesuai dengan colour atau color.\n",
    "\n",
    "phonePattern = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "txt = 'Contact kami di 021-7634562 atau 021-763-4562 atau 021 763 4562 atau 0822959020 atau +628199258688'\n",
    "print(re.sub(phonePattern,' ',txt))# clean phone\n",
    "    \n",
    "noTelp = re.findall(phonePattern,txt)\n",
    "print('Nomer telephone yang ditemukan: ',str(noTelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact kami di *** atau +***\n",
      "082295203040\n",
      "+6282295203040\n"
     ]
    }
   ],
   "source": [
    "# Pola telephone 2: untuk setiap angka 8-14 digits dipisahkan oleh \"spasi\", \",\" atau \".\"\n",
    "phonePattern = re.compile(r'\\b\\d{8,14}\\b')\n",
    "txt = 'Contact kami di 082295203040 atau +6282295203040'\n",
    "print(re.sub(phonePattern,'***',txt))# clean phone\n",
    "    \n",
    "noTelp = re.findall(phonePattern,txt)\n",
    "\n",
    "for no in noTelp:\n",
    "    if no[0]!='0':\n",
    "        print('+' + no)\n",
    "    else:\n",
    "        print(no)\n",
    "#print('Nomer telephone yang ditemukan: ',str(noTelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "website reguler expression & my site :   &  \n",
      "URL yang ditemukan:  ['https://www.regular-expressions.info/', 'https://taudata.id']\n"
     ]
    }
   ],
   "source": [
    "# Website URLS http(s) .... untuk ftp trivial\n",
    "urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "txt = 'website reguler expression & my site : https://www.regular-expressions.info/ & https://taudata.id'\n",
    "print(re.sub(urlPattern,' ',txt))# clean urls\n",
    "\n",
    "URLs = re.findall(urlPattern,txt)# get URLs\n",
    "print('URL yang ditemukan: ',str(URLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi   Mukidi  apa kabar   sapa_Pagi \n"
     ]
    }
   ],
   "source": [
    "# cleaning non alpha-numeric\n",
    "txt = 'Hi! @Mukidi, apa kabar? #sapa_Pagi.'\n",
    "print(re.sub(r'[^\\w]',' ',txt))\n",
    "# atau jika ingin exclude titik dan koma \n",
    "# re.sub(r'[^.,a-zA-Z0-9 \\n\\.]','',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Mukidi apa kabar sapaPagi\n"
     ]
    }
   ],
   "source": [
    "# alternative 2:\n",
    "print(''.join([t for t in txt if t.isalnum() or t==' ']))\n",
    "# ada perbedaan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/meme-cartoon/2_nonStandard_Language.jpg\" style=\"height:359px; width:638px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags = ['AndaiSajaIaTahu', 'ApaYangAkuRasah', 'AlayersTweet', 'd2d']\n"
     ]
    }
   ],
   "source": [
    "# Cleaning hashTags dalam posting media sosial\n",
    "tweet = 'oh IoT, #AndaiSajaIaTahu #ApaYangAkuRasah... #AlayersTweet #d2d'\n",
    "\n",
    "getHashtags = re.compile(r\"#(\\w+)\")\n",
    "print(\"Tags = {0}\".format(re.findall(getHashtags, tweet)))\n",
    "# temukan hanya tags ... perhatikan IoT bukan Tags walau ada huruf besar & kecil dalam satu kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Andai', 'Saja', 'Ia', 'Tahu']\n",
      "['Apa', 'Yang', 'Aku', 'Rasah']\n",
      "['Alayers', 'Tweet']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "pisahtags = re.compile(r'[A-Z][^A-Z]*')\n",
    "\n",
    "for tags in re.findall(getHashtags, tweet):\n",
    "    print(re.findall(pisahtags, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh IoT, Andai Saja Ia Tahu Apa Yang Aku Rasah... Alayers Tweet\n"
     ]
    }
   ],
   "source": [
    "# Mengganti hashtags dengan kata dasar pembentuknya\n",
    "tweet = 'oh IoT, #AndaiSajaIaTahu #ApaYangAkuRasah... #AlayersTweet'\n",
    "tagS = re.findall(getHashtags, tweet)\n",
    "for tag in tagS:\n",
    "    proper_words = ' '.join(re.findall(pisahtags, tag))\n",
    "    tweet = tweet.replace('#'+tag,proper_words)\n",
    "\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding-Decoding:\n",
    "\n",
    "<ul>\n",
    "\t<li>Hal berikutnya yang perlu diperhatikan dalam memproses data teks adalah encoding-decoding.</li>\n",
    "\t<li>Contoh Encoding: ASCII, utf, latin, dsb.</li>\n",
    "\t<li>saya membahas lebih jauh tetang encoding disini:&nbsp;<br />\n",
    "\t<a href=\"https://taufiksutanto.blogspot.co.id/2018/01/pereda-sakit-kepala-urgensi-memahami.html\" target=\"_blank\">https://taufiksutanto.blogspot.co.id/2018/01/pereda-sakit-kepala-urgensi-memahami.html</a></li>\n",
    "\t<li>Berikut adalah sebuah contoh sederhana tantangan proses encoding-decoding ketika kita hendak memproses data yang berasal dari internet atau media sosial.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear god, please help me\n"
     ]
    }
   ],
   "source": [
    "# kita bisa menggunakan modul unidecode untuk mendapatkan representasi ASCII terdekat\n",
    "T = \"ḊḕḀṙ ₲ØĐ, p̾l̾e̾a̾s̾e ḧḕḶṖ ṁḕ\"\n",
    "print(unidecode(T).lower())\n",
    "# Bahasa Indonesia dan Inggris secara umum mampu direpresentasikan dalam encoding ASCII: \n",
    "# https://en.wikipedia.org/wiki/ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satu < Tiga & © adalah simbol Copyright\n"
     ]
    }
   ],
   "source": [
    "# Kita juga bisa membersihkan posting media sosial/website dengan entitas html menggunakan fungsi \"unescape\" di modul \"html\"\n",
    "\n",
    "print(unescape('Satu &lt; Tiga&nbsp;&amp; &#169; adalah simbol Copyright'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Analytics (SMA)\n",
    "\n",
    "<h3 id=\"SMA-adalah-sebuah-proses-pengumpulan-data-dari-media-sosial-dan-analisanya-untuk-mendapatkan-'insights'-atau-informasi-berharga-untuk-suatu-tujuan-tertentu-(definisi-adopted-dari-Gartner*)\">SMA adalah sebuah proses pengumpulan data dari media sosial dan analisanya untuk mendapatkan &#39;insights&#39; atau informasi berharga untuk suatu tujuan tertentu (definisi diadopsi dari Gartner*).</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/8_SMA.JPG\" style=\"width: 600px; height: 304px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Crawling-Data\">Crawling/Scrapping Data</h1>\n",
    "\n",
    "<img alt=\"\" src=\"images/Digital_Media_Crawling_.png\" />\n",
    "\n",
    "## Read more here: https://taudata.blogspot.com/2022/04/sma-01.html\n",
    "\n",
    "* Untuk keperluan ini kita akan menggunakan Twint: https://medium.com/@kevctae/twitter-scraping-without-using-twitter-api-2022-guide-39eaec7ccade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Teks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>&lt;USERNAME&gt; TOLOL!! Gak ada hubungan nya kegug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>Geblek lo tata...cowo bgt dibela2in balikan......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Kmrn termewek2 skr lengket lg duhhh kok labil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Intinya kalau kesel dengan ATT nya, gausah ke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>hadewwwww permpuan itu lg!!!!sakit jiwa,knp ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                               Teks\n",
       "0  negative   <USERNAME> TOLOL!! Gak ada hubungan nya kegug...\n",
       "1  negative  Geblek lo tata...cowo bgt dibela2in balikan......\n",
       "2  negative  Kmrn termewek2 skr lengket lg duhhh kok labil ...\n",
       "3  negative  Intinya kalau kesel dengan ATT nya, gausah ke ...\n",
       "4  negative  hadewwwww permpuan itu lg!!!!sakit jiwa,knp ha..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Data \n",
    "fData = 'data/data-sentimen-indonesia.pckl'\n",
    "try:\n",
    "    with open(fData, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "except:\n",
    "    data1 = pd.read_csv('https://raw.githubusercontent.com/wiweka24/NLP_Sentiment-Analysis/main/Dataset-Sentimen-Analisis-Bahasa-Indonesia/dataset_komentar_instagram_cyberbullying.csv')\n",
    "    data2 = pd.read_csv('https://raw.githubusercontent.com/wiweka24/NLP_Sentiment-Analysis/main/Dataset-Sentimen-Analisis-Bahasa-Indonesia/dataset_tweet_sentimen_tayangan_tv.csv')\n",
    "    data3 = pd.read_csv('https://raw.githubusercontent.com/wiweka24/NLP_Sentiment-Analysis/main/Dataset-Sentimen-Analisis-Bahasa-Indonesia/dataset_tweet_sentiment_cellular_service_provider.csv')\n",
    "    data4 = pd.read_csv('https://raw.githubusercontent.com/wiweka24/NLP_Sentiment-Analysis/main/Dataset-Sentimen-Analisis-Bahasa-Indonesia/dataset_tweet_sentiment_opini_film.csv')\n",
    "    data5 = pd.read_csv('https://raw.githubusercontent.com/wiweka24/NLP_Sentiment-Analysis/main/Dataset-Sentimen-Analisis-Bahasa-Indonesia/dataset_tweet_sentiment_pilkada_DKI_2017.csv')\n",
    "    data1.drop(columns=['Id'], inplace=True)\n",
    "    data1.rename(columns={'Instagram Comment Text': 'Teks'}, inplace=True)\n",
    "    data2.drop(columns=['Id', 'Acara TV', 'Jumlah Retweet'], inplace=True)\n",
    "    data2.rename(columns={'Text Tweet': 'Teks'}, inplace=True)\n",
    "    data3.drop(columns=['Id'], inplace=True)\n",
    "    data3.rename(columns={'Text Tweet': 'Teks'}, inplace=True)\n",
    "    data4.drop(columns=['Id'], inplace=True)\n",
    "    data4.rename(columns={'Text Tweet': 'Teks'}, inplace=True)\n",
    "    data5.drop(columns=['Id', 'Pasangan Calon'], inplace=True)\n",
    "    data5.rename(columns={'Text Tweet': 'Teks'}, inplace=True)\n",
    "    data = pd.concat([data1, data2, data3, data4, data5], ignore_index=True) # Data dijadikan satu\n",
    "    with open(fData, 'wb') as f:  \n",
    "        pickle.dump(data, f)  \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kita hanya akan Gunakan isi \"Teks\" saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'banyak yang masih setia memggunakan <PROVIDER_NAME>'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Teks']]\n",
    "data.at[999,'Teks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pertama-tama kita buat fungsi cleaning text menggunakan topik yang sudah dibahas diatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(txt):\n",
    "    # hashtag Processing\n",
    "    docx = unidecode(unescape(txt.lower().strip()))\n",
    "    urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    docx = re.sub(urlPattern,' ', docx) # Remove links\n",
    "    docx = re.sub(r'[^\\w]', ' ', docx) # Remove Symbol, keep \"_\" ... recommended\n",
    "    Tokens = TextBlob(docx).words # Tokenisasi \n",
    "    docx = ' '.join([t for t in Tokens if str(t) not in Sastrawi_StopWords_id and len(t)>2]) # Stopwords filtering\n",
    "    docx = stemmer.stem(docx)\n",
    "    return docx # Silahkan latihan tambahkan penanganan Slang/Singkatan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potret kepedulian personel Polwan RI #MelindungiMelayaniWarga saat menggelar trauma healing untuk anak-anak terdampak gempa bumi yang berada di posko pengungsian di Cianjur, Jawa Barat. Sinergitas Untuk Negeri #Poldajabar.25  https://t.co/fx7C6ShJ8x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'potret peduli personel polwan melindungimelayaniwarga gelar trauma healing anak anak dampak gempa bumi ada posko ungsi cianjur jawa barat sinergitas negeri poldajabar'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "txt = 'Potret kepedulian personel Polwan RI #MelindungiMelayaniWarga saat menggelar trauma healing untuk anak-anak terdampak gempa bumi yang berada di posko pengungsian di Cianjur, Jawa Barat. Sinergitas Untuk Negeri #Poldajabar.25  https://t.co/fx7C6ShJ8x'\n",
    "print(txt)\n",
    "cleanText(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplikasikan ke semua data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bisa juga pakai fungsi \"apply\" ==> lebih cepat\n",
    "# Sengaja pakai iterrows supaya \"prosesnya terlihat\"\n",
    "\n",
    "if 'cleaned_text' not in data.columns:\n",
    "    data['cleaned_text'] = '' # inititate kolom kosong\n",
    "    for idx, post in tqdm(data.iterrows()):\n",
    "        data.at[idx, 'cleaned_text'] = cleanText(post.Teks)\n",
    "    with open(fData, 'wb') as f:  \n",
    "        pickle.dump(data, f)  \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save hasil text yang sudah dibersihkan ke file Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kalau mau bisa di save ke Folder lain yang lebih mudah ditemukan.\n",
    "data[['cleaned_text']].to_csv(\"cleaned_text.txt\", index=False, encoding='utf8', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualisasi ~ Simple Text Analytics</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Tidak seperti data terstruktur, data tidak terstruktur seperti teks termasuk salah satu data yang cukup sulit untuk divisualisasikan.<br />\n",
    "\t<img alt=\"\" src=\"images/11_charts.jpg\" style=\"height:150px; width:276px\" /></li>\n",
    "\t<li>Namun terdapat Tools seperti Voyant yang dapat membantu dalam visualisasi sekaligus analisis.<br />\n",
    "\t<img alt=\"\" src=\"images/11_voyant.png\" style=\"height:118px; width:426px\" /></li>\n",
    "    <li><a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-1:-WordClouds\">Penggunaan Voyant 1: WordClouds</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.</li>\n",
    "\t<li>slider terms: mengkontrol banyaknya terms yang disertakan.</li>\n",
    "\t<li><strong>Summary </strong>(statistics)</li>\n",
    "\t<li><strong>Documents </strong>==&gt; add more</li>\n",
    "\t<li><strong>Phrases </strong>(n-grams like)</li>\n",
    "\t<li><strong>Export </strong>Visualisasi (kanan atas - pertama)</li>\n",
    "\t<li><strong>Options </strong>(kanan atas ke-3): Font, size, stopwords, whitelist</li>\n",
    "\t<li>&quot;?&quot; ==&gt; More Help</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-2:-Bubbles\">Penggunaan Voyant 2: Bubbles</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>&nbsp;Klik tanda 4-kotak kecil (kanan atas ke-3)</li>\n",
    "\t<li>Pilih Visualization Tools ==&gt; Bubbles</li>\n",
    "\t<li>Option: hanya stopwords</li>\n",
    "\t<li>Export: Hanya PNG</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-3:-Word-Tree\">Penggunaan Voyant 3: Word Tree</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Klik branch untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-2:-Bubbles\">Penggunaan Voyant 4: Links</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Visualization Tools ==&gt; Links</li>\n",
    "\t<li>Klik sembarang terms untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-5:-Trends\">Penggunaan Voyant 5: Trends</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Document Tools ==&gt; Trends</li>\n",
    "\t<li>.. Butuh preprocessing ...&nbsp;</li>\n",
    "\t<li>Data harus terurut waktu</li>\n",
    "\t<li>Berikut contohnya</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>End of Module</h1>\n",
    "\n",
    "<hr />\n",
    "<p><img alt=\"\" src=\"images/meme-cartoon/Computer_feeling.jpg\" style=\"height: 463px; width: 600px;\" /></p>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
