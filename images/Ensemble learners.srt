1
00:00:00,220 --> 00:00:02,830
Creating an ensemble of learners

2
00:00:02,830 --> 00:00:07,030
is one way to make the learners
you've got better.

3
00:00:07,030 --> 00:00:09,940
So we're not talking about
creating a new algorithm, but

4
00:00:09,939 --> 00:00:14,019
instead assembling together
several different algorithms or

5
00:00:14,019 --> 00:00:17,589
several different models to
create an ensemble learner.

6
00:00:17,589 --> 00:00:21,230
One thing I want to emphasize here is
that you can take what you learn here

7
00:00:21,230 --> 00:00:25,670
about ensemble learners and plug it
right in to what you're already doing

8
00:00:25,670 --> 00:00:28,730
with your KNN and
linear regression models.

9
00:00:28,730 --> 00:00:32,000
Now, what we've been doing so far,
is that we've had one kind of

10
00:00:32,000 --> 00:00:37,780
learning method, say KNN, we plug our
data into there and we learn a model.

11
00:00:37,780 --> 00:00:42,670
We can query our model with an X and
it will give us a Y.

12
00:00:42,670 --> 00:00:46,420
So this is not an ensemble learner,
this is just a single learner.

13
00:00:46,420 --> 00:00:49,289
And the idea with ensemble
learners is that we have

14
00:00:49,289 --> 00:00:51,420
several additional learners.

15
00:00:51,420 --> 00:00:55,380
So, we might have a linear regression
based model, we might have a decision

16
00:00:55,380 --> 00:00:59,550
tree based model, we might have
a support vector machine based model.

17
00:00:59,549 --> 00:01:02,809
You could continue this on with any
different number of algorithms.

18
00:01:02,810 --> 00:01:06,269
They're all trained using the same data,
and so now we have,

19
00:01:06,269 --> 00:01:08,709
in this case, four different models.

20
00:01:08,709 --> 00:01:13,879
To query this ensemble of learners,
we query each model by itself and

21
00:01:13,879 --> 00:01:15,259
combine the answers.

22
00:01:15,260 --> 00:01:19,325
So if we wanted to query this model
with X, we plug X into each model,

23
00:01:19,325 --> 00:01:22,680
the same X and then our Ys come out.

24
00:01:22,680 --> 00:01:27,400
So we have a Y output from each of
these models, how do we combine them?

25
00:01:27,400 --> 00:01:30,580
If we're doing classification where for
instance we're trying to identify

26
00:01:30,579 --> 00:01:35,560
what the thing is, we might have
each of these Ys vote on what it is.

27
00:01:35,560 --> 00:01:38,780
But we're doing regression, and so
the typical thing to do here is to

28
00:01:38,780 --> 00:01:43,859
take the mean, and that is the result
for this ensemble learner.

29
00:01:43,859 --> 00:01:46,379
We can then test this
overall ensemble learner

30
00:01:46,379 --> 00:01:48,969
using this test data that we set aside.

31
00:01:48,969 --> 00:01:50,269
Why ensembles?

32
00:01:50,269 --> 00:01:52,539
Why do we use them,
why might they be better?

33
00:01:52,540 --> 00:01:53,910
Well, there's a few reasons.

34
00:01:53,909 --> 00:01:54,890
First of all,

35
00:01:54,890 --> 00:02:01,109
ensembles often have lower error than
any individual method by themselves.

36
00:02:01,109 --> 00:02:04,519
Ensemble learners offer
less overfitting.

37
00:02:04,519 --> 00:02:09,037
The ensemble of learners typically
does not overfit as much as any

38
00:02:09,038 --> 00:02:10,788
individual learner by itself.

39
00:02:10,788 --> 00:02:12,490
Now why is that?

40
00:02:13,689 --> 00:02:15,889
Here's at least an intuitive answer.

41
00:02:15,889 --> 00:02:21,019
As each kind of learner that you
might use has a sort of bias,

42
00:02:21,020 --> 00:02:24,760
it's easiest to talk about that
in terms of linear regression

43
00:02:24,759 --> 00:02:27,199
in terms of what do I mean by bias.

44
00:02:27,199 --> 00:02:32,339
So clearly, with linear regression
our bias is that the data is linear.

45
00:02:33,550 --> 00:02:37,580
KNN has its own kind of bias, decision
trees have their own kind of bias, but

46
00:02:37,580 --> 00:02:43,020
when you put them together you tend
to reduce the biases because they're

47
00:02:43,020 --> 00:02:45,740
fighting against each
other in some sort of way.

48
00:02:45,740 --> 00:02:48,500
Anyways that's what
an ensemble learner is like

49
00:02:48,500 --> 00:02:50,750
if we use multiple types of learners.

