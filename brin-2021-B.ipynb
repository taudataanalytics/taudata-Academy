{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd78d16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"red\">https://s.id/brin-2021-B</font></center>\n",
    "\n",
    "<center><img alt=\"\" src=\"images/brin/brin-2021-covers.png\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">tau-data Indonesia</font><br>(C) Taufik Sutanto - 2021<br>https://tau-data.id</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d902262",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Jadwal Workshop Digital Skill Untuk Kebencanaan BRIN 2021</font></center>\n",
    "\n",
    "### <font color=\"green\">Sesi Siang-Sore</font>\n",
    "5. PreProcessing & Visualisasi Dasar Data Banjir 2021\n",
    "6. Pemodelan Topik & Analisa HashTag Data Bencana Banjir 2021\n",
    "\n",
    "><font color=\"green\">\"*I always have a basic plot outline, but I like to leave some things to be decided while I write*.\" ~ J. K. Rowling</font>\n",
    "\n",
    "<center><img src=\"images/brin/jadwal-brin-2021.png\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd5aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/brin.py\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/Tweets_2021.json\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/tweets_cleaned.txt\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.dic\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/provinsi-latlon-radius.csv\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/kata_dasar.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-ind-def.tab\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-msa-all.tab\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/ind_SA.csv\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
    "    \n",
    "    !pip install sastrawi\n",
    "    !pip install --upgrade spacy python-crfsuite unidecode textblob sklearn-pycrfsuite networkx\n",
    "    !pip install --upgrade unidecode twython tweepy beautifulsoup4 googlemaps pyLDAvis folium gensim wordcloud\n",
    "    !python -m spacy download en\n",
    "    !python -m spacy download xx\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    nltk.download('popular')\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87130b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brin\n",
    "import tweepy, json, urllib.request, requests\n",
    "from urllib.request import Request, urlopen\n",
    "from twython import TwythonStreamer\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns, matplotlib.pyplot as plt, pandas as pd, folium\n",
    "import pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
    "from folium.plugins import MarkerCluster, HeatMap\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a8027",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi\n",
    "\n",
    "<p>Tokenisasi adalah pemisahan kata, simbol, frase, dan entitas penting lainnya (yang disebut sebagai token) dari sebuah teks untuk kemudian di analisa lebih lanjut. Token dalam NLP sering dimaknai dengan &quot;sebuah kata&quot;, walau tokenisasi juga bisa dilakukan ke kalimat, paragraf, atau entitas penting lainnya (misal suatu pola string DNA di Bioinformatika).</p>\n",
    "\n",
    "<p><strong>Mengapa perlu tokenisasi?</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Langkah penting dalam preprocessing, menghindari kompleksitas mengolah langsung pada string asal.</li>\n",
    "\t<li>Menghindari masalah (semantic) saat pemrosesan model-model natural language.</li>\n",
    "\t<li>Suatu tahapan sistematis dalam merubah unstructured (text) data ke bentuk terstruktur yang lebih mudah di olah.</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images\\2_Pipeline_Tokenization.png\" style=\"height:300px; width:768px\" /><br />\n",
    "[<a href=\"https://www.softwareadvice.com/resources/what-is-text-analytics/\" target=\"_blank\"><strong>Image Source</strong></a>]: https://www.softwareadvice.com/resources/what-is-text-analytics/</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec4e41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi tidak hanya language dependent, tapi juga environment dependent\n",
    "\n",
    "<p>Tokenization sebenarnya tidak sesederhana memisahkan berdasarkan spasi dan removing symbol. Sebagai contoh dalam bahasa Jepang/Cina/Arab suatu kata bisa terdiri dari beberapa karakter.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/2_Tokenization_Complexity.jpg\" style=\"height:500px; width:686px\" /><br />\n",
    "[<a href=\"http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc2601",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Contoh tokenisasi menggunakan Spacy\n",
    "from spacy.lang.en import English\n",
    "nlp_en = English()\n",
    "\n",
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "nlp = nlp_en(T)\n",
    "Tokens = [tok.text for tok in nlp]\n",
    "print(Tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e2784",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi (NLP) Bahasa Indonesia:\n",
    "\n",
    "<p>NLTK belum support Bahasa Indonesia, bahkan module NLP Python yang support bahasa Indonesia secara umum masih sangat langka. Beberapa <u><strong>resources </strong></u>yang dapat digunakan:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li><strong><a href=\"https://github.com/kirralabs/indonesian-NLP-resources\" target=\"_blank\">KirraLabs</a></strong>: Mix of NLP-TextMining resources</li>\n",
    "\t<li><strong><a href=\"https://pypi.python.org/pypi/Sastrawi/1.0.1\" target=\"_blank\">Sastrawi 1.0.1</a>:</strong>&nbsp;untuk &quot;stemming&quot; &amp;&nbsp;<strong><a href=\"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\" target=\"_blank\">stopwords&nbsp;</a></strong>bahasa Indonesia.</li>\n",
    "\t<li><strong><a href=\"http://stop-words-list-bahasa-indonesia.blogspot.co.id/2012/09/daftar-kata-dasar-bahasa-indonesia.html\" target=\"_blank\">Daftar Kata Dasar Indonesia</a></strong>:&nbsp;Bisa di load sebagai dictionary di Python</li>\n",
    "\t<li><strong><a href=\"https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata\" target=\"_blank\">Wiktionary</a></strong>: ProyekWiki bahasa Indonesia [termasuk Lexicon]</li>\n",
    "\t<li><a href=\"http://wn-msa.sourceforge.net/\" target=\"_blank\"><strong>WordNet Bahasa Indonesia</strong></a>: Bisa di load&nbsp;sebagai dictionary (atau NLTK<em>*</em>) di Python.</li>\n",
    "\t<li><strong><a href=\"http://kakakpintar.com/daftar-kata-baku-dan-tidak-baku-a-z-dalam-bahasa-indonesia/\" target=\"_blank\">Daftar Kata Baku-Tidak Baku</a></strong>: Bisa di load sebagai dictionary di Python.</li>\n",
    "\t<li><strong><a href=\"https://spacy.io/\" target=\"_blank\">Spacy</a></strong>: Cepat/efisien, MIT License, tapi language model Indonesia masih terbatas.</li>\n",
    "\t<li><a href=\"http://ufal.mff.cuni.cz/udpipe\" target=\"_blank\"><strong>UdPipe</strong></a>: Online request &amp; restricted license (support berbagai bahasa -&nbsp;pemrograman).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1e0e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Contoh Tokenisasi dalam bahasa Indonesia dengan Spacy\n",
    "from spacy.lang.id import Indonesian\n",
    "nlp_id = Indonesian()  # Language Model\n",
    "\n",
    "teks = 'Sore itu, Hamzah melihat kupu-kupu di taman. Ibu membeli oleh-oleh di pasar'\n",
    "nlp = nlp_id(teks)\n",
    "Tokens = [tok.text for tok in nlp]\n",
    "\n",
    "print(Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42372fbb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Jika menggunakan Language model English:\n",
    "nlp = nlp_en(teks)\n",
    "print([tok.text for tok in nlp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f67e8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><u><big><strong>Word Case</strong></big></u><big> (Huruf BESAR/kecil):</big></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menganalisa makna (<em>semantic</em>) dari suatu (frase) kata dan mencari informasi dalam proses textmining, seringnya (*) kita tidak membutuhkan informasi huruf besar/kecil dari kata&nbsp;tersebut.</li>\n",
    "\t<li><em>Text case normaliation</em> dapat dilakukan pada string secara efisien tanpa melalui tokenisasi (mengapa?).</li>\n",
    "\t<li>Namun, bergantung pada analisa teks yang akan digunakan pengguna harus berhati-hati dengan urutan proses (pipelining) dalam preprocessing. Mengapa dan apa contohnya?</li>\n",
    "</ul>\n",
    "\n",
    "<p>(*) Coba temukan minimal 2 pengecualian dimana&nbsp; huruf kapital/kecil (case) mempengaruhi makna/pemrosesan teks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713d6d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore case (huruf besar/kecil)\n",
    "T = \"Hi there!, I am a student. Nice to meet you :)\"\n",
    "print(T.lower())\n",
    "print(T.upper())\n",
    "# Perintah ini sangat efisien karena hanya merubah satu bit di setiap (awal) bytes dari setiap karakter\n",
    "# Sehingga tetap efisien jika ingin dilakukan sebelum tokenisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9a9e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Morphological-Linguistic Normalization: Stemming &amp; Lemmatization\n",
    "(Canonical Representation)\n",
    "<p><img alt=\"\" src=\"images/2_yoda.jpg\" style=\"height:400px; width:400px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5ffee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Stemming dan Lemma</font>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan konteks kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung konteks.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb60e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Mengapa melakukan Stemming &amp; Lemmatisasi</strong>?</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Sering digunakan di IR (Information Retrieval) agar ketika seseorang mencari kata tertentu, maka seluruh kata yang terkait juga diikutsertakan.<br />\n",
    "\tMisal:&nbsp;<em>organize</em>,&nbsp;<em>organizes</em>, and&nbsp;<em>organizing&nbsp;</em>&nbsp;dan&nbsp;<em>democracy</em>,&nbsp;<em>democratic</em>, and&nbsp;<em>democratization</em>.</li>\n",
    "\t<li>Di Text Mining Stemming dan Lemmatisasi akan mengurangi dimensi (mengurangi variasi morphologi), yang terkadang akan meningkatkan akurasi.</li>\n",
    "\t<li>Tapi di IR efeknya malah berkebalikan: <strong><font color=\"blue\">meningkatkan recall, tapi menurunkan akurasi&nbsp;</font></strong>[<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\"><strong>Link</strong></a>]. Contoh: kata&nbsp;<em>operate, operating, operates, operation, operative, operatives, dan operational</em>&nbsp;jika di stem menjadi <em>operate</em>, maka ketika seseorang mencari &quot;<em>operating system</em>&quot;, maka entry seperti&nbsp;<em>operational and research</em> dan&nbsp;<em>operative and dentistry</em>&nbsp;akan muncul sebagai entry dengan relevansi yang cukup tinggi.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174fc4e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Stemming tidak perlu &quot;benar&quot;, hanya perlu konsisten. Sehingga memiliki berbagai variansi, (sebagian) contoh di NLTK:</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4ebb0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Spacy Lemmatizer English\n",
    "# Perhatikan/hati-hati huruf besar/kecil-nya\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "E = \"I am sure apples and oranges are similar\"\n",
    "doc = nlp(E)\n",
    "\n",
    "for token in doc:\n",
    "    print(\"Token = \", token.text, \", Lemmanya = \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e8b13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Lemmatizer dengan Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "I = \"perayaan itu Berbarengan dengan saat kita bepergian ke Padang\"\n",
    "print(stemmer.stem(I))\n",
    "print(stemmer.stem(\"Perayaan Bepergian Menyuarakan\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aacf0be",
   "metadata": {},
   "source": [
    "## Text Level Normalization: StopWords\n",
    "<p><u>Di Text Mining</u> kata-kata yang <strong>sering muncul </strong>(dan jarang sekali muncul) memiliki sedikit sekali informasi (signifikansi) terhadap model (machine learning) yang digunakan. Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan klasifikasi) atau di semua cluster (di permasalahan pengelompokan/clustering). Kata-kata yang sering muncul ini biasa disebut &quot;StopWords&quot;. Stopwords berbeda-beda bergantung dari Bahasa dan Environment (aplikasi)-nya.<br />\n",
    "<strong>Contoh</strong>:<br />\n",
    "\n",
    "<ul>\n",
    "\t<li>Stopwords bahasa Inggris: am, is, are, do, the, of, etc.</li>\n",
    "\t<li>Stopwords bahasa Indonesia: adalah, dengan, yang, di, ke, dsb</li>\n",
    "\t<li>Stopwords twitter: RT, ...<br />\n",
    "\t<img alt=\"\" src=\"images/2_StopWords.png\" style=\"height:250px; width:419px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38161a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=open('data/stopwords_en.txt',\"r\",encoding=\"utf-8\", errors='replace')\n",
    "en_stop = df.readlines()\n",
    "df.close()\n",
    "en_stop = [t.strip().lower() for t in en_stop]\n",
    "\n",
    "df=open('data/stopwords_id.txt',\"r\",encoding=\"utf-8\", errors='replace')\n",
    "id_stop = df.readlines()\n",
    "df.close()\n",
    "id_stop = [t.strip().lower() for t in id_stop]\n",
    "\n",
    "print(en_stop[:7])\n",
    "print(id_stop[:7])\n",
    "id_stop = set(id_stop) # Merubah ke set agar komputasi jauh lebih cepat\n",
    "en_stop = set(en_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a0aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh Cara menggunakan stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "T = \"I am doing NLP at tau-data Indonesia,... \\\n",
    "    adapun saya anu sedang belajar NLP di tau-data Indonesia\"\n",
    "T = T.lower()\n",
    "id_stop.add('anu')\n",
    "\n",
    "Tokens = TextBlob(T).words # Tokenisasi \n",
    "T2 = [t for t in Tokens if t not in id_stop] # Sastrawi_StopWords_id Personal_StopWords_en Personal_StopWords_id\n",
    "T2 = [t for t in T2 if t not in en_stop] # Sastrawi_StopWords_id Personal_StopWords_en Personal_StopWords_id\n",
    "print(' '.join(T2))\n",
    "# Catatan: Selalu lakukan Stopword filtering setelah tokenisasi (dan normalisasi)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b33609",
   "metadata": {},
   "source": [
    "### Preprocessing text Menggunakan fungsi dari tau-data Indonesia ~ Module \"brin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTweets(file='Tweets.json'):\n",
    "    f=open(file,encoding='utf-8', errors ='ignore', mode='r')\n",
    "    T=f.readlines();f.close()\n",
    "    for i,t in enumerate(T):\n",
    "        T[i] = json.loads(t.strip())\n",
    "    return T\n",
    "\n",
    "fileName = 'Tweets_2021.json'\n",
    "T = loadTweets(file=fileName)\n",
    "print(\"{} tweets Loaded from disk\".format(len(T)))\n",
    "print('tweet pertama oleh \"{}\" : \"{}\"'.format(T[0]['user']['screen_name'],T[0]['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "waktu = [t['created_at'] for t in T]\n",
    "usernames = [t['user']['screen_name'] for t in T]\n",
    "tweets = [t['full_text'] for t in T]\n",
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e608e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopId, lemmaId = brin.LoadStopWords(lang='id')\n",
    "slangFixId = brin.loadCorpus(file = 'data/slang.dic', sep=':')\n",
    "\n",
    "cleaned_data = []\n",
    "for tweet in tqdm(tweets):\n",
    "    cleaned_data.append(brin.cleanText(tweet, lemma=lemmaId, lan='id', stops = stopId, fix=slangFixId))\n",
    "    \n",
    "cleaned_data[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5911c2",
   "metadata": {},
   "source": [
    "# Menyimpan Hasil Preprocessing untuk di Visualisasi\n",
    "\n",
    "### Kita simpan juga data yang tidak di preprocessing untuk module D (SNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0439c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ke txt ... lalu nanti akan di upload ke Voyant Tools\n",
    "filename = 'tweets_cleaned.txt'\n",
    "with open(filename, 'w') as f:\n",
    "    for data in cleaned_data:\n",
    "        f.write(data+'\\n')\n",
    "        \n",
    "\"Tweets Saved!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Raw Tweet ke csv\n",
    "import pandas as pd\n",
    "\n",
    "filename = 'RawTweets.csv'\n",
    "df = pd.DataFrame(zip(waktu, usernames, tweets), columns=['created_at', 'user', 'tweet'])\n",
    "df.to_csv(filename, encoding='utf8', index=False)\n",
    "\n",
    "\"Raw Tweets Saved to '{}'\".format(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ca93b",
   "metadata": {},
   "source": [
    "<h2>Text Analytics</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Tidak seperti data terstruktur, data tidak terstruktur seperti teks termasuk salah satu data yang cukup sulit untuk divisualisasikan.<br />\n",
    "\t<img alt=\"\" src=\"images/11_charts.jpg\" style=\"height:150px; width:276px\" /></li>\n",
    "\t<li>Namun terdapat Tools seperti Voyant yang dapat membantu dalam visualisasi sekaligus analisis.<br />\n",
    "\t<img alt=\"\" src=\"images/11_voyant.png\" style=\"height:118px; width:426px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c6feb",
   "metadata": {},
   "source": [
    "<h3 id=\"Voyant-dapat-digunakan-dalam-2-cara:\">Voyant dapat digunakan dalam 2 cara:</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Online</strong>:&nbsp;<a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a><br />\n",
    "\t<u>Kelebihan</u>: Sederhana &amp; portable, tanpa harus install di komputer kita.<br />\n",
    "\t<u>Kekurangan</u>: butuh koneksi internet, tidak cocok untuk data teks yang besar, privacy.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Offline </strong>di komputer kita [Java Based]</p>\n",
    "\t</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be8e38",
   "metadata": {},
   "source": [
    "[2]. Jalankan Voyant secara offline atau online di URL&nbsp;<a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a>\n",
    "\n",
    "[3]. Upload file yang baru saja kita simpan ( **tweets_unp.txt** )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb301046",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-1:-WordClouds\">Penggunaan Voyant 1: WordClouds</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.</li>\n",
    "\t<li>slider terms: mengkontrol banyaknya terms yang disertakan.</li>\n",
    "\t<li><strong>Summary </strong>(statistics)</li>\n",
    "\t<li><strong>Documents </strong>==&gt; add more</li>\n",
    "\t<li><strong>Phrases </strong>(n-grams like)</li>\n",
    "\t<li><strong>Export </strong>Visualisasi (kanan atas - pertama)</li>\n",
    "\t<li><strong>Options </strong>(kanan atas ke-3): Font, size, stopwords, whitelist</li>\n",
    "\t<li>&quot;?&quot; ==&gt; More Help</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6fff9",
   "metadata": {},
   "source": [
    "<h3>Penggunaan Voyant 2: Word Links</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Visualization Tools ==&gt; Links</li>\n",
    "\t<li>Klik sembarang terms untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e128c7a",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-3:-Word-Tree\">Penggunaan Voyant 3: Word Tree</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Klik branch untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767b18d",
   "metadata": {},
   "source": [
    "<h3>Penggunaan Voyant 4: Trends</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Document Tools ==&gt; Trends</li>\n",
    "\t<li>.. Butuh preprocessing ...&nbsp;</li>\n",
    "\t<li>Data harus terurut waktu</li>\n",
    "\t<li>Berikut contohnya</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddb6ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">HashTag Analysis</font></center>\n",
    "\n",
    "* Frequency-Based\n",
    "* Bisa ditambahkan analisa hashtags analysis ini dari waktu ke waktu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b0ce9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "HTfilters = set(['zz', 'architec', 'prize', 'stirli', 'architect', 'london', 'cpd', 'design', 'stirling', 'photogr', 'gemini', \n",
    "                 'mule', 'karaoke', 'playing', 'official', 'berita', 'follow', 'retweet', 'mufc', 'ntms', 'infolimit', 'eeaa', \n",
    "                 'eaa', 'cfc', 'caprico', 'breaking','news', 'libra', 'mereka', 'brankas', 'psikolog', 'aquarius', 'klc'])\n",
    "# modifikasi HTfilters sesuai data kamu\n",
    "HT = {'hashtags':[]}\n",
    "count = 0\n",
    "getHashTags = re.compile(r\"#(\\w+)\")\n",
    "for i, d in tqdm(enumerate(tweets)):\n",
    "    hashtags = re.findall(getHashTags, d)\n",
    "    if hashtags:\n",
    "        TG = []\n",
    "        for tag in hashtags:\n",
    "            dTag = str(tag).strip().lower()\n",
    "            if len(dTag)>2:\n",
    "                add = True\n",
    "                for f in HTfilters:\n",
    "                    if f in dTag:\n",
    "                        add=False; break\n",
    "                if add:\n",
    "                    TG.append('#'+dTag); count += 1\n",
    "            HT['hashtags'].append(TG)\n",
    "print('Found {} number of hashtags'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d0b24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N = 50 # Number of top frequent hashtags to be plotted\n",
    "\n",
    "dtHT = [x for t in tqdm(HT['hashtags']) for x in t] # any(h not in x for h in HTfilters)\n",
    "dtHT = pd.Series(dtHT)\n",
    "dtHT = dtHT.value_counts()\n",
    "dtHT = dtHT.sort_index()\n",
    "dtHT = dtHT.sort_values(ascending = True) \n",
    "dtHT.to_csv('hashTags_unp.csv', encoding='utf8')\n",
    "dtHT = dtHT.iloc[:N]\n",
    "\n",
    "p = dtHT.plot(kind='barh', figsize=(12,8), legend = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8d76c",
   "metadata": {},
   "source": [
    "# Mengambil Data dari Seluruh Provinsi di Indonesia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "file = 'data/provinsi-latlon-radius.csv'\n",
    "propinsi = pd.read_csv(file)\n",
    "propinsi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd220b",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/brin/bencana-indonesia-2020.jpeg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV Propinsi - keywords\n",
    "# API Keys \n",
    "import brin\n",
    "\n",
    "Ck = '' # consumer_key\n",
    "Cs = '' # consumer_secret\n",
    "At = '' # access_token\n",
    "As = '' # access_secret\n",
    "api = (Ck, Cs, At, As)\n",
    "\n",
    "qry = 'banjir OR gempa OR longsor OR tsunami OR kekeringan OR abrasi OR erupsi'\n",
    "tweet_, user_ = brin.getData(qry, N=300, prov=propinsi, lan='id', tKeys=api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"User di data dengan Follower terbanyak:\")\n",
    "user_.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tweet dari user dengan retweet terbanyak\")\n",
    "d_ = tweet_[tweet_['retweet_count'] == tweet_['retweet_count'].max()]\n",
    "print(d_.iloc[0].tweet)\n",
    "d_.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tweet dari user dengan Like terbanyak\")\n",
    "d_ = tweet_[tweet_['favorite_count'] == tweet_['favorite_count'].max()]\n",
    "print(d_.iloc[0].tweet)\n",
    "d_.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topHT = brin.hashTags(tweet_, N=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.countplot(y=tweet_.location, order=pd.value_counts(tweet_.location).iloc[:12].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa586ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bencana = 'banjir gempa longsor tsunami kekeringan abrasi erupsi'.split()\n",
    "count_bencana = []\n",
    "for b in bencana:\n",
    "    for i, t in tweet_.iterrows():\n",
    "        if b in t.tweet.lower():\n",
    "            count_bencana.append(b)\n",
    "            \n",
    "p = sns.countplot(y=count_bencana, order=pd.value_counts(count_bencana).iloc[:len(bencana)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755517a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster, HeatMap\n",
    "\n",
    "def generateBaseMap(default_location=[-0.789275, 113.921], default_zoom_start=5):\n",
    "    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n",
    "    return base_map\n",
    "\n",
    "mp = generateBaseMap()\n",
    "mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gambar HeatMap di Peta , warna beda untuk tiap bencana\n",
    "df_loc = brin.heatmap(tweet_)\n",
    "base_map = generateBaseMap()\n",
    "HeatMap(data=df_loc[['lat', 'lon', 'count']].groupby(['lat', 'lon']).sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(base_map)\n",
    "base_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92cb9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gambar Peta Hashtags di Folium\n",
    "mp = generateBaseMap()\n",
    "ht_pos = brin.tagsMap(df_loc)\n",
    "for propinsi, dt in ht_pos.items():\n",
    "    try:\n",
    "        latlon = dt[:2]\n",
    "        pic = 'data/clouds/{}.png'.format(propinsi)\n",
    "        icon = folium.features.CustomIcon(pic, icon_size=(100, 100))\n",
    "        folium.Marker(latlon, popup=pic[:-4], icon=icon).add_to(mp)\n",
    "    except:\n",
    "        pass\n",
    "mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c5ebc",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "<p><img alt=\"\" src=\"images/4_Document_to_Topics.png\" style=\"height: 300px ; width: 582px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604949c4",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/4_definisi topic model.JPG\" style=\"height:350px; width:809px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20090b7",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/doc-topic-matrix.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f0569",
   "metadata": {},
   "source": [
    "# Referensi Pilihan:\n",
    "\n",
    "* perhitungan Manual Topic Modelling LDA: http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/\n",
    "* http://mimno.infosci.cornell.edu/slides/details.pdf\n",
    "* https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "* http://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf\n",
    "* http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html\n",
    "* Penjelasan intuitif yang baik: https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d \n",
    "* inconjunction dengan interactive program berikut: https://lettier.com/projects/lda-topic-modeling/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff1cb57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pemodelan Topik\n",
    "# Mari kita dalami lebih jauh Topic Pembicaraan ini\n",
    "cleanTweet = brin.cleanTweet(tweet_)\n",
    "tf_w, tm_w, vec_w, ct = brin.getTopic(cleanTweet, Top_Words=30, resume_ = False)\n",
    "pyLDAvis.sklearn.prepare(tf_w, tm_w, vec_w)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c8da5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> Akhir Modul B</font></center>\n",
    "\n",
    "<hr />\n",
    "<img alt=\"\" src=\"images/meme-cartoon/.jpg\" />"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
