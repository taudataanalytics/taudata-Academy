{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> http://bit.ly/unsri-pds-2021-B\n",
    "\n",
    "<center><img alt=\"\" src=\"images/covers/cover_unsri_PDS_2021.jpg\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">tau-data Indonesia</font><br>(C) Taufik Sutanto - 2021</center>\n",
    "<center><a href=\"https://tau-data.id\">https://tau-data.id</a> ~ <a href=\"mailto:taufik@tau-data.id\">taufik@tau-data.id</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/unsri2021/jadwal-pds-2021.png\" /></center>\n",
    "* Diskusi dapat dilakukan di sembarang waktu, tidak perlu menunggu dan dapat dilakukan via chat.\n",
    "\n",
    "><font color=\"green\">\"*You don’t have to be great to start, but you have to start to be great*.” –zig ziglar</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Referensi & Resources:</font></center>\n",
    "\n",
    "* Pengenalan Python: https://tau-data.id/adsp/ & https://tau-data.id/hpds/\n",
    "* Python basic: https://www.python-course.eu/python3_history_and_philosophy.php \n",
    "* Data Science Basic: https://tau-data.id/dsbd/ & https://scikit-learn.org/stable/tutorial/index.html\n",
    "* Advanced Python: http://andy.terrel.us/blog/2012/09/27/starting-with-python/\n",
    "* Visualisasi di Python: https://matplotlib.org/gallery.html\n",
    "* Studi Kasus: https://www.kaggle.com/\n",
    "\n",
    "<img alt=\"\" src=\"images/tau-data_banner_large.jpg\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Outline EDA & Predictive Maintenance</font></center>\n",
    "\n",
    "* EDA: Preprocessing, Visualisasi, Hipotesis, & Interpretasi\n",
    "* Predictive Maintenance: Feature Engineering & Deep Learning\n",
    "\n",
    "><font color=\"green\">\"*I always have a basic plot outline, but I like to leave some things to be decided while I write*.\" ~ J. K. Rowling</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/covers/Cover_EDA.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/utils.py\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/CMAPSS_Data_train_FD004.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/CMAPSS_test_FD004.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/CMAPSS_RUL_FD004.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/RLE_PM_train.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/RLE_PM_test.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/RLE_PM_truth.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/price.csv\n",
    "    !pip install featuretools==0.16.0 \n",
    "    !pip install composeml==0.7.0\n",
    "    !pip install --upgrade pandas\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP: Jika menggunakan Google Colab, klik tombol \"reset runtime\" di Cell diatas sebelum melanjutkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Modules\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import pandas as pd, matplotlib.pyplot as plt, seaborn as sns, numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from collections import Counter\n",
    "plt.style.use('bmh'); sns.set()\n",
    "\n",
    "# Importing CSV data  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html    \n",
    "price = pd.read_csv('data/price.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studi Kasus\n",
    "\n",
    "* Misal seorang Data Scientist Bekerja di sebuah perusahaan properti.\n",
    "* Sumber Data: http://byebuyhome.com/\n",
    "* Variable:\n",
    " - **Dist_Taxi** – distance to nearest taxi stand from the property\n",
    " - **Dist_Market** – distance to nearest grocery market from the property\n",
    " - **Dist_Hospital** – distance to nearest hospital from the property\n",
    " - **Carpet** – carpet area of the property in square feet\n",
    " - **Builtup** – built-up area of the property in square feet\n",
    " - **Parking** – type of car parking available with the property\n",
    " - **City_Category** – categorization of the city based on the size\n",
    " - **Rainfall** – annual rainfall in the area where property is located\n",
    " - **House_Price** – price at which the property was sold\n",
    "\n",
    "<img alt=\"\" src=\"images/Regression-model.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisa:\n",
    "\n",
    "1. Ada komentar dari observasi sekilas dari data?\n",
    "2. Informasi lebih lanjut apakah yang dibutuhkan seorang Data Analyst/Scientist untuk memahami data ini?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistika Sederhana dari data \"Numerik\"-nya\n",
    "price.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Dasar\n",
    "price.drop(\"Observation\", axis=1, inplace=True)\n",
    "price.drop_duplicates(inplace=True)\n",
    "price['Parking'] = price['Parking'].astype('category')\n",
    "price['City_Category'] = price['City_Category'].astype('category')\n",
    "price2 = price[np.abs(price.House_Price - price.House_Price.mean())<=(2*price.House_Price.std())]\n",
    "price2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistika Sederhana dari data \"Numerik\"-nya\n",
    "price2.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apakah ada kecenderungan perbedaan harga rumah akibat dari tipe tempat parkir?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T01:01:50.484583Z",
     "start_time": "2019-09-30T01:01:50.098828Z"
    }
   },
   "outputs": [],
   "source": [
    "p= sns.catplot(x=\"Parking\", y=\"House_Price\", data=price2)\n",
    "# Apa yang bisa dilihat dari hasil ini?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tambah dimensi di Visualisasi untuk melihat insight yang lebih jelas/baik "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-30T01:01:53.025889Z",
     "start_time": "2019-09-30T01:01:52.038530Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Bisa juga plot dengan informasi dari 3 variabel sekaligus\n",
    "# (untuk melihat kemungkinan faktor interaksi)\n",
    "p= sns.catplot(x=\"Parking\", y=\"House_Price\", hue=\"City_Category\", kind=\"swarm\", data=price2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Tips Restaurant\n",
    "\n",
    "Sebuah dataset dari suatu Restaurant memuat variabel-variabel berikut:\n",
    "*\ttotal_bill: Total bill (cost of the meal), including tax, in US dollars\n",
    "*\ttip: Tip (gratuity) in US dollars\n",
    "*\tsex: Sex of person paying for the meal (0=male, 1=female)\n",
    "*\tsmoker: Smoker in party? (0=No, 1=Yes)\n",
    "*\tday: 3=Thur, 4=Fri, 5=Sat, 6=Sun\n",
    "*\ttime: 0=Day, 1=Night\n",
    "*\tsize: Size of the party\n",
    "*   **Sumber Data**: Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing\n",
    "*   **Link ke data**: https://www.kaggle.com/jsphyg/tipping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips=sns.load_dataset('tips')\n",
    "categorical = tips.select_dtypes(include = ['category']).columns\n",
    "fig, ax = plt.subplots(2, 2, figsize=(20, 10))\n",
    "for variable, subplot in zip(categorical, ax.flatten()):\n",
    "    sns.countplot(tips[variable], ax=subplot)\n",
    "    \n",
    "print(\"Ada insight apa saja dari hasil ini?\")\n",
    "print(\"Lalu apa rekomendasi kita terhadap insight/informasi tersebut?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika ada outlier grafiknya menjadi tidak jelas (data = price, bukan price2)\n",
    "p = sns.boxplot(x=\"House_Price\", y=\"Parking\", data=price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoxPlots\n",
    "p = sns.boxplot(x=\"House_Price\", y=\"Parking\", data=price2)\n",
    "# Apa makna pola yang terlihat di data oleh BoxPlot ini?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.catplot(x=\"Parking\", y=\"House_Price\", hue=\"City_Category\", kind=\"box\", data=price2)\n",
    "print(\"Data scientist melayani kebutuhan data client.\")\n",
    "print(\"Misal dalam hal ini variasi investasi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> Outline Model Predictive Maintenance</font></center>\n",
    "\n",
    "* Pendahuluan Model Prediksi Kerusakan Komponen (predictive Component)\n",
    "* Pendekatan Stasioner\n",
    "* Pendekatan Time Series\n",
    "* Contoh Penerapan:\n",
    " - Feature Tools\n",
    " - Deep Learning - CNN\n",
    "* Diskusi\n",
    "\n",
    "<p><center><img src=\"images/predictive_maintenance_outline_pic.jpg\" alt=\"\" width=\"500\" height=\"238\" /></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> References </font></center>\n",
    "\n",
    "**Disclaimer**: Beberapa informasi di Module ini adalah kutipan langsung dan-atau tidak langsung dari berbagai sumber berikut (dan tambahan sumber yang diberikan pada setiap cell):\n",
    "\n",
    "* https://towardsdatascience.com/how-to-implement-machine-learning-for-predictive-maintenance-4633cdbe4860\n",
    "* https://medium.com/swlh/machine-learning-for-equipment-failure-prediction-and-predictive-maintenance-pm-e72b1ce42da1\n",
    "* https://github.com/Featuretools/predict-remaining-useful-life/blob/master/Simple%20Featuretools%20RUL%20Demo.ipynb\n",
    "* https://towardsdatascience.com/remaining-life-estimation-with-keras-2334514f9c61\n",
    "* https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/\n",
    "* https://www.kaggle.com/r17sha/mtp-cmapps\n",
    "\n",
    "# <center><font color=\"red\"> Mohon untuk melakukan hal ini terlebih dahulu </font></center>\n",
    "\n",
    "### Runtime ==> Change Runtime Type: **GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Equipment Failure Prediction (EFP) and Preventive Maintenance (PM)</font></center>\n",
    "\n",
    "* **Tujuan**: Data dikumpulkan dari waktu ke waktu untuk mengawasi/monitor keadaan dari suatu komponen/alat. Tujuan dari EFP-PM adalah menemukan pola yang akan dapat membantu prediksi/peramalan terkait pencegahan kerusakan komponen/alat.\n",
    "* **Contoh Aplikasi**:\n",
    "\n",
    "<img src=\"images/contoh_aplikasi_predictive_maintenance.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Preventive Maintenance (PM)</font></center>\n",
    "\n",
    "* Sumber: https://www.getmaintainx.com/blog/what-is-preventative-maintenance/\n",
    "        \n",
    "<img src=\"images/preventive-maintenance-benefits-chart.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Remaining Useful Life - RUL</font></center>\n",
    "\n",
    "*  SOH: State of Health.\n",
    "\n",
    "<img src=\"images/Remaining Useful Life - RUL.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Pendekatan matematis VS ML: Apa Bedanya?</font></center>\n",
    "\n",
    "<center><img src=\"images/math_vs_ML_in_predictive_maintenance.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Reactive VS Preventive</font></center>\n",
    "\n",
    "<center><img src=\"images/Reactive VS Preventive.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Keputusan Strategis</font></center>\n",
    "\n",
    "* Biaya karena kerusakan/kegagalan biasanya lebih besar.\n",
    "* Maintenance rutin baik, namun terkadang terlalu cepat atau terlambat. Sehingga tidak optimal dari segi biaya.\n",
    "* Baseline AI-nya berarti dengan biaya maintenance saat ini.\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_plus_ML.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Benefit - Keuntungan</font></center>\n",
    "\n",
    "* EFP-PM akan meminimumkan under/over maintenance, meningkatkan mutu layanan, meminimalkan resiko (hazard) akibat kerusakan alat, dsb.\n",
    "* EFP-PM yang baik dapat menghemat biaya hingga jutaan dolar pada industri yang besar.\n",
    "* Source: https://medium.com/swlh/machine-learning-for-equipment-failure-prediction-and-predictive-maintenance-pm-e72b1ce42da1\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_cost_benefit.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Data, Data, Data</font></center>\n",
    "\n",
    "* Kita perlu data untuk memahami kerusakan. Data yang bermanfaat bisa berupa data statis/stasioner seperti cara kerja, rata-rata umur, atau \"operating condition\". Secara umum lebih banyak data tersedia lebih baik.\n",
    "* Data \"fine-grained\" yang dimonitor dalam interval waktu yang pendek lebih memungkinkan untuk digunakan sebagai prediksi.\n",
    "* Untuk menentukan data yang penting/relevan maka Data Scientist dan ahli (domain knowledge) harus bersama-sama mendiskusikannya.\n",
    "* Garbage-in-Garbage out: keep the data clean.\n",
    "* Data & Feature Engineering akan menjadi kunci utama.\n",
    "\n",
    "<center><img src=\"images/manufacturing_data.jpg\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Pendekatan Utama Predictive Maintenance Dasar: Regresi & Klasifikasi</font></center>\n",
    "\n",
    "1. Regression models to predict remaining useful lifetime (RUL)\n",
    "2. Classification models to predict failure within a given time window\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_models.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Regression Approach to Predictive Maintenance</font></center>\n",
    "\n",
    "* \"Regression\": Tidak selalu OLS, tapi bisa juga Regression tree, SVR, Deep Learning, dsb.\n",
    "* Digunakan untuk memprediksi RUL: memprediksi variabel target yang bertipe numerik.\n",
    "* Data statis dan time series (historical) serta label dibutuhkan.\n",
    "* Satu model hanya digunakan untuk satu tipe failure. Jika ada beberapa kemungkinan failure harus dibuat beberapa model.\n",
    "\n",
    "<center><img src=\"images/Regression models to predict remaining useful lifetime.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Regression Approach to Predictive Maintenance 2</font></center>\n",
    "\n",
    "* **Source**: https://github.com/Featuretools/predict-remaining-useful-life/blob/master/Simple%20Featuretools%20RUL%20Demo.ipynb\n",
    "* **Data**: https://github.com/Featuretools/predict-remaining-useful-life/blob/master/Simple%20Featuretools%20RUL%20Demo.ipynb\n",
    "* **Data**: operational_settings and sensor_measurements recorded for each cycle\n",
    "* **Model**: RandomForestRegressor \n",
    "* **Method**: Heavily rely on feature engineering via featuretools.\n",
    "\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_feature_tools.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Predict failure within a given time window</font></center>\n",
    "\n",
    "* Model multi klasifikasi yang dapat memprediksi kerusakan/kegagalan pada suatu interval waktu (diskrit)\n",
    "* Karena waktu di diskritisasi maka pendekatan dan asumsi menyesuaikan. Misal kita tidak bisa menggunakan pola sinyal tertentu yang dihasilkan output komponen/alat ketika ia akan rusak.\n",
    "* Biasanya cocok untuk permasalahan dimana kerusakan bersifat gradual/perlahan.\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_classification.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Deep Learning Approach 01</font></center>\n",
    "\n",
    "* **Source**: https://towardsdatascience.com/system-failure-prediction-using-log-analysis-8eab84d56d1\n",
    "* **Data**: RAM, CPU and Hard Disk utilization\n",
    "* **Model**: RNN-LSTM\n",
    "* **Method**: Metode reduksi dimensi (PCA) diikuti dengan penggunaan nilai ambang/threshold (telah dijelaskan Fathu di sesi sebelumnya).\n",
    "\n",
    "<center><img src=\"images/deep_learning_predictive_maintenance_PCA_LSTM.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Deep Learning Anomaly-Detection Based</font></center>\n",
    "\n",
    "* Time Series + Threshold. Triknya pada pemilihan atau rekayasa feature/variabel.\n",
    "* Gambar memperlihatkan penurunan performa mesin/alat/komponen (garis biru) terhadap nilai acuan 9garis merah). Dengan menggunakan nilai ambang (misal ~35) maka kita bisa menggunakan model time series untuk memprediksi bahwa kerusakan akan terjadi sekitar 9 hari lagi.\n",
    "* Bisa untuk Latihan, menggunakan LSTM dengan berbagai time step (window): https://tau-data.id/lstm/\n",
    "* Link referensi: https://www.kaggle.com/r17sha/mtp-cmapps\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_via_anomaly_detection.jpg\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Deep Learning Approach 02</font></center>\n",
    "\n",
    "* **Source**: https://towardsdatascience.com/remaining-life-estimation-with-keras-2334514f9c61\n",
    "* **Data**: Turbofan Engine Degradation Simulation Dataset NASA\n",
    "* **Model**: Convolutional Neural network - CNN\n",
    "* **Method**: Merubah Data TIme-Series menjadi Image, lalu menggunakan kelebihan Deep Learning dalam menemukan pola pada image.\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_cnn.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Contoh Penerapan 01: Feature Tools</font></center>\n",
    "\n",
    "* **Source**: https://github.com/Featuretools/predict-remaining-useful-life/blob/master/Simple%20Featuretools%20RUL%20Demo.ipynb\n",
    "* **Data**: https://github.com/Featuretools/predict-remaining-useful-life/blob/master/Simple%20Featuretools%20RUL%20Demo.ipynb\n",
    "* **Data**: operational_settings and sensor_measurements recorded for each cycle\n",
    "* **Model**: RandomForestRegressor \n",
    "* **Method**: Memanfaatkan struktur data khusus dimana transformasinya (feature engineering) dibantu module  featuretools.\n",
    "\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_feature_tools.png\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import numpy as np, pandas as pd, os, utils\n",
    "import featuretools as ft, composeml as cp\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"TensorFlow version = \", tf.__version__)\n",
    "if tf.test.is_built_with_cuda():\n",
    "    physical_devices = tf.config.list_physical_devices('GPU') \n",
    "    print(\"CUDA enabled TF, Num GPUs:\", len(physical_devices), physical_devices) \n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "    except Exception as err_:\n",
    "        print(err_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load/Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "file_ = \"data/CMAPSS_Data_train_FD004.txt\"\n",
    "\n",
    "index_names = ['unit_nr', 'time_cycles']\n",
    "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensor_names = ['s_{}'.format(i) for i in range(1,22)]\n",
    "col_names = index_names + setting_names + sensor_names\n",
    "data = utils.load_data(file_)\n",
    "print(data.shape)\n",
    "data.sample(5).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Data Understanding ~ Metadata</font></center>\n",
    "\n",
    "* Turbofan Engine Degradation Simulation Dataset, disediakan oleh NASA\n",
    "* Dataset ini penting di dunia data science/ML, sebagai benchmark berbagai model prediksi Remaining Useful Life (RUL) banyaknya sisa cycles pada suatu mesin sebelum membutuhkan maintenance. Datanya berasal dari fleet of engines (mesin pesawat) bertipe sama (249 engines / engine_no). \n",
    "* Variabel (time series): 3 operational settings dan 21 pengukuran sensor, serta **cycle**. Cycle adalah ukuran \"umur/usia\" komponen dalam suatu satuan tertentu.\n",
    "* Diasumsikan mesin beroperasi dengan normal pada awal setiap siklus dan suatu ketika akan rusak. Di data training kerusakan bertambah hingga system terhenti. Tujuan dari permasalhannya adalah memprediksi sisa cycle sebelum rusak di \"Data Test\". Atau dengan kata lain tujuannya banyaknya cycle operational setelah nilai cycle saat ini.\n",
    "* **Dataset ini \"spesial\"** karena mesin berjalan terus hingga terjadi kerusakan, dengan begini kita mendapatkan nilai/informasi RUL yang baik pada setiap mesin pada setiap waktu.\n",
    "* Pemodelannya **memprediksi RUL** dengan input prediksinya adalah sembarang waktu dan menggunakan informasi sebelum titik waktu tersebut untuk melakukan prediksi.\n",
    "* Variabel **Cutoff_times**/ambang/threshold bisa dihasilkan lewat module **Featuretools**. \n",
    "* Untuk membentuk struktur datanya kita akan menggunakan module **Compose* yang biasa digunakan untuk **secara automatis menghasilkan labels dengan cutoff_times**.\n",
    "\n",
    "<center><img src=\"images/nasa Turbofan Engine Degradation Simulation.png\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define Labeling Function RUL yang pada contoh ini hanyalah \"jumlah Baris df - 1\"\n",
    "def remaining_useful_life(df):\n",
    "    return len(df) - 1\n",
    "\n",
    "# Contoh dengan Toy Data apa yg akan dilakukan fungsi RUL diatas\n",
    "D = {'Name' : ['Ankit', 'Aishwarya', 'Shaurya', 'Shivangi'],\n",
    "    'Age' : [23, 21, 22, 21],\n",
    "    'University' : ['BHU', 'JNU', 'DU', 'BHU']} \n",
    "df = pd.DataFrame(D)\n",
    "print(\"Jumlah Baris df = {}\".format(df.shape[0]))\n",
    "print(\"Hasil fungsi 'remaining_useful_life(df)' = \", remaining_useful_life(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Memahami Module composeML</font></center>\n",
    "\n",
    "<center><img src=\"images/composeML.png\" /></center>\n",
    "\n",
    "* **Compose** adalah sebuah tool machine learning untuk keperluan prediksi. Fungsi utamanya adalah merubah struktur masalah prediksi dan menghasilkan \"label\" untuk ML (supervised learning).\n",
    "* Untuk memodifikasi label, pengguna menggunakan sebuah *labelling function*. \n",
    "* Setelah mendefinisikan fungsi labelling kita kan melakukan \"**search**\" yang akan secara automatis meng-ekstrak training data dari data historis yang ada.\n",
    "* Hasil dari ComposeML akan menjadi input module **FeatureTools** (lihat gambar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Mengingat kembali kolom yang ada di data kita.\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">ComposeML Label Maker</font></center>\n",
    "\n",
    "* target entity: Primary key objek di data: misal id komponen\n",
    "* Variabel waktu di data apa namanya? : \"time\" menjadi value bagi parameter \"time_index\"\n",
    "* Fungsi labelling = seperti yang sudah didefinisikan diatas. Berarti ini menjadi nilai variabel targetnya.\n",
    "* Default-nya **window_size** adalah ukuran total observasi setiap mesin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lm = cp.LabelMaker(\n",
    "    target_entity='engine_no',\n",
    "    time_index='time',\n",
    "    labeling_function=remaining_useful_life,) # window_size=\"1h\",\n",
    "\n",
    "type(lm) # Struktur data khusus ComposeML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">ComposeML Search Labels</font></center>\n",
    "\n",
    "### Data Labelling automatis menggunakan Threshold dan \"Compose\"\n",
    "\n",
    "* Misal kita akan memprediksi mesin (turbin) yang sedang beroperasi.\n",
    "* Misal diketahui juga bahwa suatu mesin (turbin) biasanya tidak rusak sebelum 120 cycle, maka kita bisa memfilter data untuk mesin yang setidaknya mencapai 100 cycle. \n",
    "* **Bagaimana dengan yang tidak sampai 100?**\n",
    "* Untuk melakukan filtering diatas parameter **minimum_data** di set ke nilai 100.\n",
    "* Dengan menggunakan parameter **num_examples_per_instance = 1**, kita mengatur batas search satu untuk setiap mesin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_times = lm.search(\n",
    "    data.sort_values('time'),\n",
    "    num_examples_per_instance=1,\n",
    "    minimum_data=100,\n",
    "    verbose=True,)\n",
    "\n",
    "label_times.head() # label times adalah dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(label_times['engine_no']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "* In the third row, we have engine number 3. At 00:00 on January 6, the remaining useful life of engine number 3 is 206. \n",
    "* Having a dataframe in this format tells Featuretools that the feature vector for engine number 3 should only be calculated with data from before that point in time.\n",
    "\n",
    "## Deep Feature Synthesis\n",
    "\n",
    "* To apply Deep Feature Synthesis we need to establish an EntitySet structure for our data. The key insight in this step is that we're really interested in our data as collected by engine. We can create an engines entity by normalizing by the engine_no column in the raw data. In the next section, we'll create a feature matrix for the engines entity directly rather than the base dataframe of recordings.\n",
    "\n",
    "## Masih belum jelas? Mari kita slice data awal: ambil data hanya dari \"Engine 1\"\n",
    "* Kunci untuk memahami struktur datanya terletak pada variabel \"time_in_cycles\" terutama melihat nilainya di awal dan akhir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "engineTiga = data[data[\"engine_no\"] == 3]\n",
    "print(\"Banyak baris data Engine 3 = {} baris x {} kolom\".format(engineTiga.shape[0], engineTiga.shape[1]))\n",
    "engineTiga.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "engineTiga.tail().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dari sini kita dapat memahami kenapa RUL Mesin 3 206 begitu pula nilai lainnya.\n",
    "\n",
    "### 206 = (307 - 100) - 1 \n",
    "\n",
    "* 307 dari data time_in_cycles tertinggi, 100 dari nilai parameter \"minimum_data\", dan minus 1 dari fungsi remaining_useful_life. Atau bagi fungsi remaining_useful_life parameter input df-nya adalah sub-dataframe engine 3 yang > minimum_data.\n",
    "* Sebagai latihan bisa verifikasi dengan data di Engine 1 atau mengganti nilai minimum_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_times.head() # label times adalah dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_entityset(data):\n",
    "    es = ft.EntitySet('Dataset')\n",
    "    es.entity_from_dataframe(\n",
    "        dataframe=data,\n",
    "        entity_id='recordings',\n",
    "        index='index',\n",
    "        time_index='time',)\n",
    "    es.normalize_entity(\n",
    "        base_entity_id='recordings',\n",
    "        new_entity_id='engines',\n",
    "        index='engine_no',)\n",
    "    es.normalize_entity(\n",
    "        base_entity_id='recordings',\n",
    "        new_entity_id='cycles',\n",
    "        index='time_in_cycles',)\n",
    "    return es\n",
    "\n",
    "es = make_entityset(data)\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "es.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFS and Creating a Model\n",
    "\n",
    "* With the work from the last section in hand, we can quickly build features using Deep Feature Synthesis (DFS). The function ft.dfs takes an EntitySet and stacks primitives like Max, Min and Last exhaustively across entities. Feel free to try the next step with a different primitive set to see how the results differ!\n",
    "\n",
    "* We build features only using data up to and including the cutoff time of each label. This is done by setting the cutoff_time parameter to the label times we generated previously. Notice that the output of Compose integrates easily with Featuretools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm, features = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_entity='engines',\n",
    "    agg_primitives=['last', 'max', 'min'],\n",
    "    trans_primitives=[],\n",
    "    cutoff_time=label_times,\n",
    "    max_depth=3,\n",
    "    verbose=True,)\n",
    "\n",
    "fm.to_csv('simple_fm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fm.shape)\n",
    "fm.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop dan mari kita pahami Bentuk Datanya terlebih Dahulu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Baselines\n",
    "\n",
    "Before we use that feature matrix to make predictions, we should check how well guessing does on this dataset. We can use a train_test_split from scikit-learn to split our training data once and for all. Then, we'll check the following baselines:\n",
    "\n",
    "1. Always predict the median value of y_train\n",
    "2. Always predict the RUL as if every engine has the median lifespan in X_train\n",
    "\n",
    "We'll check those predictions by finding the mean of the absolute value of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = pd.read_csv('simple_fm.csv', index_col='engine_no') # Not really needed kalau analisa contigous\n",
    "X = fm.copy().fillna(0)\n",
    "y = X.pop('remaining_useful_life')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)\n",
    "\n",
    "medianpredict1 = [np.median(y_train) for _ in y_test]\n",
    "mae = mean_absolute_error(medianpredict1, y_test)\n",
    "print('Baseline by median label: Mean Abs Error = {:.2f}'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_train = es['recordings'].df['engine_no'].isin(y_train.index)\n",
    "recordings_from_train = es['recordings'].df[from_train]\n",
    "engines = recordings_from_train.groupby(['engine_no'])\n",
    "median_life = np.median(engines.apply(lambda df: df.shape[0]))\n",
    "\n",
    "from_test = es['recordings'].df['engine_no'].isin(y_test.index)\n",
    "recordings_from_test = es['recordings'].df[from_test]\n",
    "engines = recordings_from_test.groupby(['engine_no'])\n",
    "life_in_test = engines.apply(lambda df: df.shape[0]) - y_test\n",
    "\n",
    "medianpredict2 = median_life - life_in_test\n",
    "medianpredict2 = medianpredict2.apply(lambda row: max(row, 0))\n",
    "mae = mean_absolute_error(medianpredict2, y_test)\n",
    "print('Baseline by median life: Mean Abs Error = {:.2f}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Using the Model\n",
    "\n",
    "Now, we can use our created features to fit a RandomForestRegressor to our data and see if we can improve on the previous scores.\n",
    "\n",
    "## Penting untuk memahami struktur data x_train dan y_train terlebih dahulu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(n_estimators=100)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "preds = reg.predict(X_test)\n",
    "scores = mean_absolute_error(preds, y_test)\n",
    "print('Mean Abs Error: {:.2f}'.format(scores))\n",
    "\n",
    "high_imp_feats = utils.feature_importances(X, reg, feats=10)\n",
    "# Stop dan mari pahami output ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips memahami model diatas, cukup amati apa dan bagaimana X dan Y-nya.\n",
    "\n",
    "# Next, we can apply the exact same transformations (including DFS) to our test data. For this particular case, the real answer isn't in the data so we don't need to worry about cutoff times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = utils.load_data('data/CMAPSS_test_FD004.txt')\n",
    "es2 = make_entityset(data2)\n",
    "\n",
    "fm2 = ft.calculate_feature_matrix(\n",
    "    entityset=es2,\n",
    "    features=features,\n",
    "    verbose=True,)\n",
    "\n",
    "fm2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = fm2.copy().fillna(0)\n",
    "y = pd.read_csv('data/CMAPSS_RUL_FD004.txt', sep=' ', header=None, names=['remaining_useful_life'],index_col=False,)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = reg.predict(X)\n",
    "mae = mean_absolute_error(preds2, y)\n",
    "print('Mean Abs Error: {:.2f}'.format(mae))\n",
    "\n",
    "medianpredict1 = [np.median(y_train) for _ in preds2]\n",
    "mae = mean_absolute_error(medianpredict1, y)\n",
    "print('Baseline by median label: Mean Abs Error = {:.2f}'.format(mae))\n",
    "\n",
    "engines = es2['recordings'].df.groupby(['engine_no'])\n",
    "medianpredict2 = median_life - engines.apply(lambda df: df.shape[0])\n",
    "medianpredict2 = medianpredict2.apply(lambda row: max(row, 0))\n",
    "mae = mean_absolute_error(medianpredict2, y)\n",
    "print('Baseline by median life: Mean Abs Error = {:.2f}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sama seperti $R^2$ di Regresi, Evaluasi ML termudah adalah dengan membandingkan dengan Baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Contoh Penerapan 02: CNN</font></center>\n",
    "\n",
    "\n",
    "* Source: https://towardsdatascience.com/remaining-life-estimation-with-keras-2334514f9c61\n",
    "* Data: Turbofan Engine Degradation Simulation Dataset NASA\n",
    "* Model: Convolutional Neural network - CNN\n",
    "* Method: Merubah Data TIme-Series menjadi Image, lalu menggunakan kelebihan Deep Learning dalam menemukan pola pada image.\n",
    "* The Question: \"How much time is left before the next fault?\"\n",
    "* Di contoh ini hanya menggunakan **100 Engine**\n",
    "\n",
    "<center><img src=\"images/predictive_maintenance_cnn.png\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Kj5s0dQGeTu",
    "outputId": "2bcb7083-9128-46fe-a2a7-ef3ce0dfbbea",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### LOAD TRAIN ### Datanya sama hanya cara load yang berbeda.\n",
    "file_ = 'data/RLE_PM_train.txt'\n",
    "train_df = pd.read_csv(file_, sep=\" \", header=None)\n",
    "    \n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "print('#id:',len(train_df.id.unique()))\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "print(train_df.shape)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALSdMiIkGeTv",
    "outputId": "7c818c83-7f52-4aa6-e5ea-51059a54ef15",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### PLOT TRAIN FREQ ###\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"medium working time:\", train_df.id.value_counts().mean())\n",
    "print(\"max working time:\", train_df.id.value_counts().max())\n",
    "print(\"min working time:\", train_df.id.value_counts().min())\n",
    "plt.figure(figsize=(20,6))\n",
    "p = train_df.id.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fungsi Penting EDA\n",
    "\n",
    "* Engines have different life durations. The average working time in train data is 206 cycles with a minimum of 128 and a maximum of 362.\n",
    "\n",
    "* The operational settings and sensor measurements, in train set for a singular engine, are plotted below.\n",
    "\n",
    "* To plot is always a good idea… In this way, we can have an impressive and general overview of the data at our disposal. At the end of the majority of the series, we can observe a **divergent behavior, which announces a future failure**.\n",
    "\n",
    "* Plot dibawah ini hanya dari Mesin pertama (1).\n",
    "* Stop dan mari diskusikan plot ini lebih cermat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8au1KYhxGeTw",
    "outputId": "52882b5e-c8f2-4ec0-a109-1a8681ff4803",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### plotting sensor data for engine ID ###\n",
    "engine_id = train_df[train_df['id'] == 1]\n",
    "\n",
    "ax1 = engine_id[train_df.columns[2:]].plot(subplots=True, sharex=True, figsize=(20,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGqxRgSfGeTw",
    "outputId": "457ef33a-1c68-4581-8995-0355f2c703e6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### LOAD TEST ###\n",
    "test_df = pd.read_csv('data/RLE_PM_test.txt', sep=\" \", header=None)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "print('#id:',len(test_df.id.unique()))\n",
    "print(test_df.shape)\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWJhO9JqGeTx",
    "outputId": "6717e1dd-60b0-46af-f994-df6cc0c7b48f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### LOAD GROUND TRUTH ###\n",
    "truth_df = pd.read_csv('data/RLE_PM_truth.txt', sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "truth_df.columns = ['more']\n",
    "truth_df = truth_df.set_index(truth_df.index + 1)\n",
    "\n",
    "print(truth_df.shape)\n",
    "truth_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfDdV41tGeTx",
    "outputId": "2c7adced-4e6a-480f-ce5f-8f1750cce00b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### CALCULATE RUL TRAIN ###\n",
    "train_df['RUL']=train_df.groupby(['id'])['cycle'].transform(max)-train_df['cycle']\n",
    "train_df.RUL[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEO9MBahGeTx",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### ADD NEW LABEL TRAIN ###\n",
    "w1 = 45\n",
    "w0 = 15\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Stop dan mari pahami fungsi np.where</font></center>\n",
    "\n",
    "<center><img src=\"images/numpy-where_featured-image.png\" /></center>\n",
    "\n",
    "* Kita perlu juga memahami bentuk/struktur train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(train_df.columns)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apa Makna label ini?\n",
    "set(train_df['label1']), set(train_df['label2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makna Label (Classification Target)\n",
    "\n",
    "* From 0 (fault) to 15 remaining cycles, we’ve labeled as 2; \n",
    "* from 16 to 45 cycles, we’ve labeled as 1 and the rest (>46) as 0. \n",
    "* It is clear that in a realistic scenario, the category labeled as 2 is the most economically valuable. Predict this class with good performance will permit us to operate an adequate program of maintenance, avoiding future faults and saving money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cH7i148qGeTy",
    "outputId": "45e01f22-4dd2-4bbf-b726-678cb03823ab",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### SCALE TRAIN DATA ### MinMax\n",
    "\n",
    "def scale(df):\n",
    "    #return (df - df.mean())/df.std()\n",
    "    return (df - df.min())/(df.max()-df.min())\n",
    "\n",
    "for col in train_df.columns:\n",
    "    if col[0] == 's':\n",
    "        train_df[col] = scale(train_df[col])\n",
    "#     elif col == 'cycle':\n",
    "#         train_df['cycle_norm'] = scale(train_df[col])\n",
    "        \n",
    "train_df = train_df.dropna(axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlNn080PGeTy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### CALCULATE RUL TEST ###\n",
    "truth_df['max'] = test_df.groupby('id')['cycle'].max() + truth_df['more']\n",
    "test_df['RUL'] = [truth_df['max'][i] for i in test_df.id] - test_df['cycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXZ55c6wGeTz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### ADD NEW LABEL TEST ###\n",
    "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
    "test_df['label2'] = test_df['label1']\n",
    "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WW_bkenCGeTz",
    "outputId": "cca724bb-6c76-43ce-c92f-18d28da2f198",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### SCALE TEST DATA ###\n",
    "\n",
    "for col in test_df.columns:\n",
    "    if col[0] == 's':\n",
    "        test_df[col] = scale(test_df[col])\n",
    "#     elif col == 'cycle':\n",
    "#         test_df['cycle_norm'] = scale(test_df[col])\n",
    "        \n",
    "test_df = test_df.dropna(axis=1)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xSDOqXCGeTz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preparing Data: GEN SEQUENCE\n",
    "\n",
    "* In order to predict the RUL for each engine, we’ve pursued a classification approach, generating the label by ourself in this way:\n",
    "* From 0 (fault) to 15 remaining cycles, we’ve labeled as 2; from 16 to 45 cycles, we’ve labeled as 1 and the rest (>46) as 0. It is clear that in a realistic scenario, the category labeled as 2 is the most economically valuable. Predict this class with good performance will permit us to operate an adequate program of maintenance, avoiding future faults and saving money.\n",
    "* In order to have at our disposal the maximum number of data for the train, we split the series with a fixed window and a sliding of 1 step. For example, engine1 have 192 cycles in train, with a window length equal to 50 we extract 142 time series with length 50:\n",
    "* window1 -> from cycle0 to cycle50, window2 -> from cycle1 to cycle51, … , window142 -> from cycle141 to cycle50, window191. Each window is labeled with the corresponding label of the final cycle taken into account by the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0nHdnl5GeT0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sequence_length = 50\n",
    "\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,142),(50,192)\n",
    "    # 0 50 (start stop) -> from row 0 to row 50\n",
    "    # 1 51 (start stop) -> from row 1 to row 51\n",
    "    # 2 52 (start stop) -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 141 191 (start stop) -> from row 141 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "def gen_labels(id_df, seq_length, label):\n",
    "\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsE379XNGeT1",
    "outputId": "018a67f4-b1e5-4c57-caf1-d34f8b3d5fad",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### SEQUENCE COL: COLUMNS TO CONSIDER ###\n",
    "sequence_cols = []\n",
    "for col in train_df.columns:\n",
    "    if col[0] == 's':\n",
    "        sequence_cols.append(col)\n",
    "#sequence_cols.append('cycle_norm')\n",
    "print(sequence_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlykTAW2GeT2",
    "outputId": "1df79983-b282-4ab8-d762-0a29fd01a107",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### GENERATE X TRAIN TEST ###\n",
    "x_train, x_test = [], []\n",
    "for engine_id in train_df.id.unique():\n",
    "    for sequence in gen_sequence(train_df[train_df.id==engine_id], sequence_length, sequence_cols):\n",
    "        x_train.append(sequence)\n",
    "    for sequence in gen_sequence(test_df[test_df.id==engine_id], sequence_length, sequence_cols):\n",
    "        x_test.append(sequence)\n",
    "    \n",
    "x_train = np.asarray(x_train)\n",
    "x_test = np.asarray(x_test)\n",
    "\n",
    "print(\"X_Train shape:\", x_train.shape)\n",
    "print(\"X_Test shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmSrqg5OGeT2",
    "outputId": "17858e85-c89f-44fb-f710-74b978c997df",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### GENERATE Y TRAIN TEST ###\n",
    "y_train, y_test = [], []\n",
    "for engine_id in train_df.id.unique():\n",
    "    for label in gen_labels(train_df[train_df.id==engine_id], sequence_length, ['label2'] ):\n",
    "        y_train.append(label)\n",
    "    for label in gen_labels(test_df[test_df.id==engine_id], sequence_length, ['label2']):\n",
    "        y_test.append(label)\n",
    "    \n",
    "y_train = np.asarray(y_train).reshape(-1,1)\n",
    "y_test = np.asarray(y_test).reshape(-1,1)\n",
    "\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Wbb4KnPGeT3",
    "outputId": "895b1416-0713-4c3f-8f90-7af5e710abb3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### ENCODE LABEL ###\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "print(y_train.shape)\n",
    "\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_Ag3wnxGeT3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FROM TIME SERIES TO IMAGES\n",
    "\n",
    "* The concept is simple… when we try to transform time series into images we always make use of spectrogram. This choice is clever but not always the best one: https://towardsdatascience.com/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd\n",
    "* In this case, the author explains his justified perplexity about dealing with audio series with a spectrogram representation. He talks about sound but the meaning can be translated in our scenario. Spectrograms are powerful but their usage may result in a loss of information, particularly if we try to approach the problem in a computer vision way. To be efficient a 2D CNN requires spatial invariance; this builds on the assumption that features of a classical image (like a photo) carry the same meaning regardless of their location. On the other side, a spectrogram implies a two dimensions representation made by two different units (frequency and time).\n",
    "* For these reasons, we decided to transform my time series windows (of length 50 cycles) making use of Recurrence Plots. They are easy to implement in python with a few lines of code, making use of Scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRb4rEfsGeT3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def rec_plot(s, eps=0.10, steps=10):\n",
    "    d = pdist(s[:,None])\n",
    "    d = np.floor(d/eps)\n",
    "    d[d>steps] = steps\n",
    "    Z = squareform(d)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With this function, we are able to generate an image of 50x50 for every time series at our disposal (I’ve excluded the constant time series with 0 variances). So every single observation is made by an array of images of size 50x50x17 (17 are the time series with no zero variance) like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhaLDhseGeT4",
    "outputId": "442ef74e-137b-470d-d3e5-440e10a966e6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(0,17):\n",
    "    plt.subplot(6, 3, i+1)    \n",
    "    rec = rec_plot(x_train[0,:,i])\n",
    "    plt.imshow(rec)\n",
    "    plt.title(sequence_cols[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qy0-jXHDGeT4",
    "outputId": "f0c8ffa2-82e2-45ea-f200-ef8b8257a3c6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### TRANSFORM X TRAIN TEST IN IMAGES ###\n",
    "x_train_img = np.apply_along_axis(rec_plot, 1, x_train).astype('float16')\n",
    "print(x_train_img.shape)\n",
    "\n",
    "x_test_img = np.apply_along_axis(rec_plot, 1, x_test).astype('float16')\n",
    "print(x_test_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qt2sQwxkGeT5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Convolutional Neural Networks (CNN)</font></center>\n",
    "\n",
    "* Convolutional Neural Networks (CNN) asalnya dari bidang image processing.\n",
    "* CNN menggunakan “filter” atas data dan menghitung \"representasi\" bentuk baru yang lebih efisien.\n",
    "* Walau di perkenalkan di bidang image processing, CNN juga dapat digunakan di data teks dan pada beberapa literatur menunjukkan bahwa CNN di data teks bekerja dengan cukup baik.\n",
    "* Video penjelasan lebih lanjut: https://www.youtube.com/watch?v=jajksuQW4mc\n",
    "* Keterangan lain: https://medium.com/data-folks-indonesia/pemahaman-dasar-convolutional-neural-networks-bfa1bf0b06e1\n",
    "\n",
    "<img alt=\"\" src=\"images/cnn convolutional neural network.jpeg\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Memahami Convolutional Neural Networks (CNN) - 01: image structure</font></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/cnn_digital_photo_data_Structure.png\" style=\"height: 400px;\"/>\n",
    "\n",
    "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Memahami Convolutional Neural Networks (CNN) - 02: Definisi</font></center>\n",
    "\n",
    "* It is a process where we take a small matrix of numbers (called kernel or filter), we pass it over our image and transform it based on the values from filter. \n",
    "* Subsequent feature map values are calculated according to the following formula, where the input image is denoted by f and our kernel by h. \n",
    "* The indexes of rows and columns of the result matrix are marked with m and n respectively.\n",
    "\n",
    "<img alt=\"\" src=\"images/cnn convolutional neural network formula.gif\" style=\"height: 400px;\"/>\n",
    "\n",
    "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img alt=\"\" src=\"images/cnn convolutional neural network.gif\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Memahami Convolutional Neural Networks (CNN) - 03: Effect</font></center>\n",
    "\n",
    "* After placing our filter over a selected pixel, we take each value from kernel and multiply them in pairs with corresponding values from the image. Finally we sum up everything and put the result in the right place in the output feature map. \n",
    "* Above we can see how such an operation looks like in micro scale, but what is even more interesting, is what we can achieve by performing it on a full image.\n",
    "\n",
    "<img alt=\"\" src=\"images/cnn convolutional neural network effect.gif\" style=\"height: 400px;\"/>\n",
    "\n",
    "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Memahami Convolutional Neural Networks (CNN) - 04: Effect continued</font></center>\n",
    "\n",
    "* when we perform convolution over the 6x6 image with a 3x3 kernel, we get a 4x4 feature map. This is because there are only 16 unique positions where we can place our filter inside this picture. \n",
    "* Since our image shrinks every time we perform convolution, we can do it only a limited number of times, before our image disappears completely. \n",
    "* What’s more, if we look at how our kernel moves through the image we see that the impact of the pixels located on the outskirts is much smaller than those in the center of image. \n",
    "* This way we lose some of the information contained in the picture. Below you can see how the position of the pixel changes its influence on the feature map.\n",
    "\n",
    "<img alt=\"\" src=\"images/cnn convolutional neural network effect 02.gif\" style=\"height: 400px;\"/>\n",
    "\n",
    "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">Memahami Convolutional Neural Networks (CNN) - 05: Max Pooling</font></center>\n",
    "\n",
    "* creating a mask that remembers the position of the values used in the first phase, which we can later utilize to transfer the gradients.\n",
    "\n",
    "<img alt=\"\" src=\"images/cnn convolutional neural network Max Pooling.gif\" style=\"height: 400px;\"/>\n",
    "\n",
    "https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFhRjqICGeT5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peFQvCSyGeT5",
    "outputId": "4d3244ef-c545-4fd3-fc40-09d885926228",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 17)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfZgkBSYGeT5",
    "outputId": "775ad6e9-003f-4b0c-be61-450ef51ce95f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "best_model = \"data/module-16-best_model\"\n",
    "try: # load the saved best model\n",
    "    model = load_model(best_model)\n",
    "except: # Run the model\n",
    "    set_seed(33)\n",
    "    es = EarlyStopping(monitor='val_accuracy', mode='auto', restore_best_weights=True, verbose=1, patience=6)\n",
    "    model.fit(x_train_img, y_train, batch_size=4, epochs=25, callbacks=[es],validation_split=0.2, verbose=2)\n",
    "    model.save(best_model)# save\n",
    "    !zip -r /content/data-model-module-16.zip /content/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4LkZmlxGeT6",
    "outputId": "2a1f017e-516a-4fbe-fcaa-2f07e7966d0d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test_img, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuClHKYTGeT6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=25)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=15)\n",
    "    plt.yticks(tick_marks, classes, fontsize=15)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 14)\n",
    "\n",
    "    plt.ylabel('True label', fontsize=20)\n",
    "    plt.xlabel('Predicted label', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAU_xT3tGeT7",
    "outputId": "0569628f-a766-44e3-edac-addb58d8b839",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "preds_prob = model.predict(x_test_img)\n",
    "preds = [np.argmax(p) for p in preds_prob]\n",
    "print(classification_report(np.where(y_test != 0)[1], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UytBuqiHGeT7",
    "outputId": "dbc21ff8-e4b6-4448-d094-86e54795041f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(np.where(y_test != 0)[1], preds)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\">The Confusion Matrix</font></center>\n",
    "\n",
    "From the confusion matrix we can see that our model can well discriminate when an engine is close to failure (2 labels: <16 cycles remaining) or when it works normally (0 label: >45 cycles). A little bit of noise is present in the intermediate class (>15, <46 cycles). We are satisfied to achieve a great and clear result for the prediction of class 2 — i.e. near to failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> Akhir Modul</font></center>\n",
    "\n",
    "<hr />\n",
    "<img alt=\"\" src=\"images/meme-cartoon/meme-no-predictive-maintenance-what-year-is-it.jpg\" style=\"height: 400px;\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
