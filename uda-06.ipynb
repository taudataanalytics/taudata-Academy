{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img alt=\"\" src=\"images/Cover_NLPTM.jpg\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">Representasi Dokumen</font></center>\n",
    "<b><center>(C) Taufik Sutanto - 2020</center>\n",
    "<center>tau-data Indonesia ~ https://tau-data.id ~ taufik@tau-data.id</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Modules for Google Colab\n",
    "import nltk\n",
    "\n",
    "!wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/taudataNlpTm.py\n",
    "!mkdir data\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-ind-def.tab\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-msa-all.tab\n",
    "!wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
    "\n",
    "!pip install spacy python-crfsuite unidecode textblob sastrawi tweepy twython\n",
    "!python -m spacy download en\n",
    "!python -m spacy download xx\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import taudataNlpTm as tau, seaborn as sns; sns.set()\n",
    "import tweepy, json, nltk, urllib.request\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from twython import TwythonStreamer  \n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Vector-Space-Model---VSM\">Vector Space Model - VSM</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/vsm.png\" style=\"width: 300px; height: 213px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Outline Representasi Dokumen :</font>\n",
    "* Representasi Sparse (VSM): Binary, tf &amp;-/ idf, Custom tf-idf, BM25\n",
    "* Frequency filtering, n-grams, vocabulary based\n",
    "* Representasi Dense:&nbsp;Word Embedding (Word2Vec dan FastText)\n",
    "* Tensor to Matrix representation untuk model Machine Learning di Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\t<li>Data yang biasanya kita ketahui berbentuk <strong>tabular </strong>(tabel/kolom-baris/matriks/<em>array</em>/larik), data seperti ini disebut data terstruktur (<strong><em>structured data</em></strong>).</li>\n",
    "\t<li>Data terstruktur dapat disimpan dengan baik di&nbsp;<em>spreadsheet</em>&nbsp;(misal:&nbsp;<em>Excel/CSV</em>) atau basis data (<em>database</em>) relasional dan secara umum dapat digunakan langsung oleh berbagai model/<em>tools</em>&nbsp;statistik/data mining konvensional.</li>\n",
    "\t<li>Sebagian data yang lain memiliki &ldquo;<em>tags</em>&rdquo; yang menjelaskan elemen semantik yang berbeda di dalamnya dan cenderung tidak memiliki skema (struktur) yang statis.</li>\n",
    "\t<li>Data seperti ini disebut data<em>&nbsp;<strong>semi-structured</strong></em>, contohnya data dalam bentuk &nbsp;<strong><a href=\"http://www.w3.org/XML/\" target=\"_blank\">XML</a></strong>.</li>\n",
    "\t<li>Apa bedanya? Apa maksudnya tidak memiliki skema yang statis? Penjelasan mudahnya bayangkan sebuah data terstruktur (tabular), namun dalam setiap baris (<em>record/instance</em>)-nya tidak memiliki jumlah variabel (peubah) yang sama.</li>\n",
    "\t<li>Tentu saja data seperti ini tidak sesuai jika disimpan dan diolah dengan&nbsp;<em>tools/software</em>&nbsp;yang mengasumsikan struktur yang statis pada setiap barisnya (misal: Excel dan SPSS).</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_tipeData.png\" style=\"height: 400px ; width: 430px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\t<li>Data multimedia seperti teks, gambar atau video <strong>tidak dapat</strong>&nbsp;<strong>secara langsung</strong>&nbsp;dianalisa dengan model statistik/data mining.</li>\n",
    "\t<li>Sebuah proses awal&nbsp;<em>(pre-process)</em>&nbsp;harus dilakukan terlebih dahulu untuk merubah data-data tidak (semi) terstruktur tersebut menjadi bentuk yang dapat digunakan oleh model statistik/data mining konvensional.</li>\n",
    "\t<li>Terdapat berbagai macam cara mengubah data-data tidak terstruktur tersebut ke dalam bentuk yang lebih sederhana, dan ini adalah suatu bidang ilmu tersendiri yang cukup dalam. Sebagai contoh saja sebuah teks biasanya dirubah dalam bentuk vektor/<em>topics</em>&nbsp;terlebih dahulu sebelum diolah.</li>\n",
    "\t<li>Vektor data teks sendiri bermacam-macam jenisnya: ada yang berdasarkan eksistensi (<strong><em>binary</em></strong>), frekuensi dokumen (<strong>tf</strong>), frekuensi dan invers jumlah dokumennya dalam corpus (<strong><a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\">tf-idf</a></strong>), <strong>tensor</strong>, dan sebagainya.</li>\n",
    "\t<li>&nbsp;Proses perubahan ini sendiri biasanya tidak&nbsp;<em>lossless</em>, artinya terdapat cukup banyak informasi yang hilang. Maksudnya bagaimana? Sebagai contoh ketika teks direpresentasikan dalam vektor (sering disebut sebagai model <strong>bag-of-words</strong>) maka informasi urutan antar kata menghilang.&nbsp;</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_structureData.png\" style=\"height:270px; width:578px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Contoh bentuk umum representasi dokumen:</strong></p>\n",
    "\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_Bentuk umum representasi dokumen.JPG\" style=\"height: 294px ; width: 620px\" /></p>\n",
    "\n",
    "<p>Pada Model <em>n-grams</em> kolom bisa juga berupa frase.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Document-Term-Matrix-:-Vector-Space-Model---VSM\">Document-Term Matrix : Vector Space Model - VSM</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/vsm_matrix.png\" style=\"width: 500px; height: 283px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_rumus tfidf.png\" style=\"height:370px; width:367px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_tfidf logic.jpg\" style=\"height:359px; width:638px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_variant tfidf.png\" style=\"height:334px; width:955px\" /></p>\n",
    "K = |d|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pertama-tama mari kita Load Data twitter dari pertemuan sebelumnya\n",
    "\n",
    "* Silahkan gunakan data baru (crawl lagi) jika diinginkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTweets(file='Tweets.json'):\n",
    "    f=open(file,encoding='utf-8', errors ='ignore', mode='r')\n",
    "    T=f.readlines();f.close()\n",
    "    for i,t in enumerate(T):\n",
    "        T[i] = json.loads(t.strip())\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data = 300\n",
      "tweet pertama oleh \"maher68554742\" : \"RT @TRANSTV_CORP: Masuk kembali ke masa PSBB Transisi, kini traveler bisa kembali melangsungkan pernikahan di hotel. Hanya wajib lapor dulu…\"\n"
     ]
    }
   ],
   "source": [
    "# karena ToS data json ini dikirimkan terpisah hanya untuk kalangan terbatas.\n",
    "import json\n",
    "\n",
    "T2 = loadTweets(file='data/tweets_sma-01.json')\n",
    "print('Total data = {}'.format(len(data)))\n",
    "print('tweet pertama oleh \"{}\" : \"{}\"'.format(T2[0]['user']['screen_name'],T2[0]['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @TRANSTV_CORP: Masuk kembali ke masa PSBB Transisi, kini traveler bisa kembali melangsungkan pernikahan di hotel. Hanya wajib lapor dulu…',\n",
       " 'RT @TRANSTV_CORP: Masuk kembali ke masa PSBB Transisi, kini traveler bisa kembali melangsungkan pernikahan di hotel. Hanya wajib lapor dulu…',\n",
       " 'RT @tempodotco: PSBB Transisi, 77 RPTRA di Jakarta Utara Dibuka Lagi #TempoMetro https://t.co/yipPjc3J0D',\n",
       " 'Pemprov DKI Jakarta akhirnya mengizinkan bioskop buka saat PSBB transisi. Namun, kebijakan ini masih sulit membangkitkan bisnis bioskop yang telah terpuruk selama pandemii Covid-19. #Analisis\\nhttps://t.co/WeK7rk2ieU',\n",
       " 'Hari Ke-9 PSBB Transisi, Jumlah Positif Corona di Jakarta Tambah 964 Kasus https://t.co/olB2lkHhTb #DariSuara']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contoh mengambil hanya data tweet\n",
    "data = [t['full_text'] for t in T2]\n",
    "data[:5] # 5 tweet pertama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing Data Text-nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b2185ada3d403499e744cb373e7cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transtv_corp masuk psbb transisi traveler melangsungkan pernikahan hotel wajib lapor\n"
     ]
    }
   ],
   "source": [
    "# pre processing\n",
    "import taudataNlpTm as tau \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# cleanText(T, fix={}, lemma=None, stops = set(), symbols_remove = True, min_charLen = 2, fixTag= True)\n",
    "stops, lemmatizer = tau.LoadStopWords(lang='id')\n",
    "stops.add('rt')\n",
    "stops.add('..')\n",
    "for i,d in tqdm(enumerate(data)):\n",
    "    data[i] = tau.cleanText(d, lemma=lemmatizer, stops = stops, symbols_remove = True, min_charLen = 2)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan modul SciKit untuk merubah data tidak terstruktur ke VSM\n",
    "# Scikit implementation http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 614)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VSM - \"binari\"\n",
    "binary_vectorizer = CountVectorizer(binary = True)\n",
    "binari = binary_vectorizer.fit_transform(data)\n",
    "binari.shape # ukuran VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x820 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparse vectors/matrix\n",
    "binari[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[627 766 437 353 335 433 595 765 372 767 129 443 576 176 261 252 793 397\n",
      " 214]\n"
     ]
    }
   ],
   "source": [
    "# Mengakses Datanya\n",
    "print(binari[0].data)\n",
    "print(binari[0].indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rt': 627, 'transtv_corp': 766, 'masuk': 437, 'kembali': 353, 'ke': 335, 'masa': 433, 'psbb'\n"
     ]
    }
   ],
   "source": [
    "# Kolom dan term\n",
    "print(str(binary_vectorizer.vocabulary_)[:93])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 614)\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[576 328 457 575 577 330 440 199 597 296]\n"
     ]
    }
   ],
   "source": [
    "# VSM term Frekuensi : \"tf\"\n",
    "tf_vectorizer = CountVectorizer(binary = False)\n",
    "tf = tf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tf.shape) # Sama\n",
    "print(tf[0].data) # Hanya data ini yg berubah\n",
    "print(tf[0].indices) # Letak kolomnya tetap sama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'benhil'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = tf_vectorizer.vocabulary_\n",
    "kata_kolom = {k:v for v,k in d.items()}\n",
    "kata_kolom[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 614)\n",
      "[0.34054588 0.32748541 0.33604046 0.33827305 0.33827305 0.33827305\n",
      " 0.27160567 0.14364223 0.3295704  0.34286042]\n",
      "[296 597 199 440 330 577 575 457 328 576]\n"
     ]
    }
   ],
   "source": [
    "# VSM term Frekuensi : \"tf-idf\"\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tfidf.shape) # Sama\n",
    "print(tfidf[0].data) # Hanya data ini yg berubah\n",
    "print(tfidf[0].indices) # Letak kolomnya berbeda, namun jumlah kolom dan elemennya tetap sama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom tf-idf:\n",
    "* Menurut http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "* default formula tf-idf yang digunakan sk-learn adalah:\n",
    "* $tfidf = tf * log(\\frac{N}{df+1})$ ==> Smooth IDF\n",
    "* namun kita merubahnya menjadi:\n",
    "* $tfidf = tf * log(\\frac{N}{df})$ ==> Non Smooth IDF\n",
    "* $tfidf = tf * log(\\frac{N}{df+1})$ ==> linear_tf, Smooth IDF\n",
    "* $tfidf = (1+log(tf)) * log(\\frac{N}{df})$ ==> sublinear_tf, Non Smooth IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 820)\n",
      "[0.24182422 0.24182422 0.23243909 0.20570688 0.23858472 0.15261888\n",
      " 0.24018974 0.24018974 0.20681344 0.24018974 0.24018974 0.19244826\n",
      " 0.10148126 0.21138245 0.23096672 0.40128961 0.23393621 0.24348924\n",
      " 0.11197294]\n",
      "[214 397 793 252 261 176 576 443 129 767 372 765 595 433 335 353 437 766\n",
      " 627]\n"
     ]
    }
   ],
   "source": [
    "# VSM term Frekuensi : \"tf-idf\"\n",
    "tfidf_vectorizer = TfidfVectorizer(smooth_idf= False, sublinear_tf=True)\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "print(tfidf.shape) # Sama\n",
    "print(tfidf[0].data) # Hanya data ini yg berubah\n",
    "print(tfidf[0].indices) # Letak kolomnya = tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alasan melakukan filtering berdasarkan frekuensi:\n",
    "* Intuitively filter noise \n",
    "* Curse of Dimensionality (akan dibahas kemudian)\n",
    "* Computational Complexity\n",
    "* Improving accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 614)\n",
      "(300, 107)\n"
     ]
    }
   ],
   "source": [
    "# Frequency Filtering di VSM\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_1 = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.75, min_df=5)\n",
    "tfidf_2 = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tfidf_1.shape)\n",
    "print(tfidf_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 469)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, smooth_idf= True, sublinear_tf=True, \n",
    "                                   ngram_range=(1, 2), max_df=0.90, min_df=2)\n",
    "\n",
    "tfidf_3 = tfidf_vectorizer.fit_transform(data)\n",
    "print(tfidf_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Best-Match-Formula-:-BM25\">Best-Match Formula : BM25</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_bm25_simple.png\" style=\"height: 123px; width: 300px;\" /></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>di IR nilai b dan k yang optimal adalah :&nbsp;<strong> <em>b</em> = 0.75&nbsp; dan k = [1.2 - 2.0]&nbsp; &nbsp;</strong><br />\n",
    "\tref:&nbsp;<em>Christopher, D. M., Prabhakar, R., &amp; Hinrich, S. C. H. &Uuml;. T. Z. E. (2008). Introduction to information retrieval.&nbsp;An Introduction To Information Retrieval,&nbsp;151, 177.</em></li>\n",
    "\t<li>Tapi kalau untuk TextMining (clustering) nilai <strong>k optimal adalah 20, nilai b = sembarang (boleh = 0.75)</strong><br />\n",
    "\tref:&nbsp;<em>Whissell, J. S., &amp; Clarke, C. L. (2011). Improving document clustering using Okapi BM25 feature weighting.&nbsp;Information retrieval,&nbsp;14(5), 466-487.</em></li>\n",
    "\t<li><strong>avgDL </strong>adalah rata-rata panjang dokumen di seluruh dataset dan <strong>DL </strong>adalah panjang dokumen D.<br />\n",
    "\thati-hati, ini berbeda dengan &nbsp;tf-idf MySQL diatas.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'udin76': 35, 'minum': 21, 'kopi': 14, 'pagi': 24, 'sambil': 28, 'makan': 18, 'pisang': 25, 'goreng': 11, 'is': 12, 'the': 33, 'best': 6, 'belajar': 4, 'nlp': 22, 'dan': 8, 'text': 32, 'mining': 20, 'ternyata': 31, 'seru': 29, 'banget': 3, 'sadiezz': 27, 'sudah': 30, 'lumayan': 17, 'lama': 15, 'bingits': 7, 'tukang': 34, 'bakso': 2, 'belum': 5, 'lewat': 16, 'aduh': 0, 'ga': 10, 'mie': 19, 'ayam': 1, 'p4k4i': 23, 'kesyap': 13, 'please': 26, 'deh': 9}\n"
     ]
    }
   ],
   "source": [
    "# Variasi pembentukan matriks VSM:\n",
    "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
    "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat'\n",
    "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
    "\n",
    "D = [d1, d2, d3, d4]\n",
    "# Jika kita menggunakan cara biasa: \n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'udin76': 69, 'minum': 41, 'kopi': 27, 'pagi': 47, 'sambil': 55, 'makan': 34, 'pisang': 50, 'goreng': 21, 'is': 23, 'the': 65, 'best': 13, 'udin76 minum': 70, 'minum kopi': 42, 'kopi pagi': 28, 'pagi pagi': 48, 'pagi sambil': 49, 'sambil makan': 56, 'makan pisang': 36, 'pisang goreng': 51, 'goreng is': 22, 'is the': 24, 'the best': 66, 'belajar': 9, 'nlp': 43, 'dan': 16, 'text': 63, 'mining': 39, 'ternyata': 61, 'seru': 57, 'banget': 6, 'sadiezz': 54, 'belajar nlp': 10, 'nlp dan': 44, 'dan text': 17, 'text mining': 64, 'mining ternyata': 40, 'ternyata seru': 62, 'seru banget': 58, 'banget sadiezz': 8, 'sudah': 59, 'lumayan': 32, 'lama': 29, 'bingits': 14, 'tukang': 67, 'bakso': 4, 'belum': 11, 'lewat': 31, 'sudah lumayan': 60, 'lumayan lama': 33, 'lama bingits': 30, 'bingits tukang': 15, 'tukang bakso': 68, 'bakso belum': 5, 'belum lewat': 12, 'aduh': 0, 'ga': 19, 'mie': 37, 'ayam': 2, 'p4k4i': 45, 'kesyap': 25, 'please': 52, 'deh': 18, 'aduh ga': 1, 'ga banget': 20, 'banget makan': 7, 'makan mie': 35, 'mie ayam': 38, 'ayam p4k4i': 3, 'p4k4i kesyap': 46, 'kesyap please': 26, 'please deh': 53}\n"
     ]
    }
   ],
   "source": [
    "# N-Grams VSM\n",
    "# Bermanfaat untuk menangkap frase kata, misal: \"ga banget\", \"pisang goreng\", dsb\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seru banget': 0,\n",
       " 'seru': 1,\n",
       " 'the best': 2,\n",
       " 'lama': 3,\n",
       " 'text mining': 4,\n",
       " 'nlp': 5,\n",
       " 'ayam': 6}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary based VSM\n",
    "# Bermanfaat untuk menghasilkan hasil analisa yang \"bersih\"\n",
    "# variasi 2\n",
    "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
    "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat seru'\n",
    "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
    "D = [d1,d2,d3,d4]\n",
    "Vocab = {'seru banget':0, 'seru':1, 'the best':2, 'lama':3, 'text mining':4, 'nlp':5, 'ayam':6}\n",
    "tf_vectorizer = CountVectorizer(binary = False, vocabulary=Vocab)\n",
    "tf = tf_vectorizer.fit_transform(D)\n",
    "print(tf.toarray())\n",
    "tf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seru banget': 0, 'the best': 1, 'lama': 2, 'text mining': 3, 'nlp': 4, 'ayam': 5}\n"
     ]
    }
   ],
   "source": [
    "Vocab = {'seru banget':0, 'the best':1, 'lama':2, 'text mining':3, 'nlp':4, 'ayam':5}\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, lowercase=True, vocabulary=Vocab)\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "# VSM terurut sesuai definisi dan terkesan lebih \"bersih\"\n",
    "# Perusahaan besar biasanya memiliki menggunakan teknik ini dengan vocabulary yang comprehensif\n",
    "# Sangat cocok untuk Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><strong>Word Embeddings</strong></h2>\n",
    "\n",
    "<h2><img alt=\"\" src=\"images/3_word_embeddings.png\" style=\"height: 296px ; width: 602px\" /></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_word2vec_example.png\" style=\"height:400px; width:667px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word2Vec</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_word2Vec.png\" style=\"height:400px; width:636px\" /><br />\n",
    "Dikembangkan oleh Tomas Mikolov - Google :</p>\n",
    "\n",
    "<p>Goldberg, Yoav; Levy, Omer. &quot;word2vec Explained: Deriving Mikolov et al.&#39;s Negative-Sampling Word-Embedding Method&quot;.&nbsp;<a href=\"https://en.wikipedia.org/wiki/ArXiv\">arXiv</a>:<a href=\"https://arxiv.org/abs/1402.3722\">1402.3722</a> </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/BoW_VS_WordEmbedding.png\" style=\"width: 248px; height: 372px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transtv_corp masuk psbb transisi traveler melangsungkan pernikahan hotel wajib lapor',\n",
       " 'transtv_corp masuk psbb transisi traveler melangsungkan pernikahan hotel wajib lapor',\n",
       " 'tempodotco psbb transisi 77 rptra jakarta utara dibuka tempo metro']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['transtv_corp', 'masuk', 'psbb', 'transisi', 'traveler', 'melangsungkan', 'pernikahan', 'hotel', 'wajib', 'lapor'], ['transtv_corp', 'masuk', 'psbb', 'transisi', 'traveler', 'melangsungkan', 'pernikahan', 'hotel', 'wajib', 'lapor'], ['tempodotco', 'psbb', 'transisi', '77', 'rptra', 'jakarta', 'utara', 'dibuka', 'tempo', 'metro']]\n"
     ]
    }
   ],
   "source": [
    "# Rubah bentuk data seperti yang dibutuhkan genSim\n",
    "# Bisa juga dilakukan dengan memodifikasi fungsi \"cleanText\" (agar lebih efisien)\n",
    "\n",
    "data_we = []\n",
    "for doc in data:\n",
    "    Tokens = [str(w) for w in TextBlob(doc).words]\n",
    "    data_we.append(Tokens)\n",
    "print(data_we[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!...\n"
     ]
    }
   ],
   "source": [
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# train word2vec dengan data di atas\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "L = 300 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
    "model_wv = Word2Vec(data_we, min_count=2, size=L, window = 5, workers= -2)\n",
    "# min_count adalah jumlah kata minimal yang muncul di corpus\n",
    "# \"size\" adalah Dimensionality of the word vectors \n",
    "# (menurut beberapa literature untuk text disarankan 300-500)\n",
    "# \"window\" adalah jarak maximum urutan kata yang di pertimbangkan\n",
    "# workers = jumlah prosesor yang digunakan untuk menjalankan word2vec\n",
    "print('Done!...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!...\n"
     ]
    }
   ],
   "source": [
    "# di data yang sebenarnya (i.e. besar) Gensim sering membutuhkan waktu cukup lama\n",
    "# Untungnya kita bisa menyimpan dan me-load kembali hasil perhitungan model word2vec, misal\n",
    "model_wv.save('data/model_w2v')\n",
    "model_wv = Word2Vec.load('data/model_w2v')\n",
    "print('Done!...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hati-hati, Word2vec menggunakan Matriks Dense\n",
    "\n",
    "<p>Penggunaan memory oleh Gensim kurang lebih sebagai berikut:</p>\n",
    "\n",
    "<p>Jumlah kata x &quot;size&quot; x 12 bytes</p>\n",
    "\n",
    "<p>Misal terdapat 100 000 kata unik dan menggunakan 200 layers, maka penggunaan memory =&nbsp;</p>\n",
    "\n",
    "<p>100,000x200x12 bytes = ~229MB</p>\n",
    "\n",
    "<p>Jika jumlah size semakin banyak, maka jumlah training data yang diperlukan juga semakin banyak, namun model akan semakin akurat.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[ 1.3707536e-03 -9.4042778e-05  3.2752458e-04  1.3641898e-03\n",
      "  1.1304108e-04]\n"
     ]
    }
   ],
   "source": [
    "# Melihat vector suatu kata\n",
    "vektor = model_wv.wv.__getitem__(['psbb'])\n",
    "print(len(vektor[0])) # Panjang vektor keseluruhan = jumlah neuron yang digunakan\n",
    "print(vektor[0][:5]) # 5 elemen pertama dari vektornya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('menjalankan', 0.15901216864585876),\n",
       " ('memberlakukan', 0.13615277409553528),\n",
       " ('tingkat', 0.12480050325393677),\n",
       " ('10', 0.11427606642246246),\n",
       " ('seminggu', 0.10847358405590057),\n",
       " ('ayat', 0.10166313499212265),\n",
       " ('diizinkan', 0.09671982377767563),\n",
       " ('besok', 0.09351169317960739),\n",
       " ('selasa', 0.09345486015081406),\n",
       " ('bioskop', 0.08977720141410828)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mencari kata terdekat menurut data training dan Word2Vec\n",
    "model_wv.wv.most_similar('psbb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023337541\n",
      "-0.11861667\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Melihat similarity antar kata\n",
    "print(model_wv.wv.similarity('psbb', 'corona'))\n",
    "print(model_wv.wv.similarity('psbb', 'jakarta'))\n",
    "print(model_wv.wv.similarity('psbb', 'psbb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_cosine.png\" style=\"height:400px; width:683px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hati-hati Cosine adalah similarity bukan distance\n",
    "Hal ini akan mempengaruhi interpretasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error! kata \" copid \" tidak ada di training data\n"
     ]
    }
   ],
   "source": [
    "# error jika kata tidak ada di training data\n",
    "# beckman bukan beckmans ==> hence di Word Embedding PreProcessing harus thourough\n",
    "\n",
    "kata = 'copid'\n",
    "try:\n",
    "    print(model_wv.wv.most_similar(kata))\n",
    "except:\n",
    "    print('error! kata \"',kata,'\" tidak ada di training data')\n",
    "# ini salah satu kelemahan Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips:\n",
    "\n",
    "<p>Hati-hati GenSim tidak menggunakan seluruh kata di training data!.</p>\n",
    "\n",
    "<p>Perintah berikut akan menghasilkan kata-kata yang terdapat di vocabulary GenSim</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['transtv_corp', 'masuk', 'psbb', 'transisi', 'traveler', 'melangsungkan', 'pernikahan', 'hotel', 'wajib', 'lapor', 'tempodotco', '77', 'rptra', 'jakarta', 'utara', 'dibuka', 'tempo', 'metro', 'pemprov', 'dki', 'mengizinkan', 'bioskop', 'bu\n"
     ]
    }
   ],
   "source": [
    "Vocabulary = model_wv.wv.vocab\n",
    "print(str(Vocabulary.keys())[:250])\n",
    "# Gunakan vocabulary ini (rubah ke \"set\") untuk membuat program menjadi lebih robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hati-hati menginterpretasikan hasil Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\" FastText-(Facebook-2016)\">&nbsp;FastText (Facebook-2016)</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Menggunakan Sub-words: app, ppl, ple - apple</li>\n",
    "\t<li>Paper:&nbsp;https://arxiv.org/abs/1607.04606&nbsp;&nbsp;</li>\n",
    "\t<li>Website:&nbsp;https://fasttext.cc/</li>\n",
    "\t<li>Source:&nbsp;https://github.com/facebookresearch/fastText&nbsp;</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caution penggunaan memory besar, bila timbul \"Memory Error\" kecilkan nilai L\n",
    "from gensim.models import FastText\n",
    "\n",
    "L = 100 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
    "model_FT = FastText(data_we, size=L, window=5, min_count=2, workers=-2)\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('psbbjakarta', 0.39620503783226013),\n",
       " ('psbbtransisi', 0.3755951523780823),\n",
       " ('pemprov', 0.25807705521583557),\n",
       " ('ya', 0.2563023567199707),\n",
       " ('12', 0.22505980730056763),\n",
       " ('pengunjung', 0.22196458280086517),\n",
       " ('positive', 0.21561191976070404),\n",
       " ('19', 0.2093917727470398),\n",
       " ('biar', 0.2086964249610901),\n",
       " ('dkijakarta', 0.20685012638568878)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mencari kata terdekat menurut data training dan Word2Vec\n",
    "model_FT.wv.most_similar('psbb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015008051\n",
      "0.07263649\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Melihat similarity antar kata\n",
    "print(model_FT.wv.similarity('psbb', 'corona'))\n",
    "print(model_FT.wv.similarity('psbb', 'jakarta'))\n",
    "print(model_FT.wv.similarity('psbb', 'psbb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec error!\n",
      "[('corona', 0.42177480459213257), ('maju', 0.2425994575023651), ('berpendapat', 0.23979200422763824), ('selasa', 0.22723904252052307), ('kendaraan', 0.21705520153045654), ('udara', 0.21434135735034943), ('udah', 0.21017128229141235), ('melangsungkan', 0.20929008722305298), ('bisnis', 0.2072659730911255), ('cibubur', 0.19852080941200256)]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec VS FastText\n",
    "try:\n",
    "    print(model_wv.wv.most_similar('coro'))\n",
    "except:\n",
    "    print('Word2Vec error!')\n",
    "    \n",
    "try:\n",
    "    print(model_FT.wv.most_similar('coro'))\n",
    "except:\n",
    "    print('FastText error!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diskusi:\n",
    "<ul>\n",
    "\t<li>Apakah kelebihan dan kekurangan WE secara umum?</li>\n",
    "\t<li>Apakah kira-kira aplikasi WE?</li>\n",
    "\t<li>Apakah bisa dijadikan representasi dokumen? Bagaimana caranya?</li>\n",
    "\t<li>Bergantung pada apa sajakah performa model WE?</li>\n",
    "</ul>\n",
    "\n",
    "* Preprocessing apa yang sebaiknya dilakukan pada model Word Embedding?\n",
    "* Apakah Pos Tag bermanfaat disini? Jika iya bagaimana menggunakannya?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>End of Module UDA-06</h1>\n",
    "<hr />\n",
    "<p><img alt=\"\" src=\"images/2_Studying_Linguistic.png\" style=\"height:500px; width:667px\" /></p>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
