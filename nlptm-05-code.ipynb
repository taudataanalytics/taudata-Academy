{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2><strong><font color=\"blue\">Social Network Analysis (SNA)</font></strong></h2></center>\n",
    "<center><h3><strong><font color=\"blue\"><a href=\"https://taudata.blogspot.com\">https://taudata.blogspot.com</a></font></strong></h3></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/covers/taudata-cover.jpg\"/>\n",
    "\n",
    "<center><h2><strong><font color=\"blue\">NLPTM-05: Pendahuluan Representasi Teks/Dokumen (VSM & Word Embedding)</font></strong></h2></center>\n",
    "<center><h3><strong><font color=\"blue\"><a href=\"https://taudata.blogspot.com/2022/04/nlptm-05.html\">https://taudata.blogspot.com/2022/04/nlptm-05.html</a></font></strong></h3></center>\n",
    "<b><center><h3>(C) Taufik Sutanto</h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\n"
     ]
    }
   ],
   "source": [
    "# Installing Modules for Google Colab\n",
    "import warnings, nltk; warnings.simplefilter('ignore')\n",
    "\n",
    "try:\n",
    "    import google.colab; IN_COLAB = True\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/taudataNlpTm.py\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/kata_dasar.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-ind-def.tab\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-msa-all.tab\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/Tweets_example.json\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
    "\n",
    "    !pip install spacy python-crfsuite unidecode textblob sastrawi tweepy twython\n",
    "    !python -m spacy download xx\n",
    "    !python -m spacy download en_core_web_sm\n",
    "\n",
    "    nltk.download('popular')\n",
    "    print(\"Running the code in colab, don't forget to change the runtime (if needed)\")    \n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import taudataNlpTm as tau, seaborn as sns; sns.set()\n",
    "import tweepy, json, nltk, urllib.request\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pertama-tama mari kita Load Data twitter dari pertemuan sebelumnya\n",
    "\n",
    "* Silahkan gunakan data baru (crawl lagi) jika diinginkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTweets(file='Tweets.json'):\n",
    "    f=open(file,encoding='utf-8', errors ='ignore', mode='r')\n",
    "    T=f.readlines();f.close()\n",
    "    for i,t in enumerate(T):\n",
    "        T[i] = json.loads(t.strip())\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data = 10000\n",
      "tweet pertama oleh \"Nuy_indra\" : \"RT @aiceindonesia: Semoga dengan adanya aksi sosial ini masyarakat Indonesia makin disiplin memakai masker, patuhi protokol kesehatan, danâ€¦\"\n"
     ]
    }
   ],
   "source": [
    "# karena ToS data json ini dikirimkan terpisah hanya untuk kalangan terbatas.\n",
    "\n",
    "T2 = loadTweets(file='data/Tweets_example.json')\n",
    "print('Total data = {}'.format(len(T2)))\n",
    "print('tweet pertama oleh \"{}\" : \"{}\"'.format(T2[0]['user']['screen_name'],T2[0]['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @aiceindonesia: Semoga dengan adanya aksi sosial ini masyarakat Indonesia makin disiplin memakai masker, patuhi protokol kesehatan, danâ€¦',\n",
       " 'RT @ecosocrights: Bukan hanya konglomerat, kekayaan para pejabat juga naik selama pandemi, termasuk kekayaan Presiden. Pandemi menguntungkaâ€¦',\n",
       " 'siapa diantara temen temen disini yg kekayaannya selama pandemi naik 70%?',\n",
       " 'RT @Gamal_Albinsaid: Tercatat belasan ribu anak menjadi yatim piatu karena pandemi COVID-19.\\n\\nIni memotivasi saya mengembangkan program Anaâ€¦',\n",
       " 'RT @kyumbin131: âœ¨ 22nd Junkyu Birthday Project ðŸ‡®ðŸ‡© âœ¨\\nPenyaluran 22 paket sembako utk keluarga/masyarakat yg kesulitan di masa pandemi.\\n\\nSemoâ€¦']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contoh mengambil hanya data tweet\n",
    "data = [t['full_text'] for t in T2]\n",
    "data[:5] # 5 tweet pertama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing Data Text-nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180668bca3d042beacf571bbca31011f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiceindonesia moga aksi sosial masyarakat indonesia disiplin pakai masker patuh protokol sehat\n"
     ]
    }
   ],
   "source": [
    "# pre processing\n",
    "\n",
    "stops, lemmatizer = tau.LoadStopWords(lang='id')\n",
    "stops.add('rt')\n",
    "stops.add('..')\n",
    "for i,d in tqdm(enumerate(data)):\n",
    "    data[i] = tau.cleanText(d, lemma=lemmatizer, stops = stops, symbols_remove = True, min_charLen = 2)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan modul SciKit untuk merubah data tidak terstruktur ke VSM\n",
    "# Scikit implementation http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 9851)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VSM - \"binari\"\n",
    "binary_vectorizer = CountVectorizer(binary = True)\n",
    "binari = binary_vectorizer.fit_transform(data)\n",
    "binari.shape # ukuran VSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x9851 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparse vectors/matrix\n",
    "binari[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[ 141 5623  186 8381 5299 3493 2048 6503 5279 6644 7158 7909]\n"
     ]
    }
   ],
   "source": [
    "# Mengakses Datanya\n",
    "print(binari[0].data)\n",
    "print(binari[0].indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aiceindonesia': 141, 'moga': 5623, 'aksi': 186, 'sosial': 8381, 'masyarakat': 5299, 'indone\n"
     ]
    }
   ],
   "source": [
    "# Kolom dan term\n",
    "print(str(binary_vectorizer.vocabulary_)[:93])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9851)\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[ 141 5623  186 8381 5299 3493 2048 6503 5279 6644 7158 7909]\n"
     ]
    }
   ],
   "source": [
    "# VSM term Frekuensi : \"tf\"\n",
    "tf_vectorizer = CountVectorizer(binary = False)\n",
    "tf = tf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tf.shape) # Sama\n",
    "print(tf[0].data) # Hanya data ini yg berubah\n",
    "print(tf[0].indices) # Letak kolomnya tetap sama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awkward'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = tf_vectorizer.vocabulary_\n",
    "kata_kolom = {k:v for v,k in d.items()}\n",
    "kata_kolom[597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9851)\n",
      "[0.24587303 0.30198705 0.33266445 0.27506537 0.29841056 0.31648834\n",
      " 0.21882555 0.19953934 0.25019267 0.3566293  0.23883859 0.37186648]\n",
      "[7909 7158 6644 5279 6503 2048 3493 5299 8381  186 5623  141]\n"
     ]
    }
   ],
   "source": [
    "# VSM term Frekuensi : \"tf-idf\"\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tfidf.shape) # Sama\n",
    "print(tfidf[0].data) # Hanya data ini yg berubah\n",
    "print(tfidf[0].indices) # Letak kolomnya berbeda, namun jumlah kolom dan elemennya tetap sama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9851)\n",
      "[0.24525912 0.3016796  0.3328828  0.27454194 0.29806418 0.31638117\n",
      " 0.21820422 0.19893964 0.24958561 0.3576212  0.2382173  0.37361798]\n",
      "[7909 7158 6644 5279 6503 2048 3493 5299 8381  186 5623  141]\n"
     ]
    }
   ],
   "source": [
    "# VSM term Frekuensi : \"tf-idf\"\n",
    "tfidf_vectorizer = TfidfVectorizer(smooth_idf= False, sublinear_tf=True)\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "print(tfidf.shape) # Sama\n",
    "print(tfidf[0].data) # Hanya data ini yg berubah\n",
    "print(tfidf[0].indices) # Letak kolomnya = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9851)\n",
      "(10000, 2382)\n"
     ]
    }
   ],
   "source": [
    "# Frequency Filtering di VSM\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_1 = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.75, min_df=5)\n",
    "tfidf_2 = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tfidf_1.shape)\n",
    "print(tfidf_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 12312)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, smooth_idf= True, sublinear_tf=True, \n",
    "                                   ngram_range=(1, 2), max_df=0.90, min_df=2)\n",
    "\n",
    "tfidf_3 = tfidf_vectorizer.fit_transform(data)\n",
    "print(tfidf_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Best-Match-Formula-:-BM25\">Best-Match Formula : BM25</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_bm25_simple.png\" style=\"height: 123px; width: 300px;\" /></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>di IR nilai b dan k yang optimal adalah :&nbsp;<strong> <em>b</em> = 0.75&nbsp; dan k = [1.2 - 2.0]&nbsp; &nbsp;</strong><br />\n",
    "\tref:&nbsp;<em>Christopher, D. M., Prabhakar, R., &amp; Hinrich, S. C. H. &Uuml;. T. Z. E. (2008). Introduction to information retrieval.&nbsp;An Introduction To Information Retrieval,&nbsp;151, 177.</em></li>\n",
    "\t<li>Tapi kalau untuk TextMining (clustering) nilai <strong>k optimal adalah 20, nilai b = sembarang (boleh = 0.75)</strong><br />\n",
    "\tref:&nbsp;<em>Whissell, J. S., &amp; Clarke, C. L. (2011). Improving document clustering using Okapi BM25 feature weighting.&nbsp;Information retrieval,&nbsp;14(5), 466-487.</em></li>\n",
    "\t<li><strong>avgDL </strong>adalah rata-rata panjang dokumen di seluruh dataset dan <strong>DL </strong>adalah panjang dokumen D.<br />\n",
    "\thati-hati, ini berbeda dengan &nbsp;tf-idf MySQL diatas.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'udin76': 35, 'minum': 21, 'kopi': 14, 'pagi': 24, 'sambil': 28, 'makan': 18, 'pisang': 25, 'goreng': 11, 'is': 12, 'the': 33, 'best': 6, 'belajar': 4, 'nlp': 22, 'dan': 8, 'text': 32, 'mining': 20, 'ternyata': 31, 'seru': 29, 'banget': 3, 'sadiezz': 27, 'sudah': 30, 'lumayan': 17, 'lama': 15, 'bingits': 7, 'tukang': 34, 'bakso': 2, 'belum': 5, 'lewat': 16, 'aduh': 0, 'ga': 10, 'mie': 19, 'ayam': 1, 'p4k4i': 23, 'kesyap': 13, 'please': 26, 'deh': 9}\n"
     ]
    }
   ],
   "source": [
    "# Variasi pembentukan matriks VSM:\n",
    "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
    "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat'\n",
    "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
    "\n",
    "D = [d1, d2, d3, d4]\n",
    "# Jika kita menggunakan cara biasa: \n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'udin76': 69, 'minum': 41, 'kopi': 27, 'pagi': 47, 'sambil': 55, 'makan': 34, 'pisang': 50, 'goreng': 21, 'is': 23, 'the': 65, 'best': 13, 'udin76 minum': 70, 'minum kopi': 42, 'kopi pagi': 28, 'pagi pagi': 48, 'pagi sambil': 49, 'sambil makan': 56, 'makan pisang': 36, 'pisang goreng': 51, 'goreng is': 22, 'is the': 24, 'the best': 66, 'belajar': 9, 'nlp': 43, 'dan': 16, 'text': 63, 'mining': 39, 'ternyata': 61, 'seru': 57, 'banget': 6, 'sadiezz': 54, 'belajar nlp': 10, 'nlp dan': 44, 'dan text': 17, 'text mining': 64, 'mining ternyata': 40, 'ternyata seru': 62, 'seru banget': 58, 'banget sadiezz': 8, 'sudah': 59, 'lumayan': 32, 'lama': 29, 'bingits': 14, 'tukang': 67, 'bakso': 4, 'belum': 11, 'lewat': 31, 'sudah lumayan': 60, 'lumayan lama': 33, 'lama bingits': 30, 'bingits tukang': 15, 'tukang bakso': 68, 'bakso belum': 5, 'belum lewat': 12, 'aduh': 0, 'ga': 19, 'mie': 37, 'ayam': 2, 'p4k4i': 45, 'kesyap': 25, 'please': 52, 'deh': 18, 'aduh ga': 1, 'ga banget': 20, 'banget makan': 7, 'makan mie': 35, 'mie ayam': 38, 'ayam p4k4i': 3, 'p4k4i kesyap': 46, 'kesyap please': 26, 'please deh': 53}\n"
     ]
    }
   ],
   "source": [
    "# N-Grams VSM\n",
    "# Bermanfaat untuk menangkap frase kata, misal: \"ga banget\", \"pisang goreng\", dsb\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seru banget': 0,\n",
       " 'seru': 1,\n",
       " 'the best': 2,\n",
       " 'lama': 3,\n",
       " 'text mining': 4,\n",
       " 'nlp': 5,\n",
       " 'ayam': 6}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary based VSM\n",
    "# Bermanfaat untuk menghasilkan hasil analisa yang \"bersih\"\n",
    "# variasi 2\n",
    "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
    "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat seru'\n",
    "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
    "D = [d1,d2,d3,d4]\n",
    "Vocab = {'seru banget':0, 'seru':1, 'the best':2, 'lama':3, 'text mining':4, 'nlp':5, 'ayam':6}\n",
    "tf_vectorizer = CountVectorizer(binary = False, vocabulary=Vocab)\n",
    "tf = tf_vectorizer.fit_transform(D)\n",
    "print(tf.toarray())\n",
    "tf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seru banget': 0, 'the best': 1, 'lama': 2, 'text mining': 3, 'nlp': 4, 'ayam': 5}\n"
     ]
    }
   ],
   "source": [
    "Vocab = {'seru banget':0, 'the best':1, 'lama':2, 'text mining':3, 'nlp':4, 'ayam':5}\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, lowercase=True, vocabulary=Vocab)\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "# VSM terurut sesuai definisi dan terkesan lebih \"bersih\"\n",
    "# Perusahaan besar biasanya memiliki menggunakan teknik ini dengan vocabulary yang comprehensif\n",
    "# Sangat cocok untuk Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiceindonesia moga aksi sosial masyarakat indonesia disiplin pakai masker patuh protokol sehat',\n",
       " 'ecosocrights konglomerat kaya jabat pandemi kaya presiden pandemi menguntungka',\n",
       " 'temen temen yg kaya pandemi']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['aiceindonesia', 'moga', 'aksi', 'sosial', 'masyarakat', 'indonesia', 'disiplin', 'pakai', 'masker', 'patuh', 'protokol', 'sehat'], ['ecosocrights', 'konglomerat', 'kaya', 'jabat', 'pandemi', 'kaya', 'presiden', 'pandemi', 'menguntungka'], ['temen', 'temen', 'yg', 'kaya', 'pandemi']]\n"
     ]
    }
   ],
   "source": [
    "# Rubah bentuk data seperti yang dibutuhkan genSim\n",
    "# Bisa juga dilakukan dengan memodifikasi fungsi \"cleanText\" (agar lebih efisien)\n",
    "\n",
    "data_we = []\n",
    "for doc in data:\n",
    "    Tokens = [str(w) for w in TextBlob(doc).words]\n",
    "    data_we.append(Tokens)\n",
    "print(data_we[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!...\n"
     ]
    }
   ],
   "source": [
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# train word2vec dengan data di atas\n",
    "\n",
    "L = 300 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
    "model_wv = Word2Vec(data_we, min_count=2, vector_size=L, window = 5, workers= -2)\n",
    "# min_count adalah jumlah kata minimal yang muncul di corpus\n",
    "# \"vector_size\" adalah Dimensionality of the word vectors \n",
    "# (menurut beberapa literature untuk text disarankan 300-500)\n",
    "# \"window\" adalah jarak maximum urutan kata yang di pertimbangkan\n",
    "# workers = jumlah prosesor yang digunakan untuk menjalankan word2vec\n",
    "print('Done!...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!...\n"
     ]
    }
   ],
   "source": [
    "# di data yang sebenarnya (i.e. besar) Gensim sering membutuhkan waktu cukup lama\n",
    "# Untungnya kita bisa menyimpan dan me-load kembali hasil perhitungan model word2vec, misal\n",
    "model_wv.save('data/model_w2v')\n",
    "model_wv = Word2Vec.load('data/model_w2v')\n",
    "print('Done!...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hati-hati, Word2vec menggunakan Matriks Dense\n",
    "\n",
    "<p>Penggunaan memory oleh Gensim kurang lebih sebagai berikut:</p>\n",
    "\n",
    "<p>Jumlah kata x &quot;size&quot; x 12 bytes</p>\n",
    "\n",
    "<p>Misal terdapat 100 000 kata unik dan menggunakan 200 layers, maka penggunaan memory =&nbsp;</p>\n",
    "\n",
    "<p>100,000x200x12 bytes = ~229MB</p>\n",
    "\n",
    "<p>Jika jumlah size semakin banyak, maka jumlah training data yang diperlukan juga semakin banyak, namun model akan semakin akurat.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[-0.00139513 -0.00099477  0.00258718  0.00241652 -0.00276821]\n"
     ]
    }
   ],
   "source": [
    "# Melihat vector suatu kata\n",
    "vektor = model_wv.wv.__getitem__(['psbb'])\n",
    "print(len(vektor[0])) # Panjang vektor keseluruhan = jumlah neuron yang digunakan\n",
    "print(vektor[0][:5]) # 5 elemen pertama dari vektornya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kontannews', 0.21803781390190125),\n",
       " ('muhammad', 0.2029067873954773),\n",
       " ('bela', 0.19181834161281586),\n",
       " ('biang', 0.1762441098690033),\n",
       " ('stiap', 0.17516076564788818),\n",
       " ('teori', 0.1729869544506073),\n",
       " ('manajemen', 0.1714370846748352),\n",
       " ('mensos', 0.16922974586486816),\n",
       " ('aktivitas', 0.16919642686843872),\n",
       " ('ppkn', 0.16815714538097382)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mencari kata terdekat menurut data training dan Word2Vec\n",
    "model_wv.wv.most_similar('psbb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.062678315\n",
      "-0.039310396\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Melihat similarity antar kata\n",
    "print(model_wv.wv.similarity('psbb', 'corona'))\n",
    "print(model_wv.wv.similarity('psbb', 'bioskop'))\n",
    "print(model_wv.wv.similarity('psbb', 'psbb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hati-hati Cosine adalah similarity bukan distance\n",
    "Hal ini akan mempengaruhi interpretasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error! kata \" copid \" tidak ada di training data\n"
     ]
    }
   ],
   "source": [
    "# error jika kata tidak ada di training data\n",
    "# beckman bukan beckmans ==> hence di Word Embedding PreProcessing harus thourough\n",
    "\n",
    "kata = 'copid'\n",
    "try:\n",
    "    print(model_wv.wv.most_similar(kata))\n",
    "except:\n",
    "    print('error! kata \"',kata,'\" tidak ada di training data')\n",
    "# ini salah satu kelemahan Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips:\n",
    "\n",
    "<p>Hati-hati GenSim tidak menggunakan seluruh kata di training data!.</p>\n",
    "\n",
    "<p>Perintah berikut akan menghasilkan kata-kata yang terdapat di vocabulary GenSim</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pandemi',\n",
       " 'yg',\n",
       " 'kaya',\n",
       " 'bantu',\n",
       " 'rp',\n",
       " 'miliar',\n",
       " 'harta',\n",
       " 'dampak',\n",
       " 'ga',\n",
       " 'luhut',\n",
       " 'jabat',\n",
       " 'musisi',\n",
       " 'pandjaitan',\n",
       " 'binsar',\n",
       " 'ekonomi',\n",
       " 'kuliah',\n",
       " 'oposisicerdas',\n",
       " 'ya',\n",
       " 'jokowi',\n",
       " 'jatengopini',\n",
       " 'orang',\n",
       " 'jawa',\n",
       " 'halo',\n",
       " 'lho',\n",
       " 'masyarakat',\n",
       " 'kreatif',\n",
       " 'gak',\n",
       " 'manggung',\n",
       " 'tangis',\n",
       " 'tapi',\n",
       " 'anies',\n",
       " 'rakyat',\n",
       " 'turun']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampel_ = 33\n",
    "model_wv.wv.index_to_key[:sampel_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hati-hati menginterpretasikan hasil Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Â FastText-(Facebook-2016)\">&nbsp;FastText (Facebook-2016)</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Menggunakan Sub-words: app, ppl, ple - apple</li>\n",
    "\t<li>Paper:&nbsp;https://arxiv.org/abs/1607.04606&nbsp;&nbsp;</li>\n",
    "\t<li>Website:&nbsp;https://fasttext.cc/</li>\n",
    "\t<li>Source:&nbsp;https://github.com/facebookresearch/fastText&nbsp;</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caution penggunaan memory besar, bila timbul \"Memory Error\" kecilkan nilai L\n",
    "\n",
    "L = 100 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
    "model_FT = FastText(data_we, vector_size=L, window=5, min_count=2, workers=-2)\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('naek', 0.34502407908439636),\n",
       " ('mek', 0.3286518454551697),\n",
       " ('home', 0.3269801735877991),\n",
       " ('bulan', 0.31775084137916565),\n",
       " ('blm', 0.3159526288509369),\n",
       " ('kapolres', 0.3140672445297241),\n",
       " ('himpun', 0.3126278817653656),\n",
       " ('psi', 0.3098238706588745),\n",
       " ('himperaktif', 0.30772924423217773),\n",
       " ('cipedes', 0.30604347586631775)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mencari kata terdekat menurut data training dan Word2Vec\n",
    "model_FT.wv.most_similar('psbb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.082103185\n",
      "-0.1547481\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Melihat similarity antar kata\n",
    "print(model_FT.wv.similarity('psbb', 'corona'))\n",
    "print(model_FT.wv.similarity('psbb', 'jakarta'))\n",
    "print(model_FT.wv.similarity('psbb', 'psbb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec error!\n",
      "[('corona', 0.5362619161605835), ('coronavirus', 0.4721188545227051), ('narsih', 0.3698681890964508), ('panggil', 0.3449081778526306), ('bersih', 0.3184361159801483), ('coronce', 0.31669026613235474), ('jepit', 0.3149551451206207), ('orgil', 0.3126393258571625), ('sih', 0.3084302246570587), ('siap', 0.3081304430961609)]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec VS FastText\n",
    "try:\n",
    "    print(model_wv.wv.most_similar('coro'))\n",
    "except:\n",
    "    print('Word2Vec error!')\n",
    "    \n",
    "try:\n",
    "    print(model_FT.wv.most_similar('coro'))\n",
    "except:\n",
    "    print('FastText error!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diskusi:\n",
    "<ul>\n",
    "\t<li>Apakah kelebihan dan kekurangan WE secara umum?</li>\n",
    "\t<li>Apakah kira-kira aplikasi WE?</li>\n",
    "\t<li>Apakah bisa dijadikan representasi dokumen? Bagaimana caranya?</li>\n",
    "\t<li>Bergantung pada apa sajakah performa model WE?</li>\n",
    "</ul>\n",
    "\n",
    "* Preprocessing apa yang sebaiknya dilakukan pada model Word Embedding?\n",
    "* Apakah Pos Tag bermanfaat disini? Jika iya bagaimana menggunakannya?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>End of Module</h1>\n",
    "<hr />\n",
    "<p><img alt=\"\" src=\"images/meme-cartoon/2_Studying_Linguistic.png\" style=\"height:500px; width:667px\" /></p>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
