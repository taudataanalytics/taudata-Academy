{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2><strong><font color=\"blue\">Natural Language Processing dan Text Mining (NLPTM)</font></strong></h2></center>\n",
    "<center><h2><strong><font color=\"blue\">Social Media Analytics (SMA)</font></strong></h2></center>\n",
    "\n",
    "<center><h3><strong><font color=\"blue\"><a href=\"https://taudata.blogspot.com\">https://taudata.blogspot.com</a></font></strong></h3></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/covers/taudata-cover.jpg\"/>\n",
    "\n",
    "<center><h2><strong><font color=\"blue\">NLPTM-06: Pendahuluan Topic Modelling</font></strong></h2></center>\n",
    "<center><h3><strong><font color=\"blue\"><a href=\"https://taudata.blogspot.com/2022/05/nlptm-07.html\">https://taudata.blogspot.com/2022/05/nlptm-07.html</a></font></strong></h3></center>\n",
    "<b><center><h3>(C) Taufik Sutanto ~ taudata Analytics</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Outline Topic Modelling :</font>\n",
    "\n",
    "* Importing Data\n",
    "* Pendahuluan Topic Modelling\n",
    "* Soft Clustering (Topic Modelling): LDA dan NMF\n",
    "* Visualisasi dan Interpretasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:14:55.340587Z",
     "start_time": "2022-05-30T11:14:55.301588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Jalankan Cell ini \"HANYA\" jika anda menggunakan Google Colab\n",
    "# Jika di jalankan di komputer local, silahkan lihat NLPTM-02 untuk instalasinya.\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    import google.colab; IN_COLAB = True\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/taudataNlpTm.py\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/kata_dasar.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-ind-def.tab\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-msa-all.tab\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/ind_SA.csv\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/dataset_tweet_sentiment_opini_film.csv\n",
    "    !pip install spacy unidecode textblob sastrawi pyLDAvis\n",
    "    !pip install --upgrade python-crfsuite gensim\n",
    "    !pip install sklearn-pycrfsuite\n",
    "    !python -m spacy download xx_ent_wiki_sm\n",
    "    !python -m spacy download xx_sent_ud_sm\n",
    "    !python -m spacy download en_core_web_sm\n",
    "\n",
    "    nltk.download('popular')\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgement:\n",
    "* Sentimen Data Source: https://raw.githubusercontent.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia/master/dataset_tweet_sentiment_opini_film.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:14:55.356587Z",
     "start_time": "2022-05-30T11:14:55.341587Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing Modules untuk Notebook ini\n",
    "import taudataNlpTm as tau, itertools, re, pickle, pyLDAvis, pyLDAvis.sklearn, spacy, urllib.request\n",
    "import numpy as np, matplotlib.pyplot as plt, pandas as pd, seaborn as sns \n",
    "from tqdm import tqdm\n",
    "from nltk.tag import CRFTagger\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "random_state = 99\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:14:55.372590Z",
     "start_time": "2022-05-30T11:14:55.357587Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    f = open('data/20newsgroups.pckl', 'rb')\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "except:\n",
    "    categories = ['sci.med', 'talk.politics.misc',  'rec.autos']\n",
    "    data = fetch_20newsgroups(categories=categories,remove=('headers', 'footers', 'quotes'))\n",
    "    f = open('data/20newsgroups.pckl', 'wb')\n",
    "    pickle.dump(data, f)\n",
    "    f.close()\n",
    "    \n",
    "D = [doc for doc in data.data]\n",
    "Y = data.target\n",
    "print(\"Finished, {} documents loaded\".format(len(D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:15:57.152786Z",
     "start_time": "2022-05-30T11:14:55.373588Z"
    }
   },
   "outputs": [],
   "source": [
    "import taudataNlpTm as tau \n",
    "from tqdm import tqdm\n",
    "\n",
    "stops, lemmatizer = tau.LoadStopWords(lang='en')\n",
    "\n",
    "for i,d in tqdm(enumerate(D)):\n",
    "    D[i] = tau.cleanText(d, lemma=None, stops = stops, symbols_remove = True, min_charLen=2)\n",
    "    # Ganti lemma=lemmatizer jika memungkinkan (hati-hati jauh lebih lambat)\n",
    "print(D[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add LDA Matrix Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:15:57.248773Z",
     "start_time": "2022-05-30T11:15:57.153778Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kita mulai dengan membuat VSM-nya\n",
    "# kita gunakan perintah yang ada di Segmen sebelumnya \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer()\n",
    "\n",
    "data = D.copy()\n",
    "tf = tf_vectorizer.fit_transform(data)\n",
    "tf_terms = tf_vectorizer.get_feature_names()\n",
    "# Mengapa tf bukan tfidf?\n",
    "# Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.\n",
    "# Bisa di tamahkan dengan Frequency filtering \"Max_df\" dan \"Min_df\"\n",
    "\n",
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:16:00.785023Z",
     "start_time": "2022-05-30T11:15:57.250773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dilanjutkan dengan membentuk model LDA-nya\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "n_topics = 3 # Misal tidak di optimalkan terlebih dahulu\n",
    "lda = LDA(n_components=n_topics, learning_method='batch', random_state=0).fit(tf)   \n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:16:01.008188Z",
     "start_time": "2022-05-30T11:16:00.786010Z"
    }
   },
   "outputs": [],
   "source": [
    "# Melihat Topik-topiknya\n",
    "vsm_topics = lda.transform(tf)\n",
    "print(vsm_topics.shape)\n",
    "vsm_topics\n",
    "# Ukuran kolom = #Topics ==> Dimension Reduction\n",
    "# Mengapa tidak dibagi Train & Test???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:16:01.024186Z",
     "start_time": "2022-05-30T11:16:01.009186Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"Seandainya\" diasumsikan 1 dokumen hanya 1 topic dengan nilai skor topic terbesar\n",
    "doc_topic =  [a.argmax()+1 for a in tqdm(vsm_topics)] # topic of docs\n",
    "doc_topic[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:16:01.119186Z",
     "start_time": "2022-05-30T11:16:01.025186Z"
    }
   },
   "outputs": [],
   "source": [
    "# mari kita plot\n",
    "plot = sns.countplot(doc_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:16:01.135187Z",
     "start_time": "2022-05-30T11:16:01.121186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mari kita coba maknai masing-masing topic ini\n",
    "Top_Words = 25\n",
    "print('Printing top {0} Topics, with top {1} Words:'.format(n_topics, Top_Words))\n",
    "tau.print_Topics(lda, tf_terms, n_topics, Top_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:16:04.332507Z",
     "start_time": "2022-05-30T11:16:01.136186Z"
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# Mari kita Plot, supaya lebih jelas\n",
    "# Catatan, bergantung dari laptop yang digunakan, image terkadang cukup lama untuk muncul.\n",
    "import pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagaimana jika kita ingin menggunakan semi-supervised (guided) LDA?\n",
    "https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagaimana melakukan Post-Tag sebelum Topic Modelling & Bahasa Indonesia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:16:04.348506Z",
     "start_time": "2022-05-30T11:16:04.333507Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sumber Data:\n",
    "# https://github.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia/blob/master/dataset_tweet_sentiment_opini_film.csv\n",
    "\n",
    "\n",
    "file_ = 'data/dataset_tweet_sentiment_opini_film.csv'\n",
    "\n",
    "data = pd.read_csv(file_)\n",
    "\n",
    "col = data.columns\n",
    "print(\"Kolom di data: \", col)\n",
    "data = data[col[-1]].values\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Bahasa Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:16:34.934141Z",
     "start_time": "2022-05-30T11:16:04.349506Z"
    }
   },
   "outputs": [],
   "source": [
    "stopID, _ = tau.LoadStopWords(lang='id')\n",
    "stopID.add('rt'); stopID.add('..')\n",
    "\n",
    "for i,d in tqdm(enumerate(data)):\n",
    "    data[i] = tau.cleanText(d, lan='id', lemma=True, stops=stopID, symbols_remove = True, min_charLen = 2)\n",
    "    \n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:16:36.983331Z",
     "start_time": "2022-05-30T11:16:34.935141Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lang.id import Indonesian\n",
    "from nltk.tag import CRFTagger\n",
    "nlp_id = Indonesian()  # Language Model\n",
    "\n",
    "def NLPfilter(t, filters):\n",
    "    tokens = nlp_id(t)\n",
    "    tokens = [str(k) for k in tokens if len(k)>2]\n",
    "    hasil = ct.tag_sents([tokens])\n",
    "    return [k[0] for k in hasil[0] if k[1] in filters]\n",
    "\n",
    "filters = set(['NN', 'NNP', 'NNS', 'NNPS', 'JJ'])\n",
    "data_postTag = []\n",
    "try:\n",
    "    nlp_id = Indonesian()  # Language Model\n",
    "    ct = CRFTagger()\n",
    "    ct.set_model_file('data/all_indo_man_tag_corpus_model.crf.tagger')\n",
    "    for i, d in tqdm(enumerate(data)):\n",
    "        data_postTag.append(NLPfilter(d,filters))\n",
    "    print(' '.join(data_postTag[0]))\n",
    "    data = [d for d in data_postTag if d]\n",
    "except:\n",
    "    data_postTag = data\n",
    "    data = [d.split() for d in data_postTag if d]\n",
    "    print(\"CRFtagger Failed, try to run it locally (not using Google Colab).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:19:06.835068Z",
     "start_time": "2022-05-30T11:19:06.831082Z"
    }
   },
   "outputs": [],
   "source": [
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi: Bagaimana mendapatkan parameter Optimal Topic Modelling?\n",
    "\n",
    "**Beberapa catatan penting**:\n",
    "1. Hati-hati Struktur Data, untuk melakukan evaluasi Topic Modelling struktur data yang digunakan mirip dengan Word Embedding.\n",
    "2. Kita akan melakukan cross-validasi dan N-Gram\n",
    "3. Ada berbagai metric evaluasi https://radimrehurek.com/gensim/models/coherencemodel.html\n",
    "4. Referensi paper: http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:19:15.164634Z",
     "start_time": "2022-05-30T11:19:15.149633Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_t = Phrases(data, min_count=5)\n",
    "trigram_t = Phrases(bigram_t[data], min_count=5)\n",
    "for idx, d in enumerate(data):\n",
    "    for token in bigram_t[d]:\n",
    "        if '_' in token:# Token is a bigram, add to document.\n",
    "            data[idx].append(token)\n",
    "    for token in trigram_t[d]:\n",
    "        if '_' in token:# Token is a bigram, add to document.\n",
    "            data[idx].append(token)\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "# Remove rare & common tokens\n",
    "dictionary_t = Dictionary(data)\n",
    "dictionary_t.filter_extremes(no_below=2, no_above=0.90)\n",
    "#Create dictionary and corpus required for Topic Modeling\n",
    "corpus_t = [dictionary_t.doc2bow(doc) for doc in data]\n",
    "corpus_t = [t for t in corpus_t if t] # remove empty corpus\n",
    "print('Number of unique tokens: %d' % len(dictionary_t))\n",
    "print('Number of documents: %d' % len(corpus_t))\n",
    "print(corpus_t[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:19:18.512578Z",
     "start_time": "2022-05-30T11:19:18.495900Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "id2word = corpora.Dictionary(data)\n",
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:19:26.239853Z",
     "start_time": "2022-05-30T11:19:26.223168Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "def compute_coherence_values(id2word, corpus, texts, limit=1, start=2, step=1):\n",
    "    coherence_values = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        LDAmodel_ = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "        cm = CoherenceModel(model=LDAmodel_, texts=texts, corpus=corpus, coherence='c_v')\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            coherence_values.append(cm.get_coherence())\n",
    "    return coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caution cell berikut ini membutuhkan waktu yang cukup signifikan untuk selesai, karena selain LDA *computationally expensive* loopingnya juga melakukan Cross-validasi di setiap jumlah topik *k*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:22:43.248385Z",
     "start_time": "2022-05-30T11:20:12.221552Z"
    }
   },
   "outputs": [],
   "source": [
    "start, step, limit = 2, 1, 10 # Ganti dengan berapa banyak Topic yang ingin di hitung/explore\n",
    "coh_t, kCV = [], 5 # hati-hati sangat LAMBAT karena cross validasi pada metode yang memang tidak efisien (LDA)\n",
    "\n",
    "print('iterasi ke: ', end = ' ')\n",
    "for i in range(kCV):\n",
    "    print(i+1, end = ', ', flush=True)\n",
    "    c = compute_coherence_values(id2word, corpus_t, data, limit=limit, start=start, step=step)\n",
    "    coh_t.append(c)\n",
    "    \n",
    "coherence_t = np.mean(np.array(coh_t), axis=0)\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:22:43.373378Z",
     "start_time": "2022-05-30T11:22:43.248385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show graph\n",
    "x = range(start, limit, step)\n",
    "plt.figure(figsize=(12,10))\n",
    "for c in coh_t:\n",
    "    plt.plot(x, c, '--', color = 'lawngreen', linewidth = 2)\n",
    "plt.plot(x, coherence_t, '-', color = 'black', linewidth = 5)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referensi Pilihan:\n",
    "\n",
    "* perhitungan Manual Topic Modelling LDA: http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/\n",
    "* http://mimno.infosci.cornell.edu/slides/details.pdf\n",
    "* https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "* http://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf\n",
    "* http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html\n",
    "* Penjelasan intuitif yang baik: https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d \n",
    "* in conjunction dengan interactive program berikut: https://lettier.com/projects/lda-topic-modeling/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:22:43.529626Z",
     "start_time": "2022-05-30T11:22:43.373378Z"
    }
   },
   "outputs": [],
   "source": [
    "# Langsung Aplikasi-nya\n",
    "# Perhatikan NMF bisa menggunakan Float\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = D.copy()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "nmf_model = NMF(n_components = 3, random_state=0)\n",
    "nmf = nmf_model.fit(tfidf)\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "tau.print_Topics(nmf, tfidf_feature_names, n_topics, Top_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:22:43.545265Z",
     "start_time": "2022-05-30T11:22:43.529626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sama seperti LDA kita bisa melihat distribusi topic setiap dokumen\n",
    "vsm_topics = nmf.transform(tfidf)\n",
    "vsm_topics[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-30T11:22:43.607750Z",
     "start_time": "2022-05-30T11:22:43.545265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seandainya diasumsikan 1 dokumen hanya 1 topic dengan nilai skor topic terbesar\n",
    "doc_topic =  [a.argmax()+1 for a in tqdm(vsm_topics)] # topic of docs\n",
    "print('In total there are {0} major topics, distributed as follows'.format(len(set(doc_topic))))\n",
    "sns.countplot(doc_topic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latihan :\n",
    "\n",
    "* Load data data tweet dengan isu berbeda.\n",
    "* Lakukan preprocessing (termasuk lemma) dan pos tag (ambil hanya noun saja)\n",
    "* Bandingkan hasil topic dari LDA, dan NMF dari data tersebut.\n",
    "* Apakah hasilnya sudah baik?\n",
    "* Buat visualisasi pyLDAvis-nya dan analisa lebih lanjut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Module\n",
    "\n",
    "<hr />"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
