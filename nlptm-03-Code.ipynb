{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><strong>Natural Language Processing and Text Mining (NLPTM)<br> Unstructured Data Analysis (UDA)*</strong></center>\n",
    "    \n",
    "## <center><strong><font color=\"blue\">NLPTM-03: Dasar-Dasar Natural Language Processing (NLP)- Bagian ke-03</font></strong></center>\n",
    "<center><img alt=\"\" src=\"images/SocMed.png\"/> </center>\n",
    "    \n",
    "## <center>(C) Taufik Sutanto - 2020 <br><strong><font color=\"blue\"> tau-data Indonesia ~ https://tau-data.id/uda/</font></strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline Module NLPTM-03/UDA-03:\n",
    "* Pos tag \n",
    "* WordNet dan WSD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Jalankan Cell ini \"HANYA\" jika anda menggunakan Google Colab\n",
    "# Jika di jalankan di komputer local, silahkan lihat NLPTM-02 untuk instalasinya.\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/taudataDDGsna.py\n",
    "    !wget https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/taudataNlpTm.py\n",
    "    !mkdir data\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/contoh.pdf\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/slang.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_id.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/stopwords_en.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/kata_dasar.txt\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-ind-def.tab\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/wn-msa-all.tab\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/ind_SA.csv\n",
    "    !wget -P data/ https://raw.githubusercontent.com/taudata-indonesia/eLearning/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
    "\n",
    "    !pip install spacy python-crfsuite unidecode textblob sastrawi sklearn-pycrfsuite\n",
    "    !pip install unidecode twython beautifulsoup4 tika\n",
    "    !pip install --upgrade tweepy\n",
    "    !python -m spacy download xx_core_web_sm\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    nltk.download('popular')\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Stemming dan Lemma</font>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan konteks kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung konteks.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('currently', 'RB'), ('learning', 'VBG'), ('NLP', 'NNP'), ('in', 'IN'), ('English', 'NNP'), (',', ','), ('but', 'CC'), ('if', 'IN'), ('possible', 'JJ'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('know', 'VB'), ('NLP', 'NNP'), ('in', 'IN'), ('Indonesian', 'JJ'), ('language', 'NN'), ('too', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "# POS tags in NLTK (English)\n",
    "import nltk\n",
    "T = 'I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too'\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(T)\n",
    "print(nltk.pos_tag(nltk_tokens))\n",
    "# Tidak lagi hanya 9 macam tags seperti yang dibahas ahli bahasa (linguist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRP'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = nltk.pos_tag(nltk_tokens)\n",
    "Z[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['possible', 'Indonesian', 'language']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering berdasarkan \"pos\"\n",
    "pos = set(['NN','JJ'])\n",
    "hasil = []\n",
    "for pt in Z:\n",
    "    if pt[1] in pos:\n",
    "        hasil.append(pt[0])\n",
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['possible', 'Indonesian', 'language']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pythonista! .... Flat.\n",
    "[t[0] for t in Z if t[1] in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I_PRP', 'am_VBP', 'currently_RB', 'learning_VBG', 'NLP_NNP', 'in_IN', 'English_NNP', ',_,', 'but_CC', 'if_IN', 'possible_JJ', 'I_PRP', 'want_VBP', 'to_TO', 'know_VB', 'NLP_NNP', 'in_IN', 'Indonesian_JJ', 'language_NN', 'too_RB']\n"
     ]
    }
   ],
   "source": [
    "# Penggunaan di text mining jika suatu kata ingin dibedakan jika berbeda pos\n",
    "hasil = []\n",
    "for pt in Z:\n",
    "    hasil.append(pt[0]+'_'+pt[1])\n",
    "print(hasil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_PRP, am_VBP, currently_RB, learning_VBG, NLP_NNP, in_IN, English_NNP, but_CC, if_IN, possible_JJ, I_PRP, want_VBP, to_TO, know_VB, NLP_NNP, in_IN, Indonesian_JJ, language_NN, too_RB, "
     ]
    }
   ],
   "source": [
    "# Pos tags in TextBlob (English)\n",
    "from textblob import TextBlob\n",
    "\n",
    "for word, pos in TextBlob(T).tags:\n",
    "    print(\"{}_{}\".format(word, pos), end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_PRP, am_VBP, currently_RB, learning_VBG, NLP_NNP, in_IN, English_NNP, ,_,, but_CC, if_IN, possible_JJ, I_PRP, want_VBP, to_TO, know_VB, NLP_NNP, in_IN, Indonesian_JJ, language_NN, too_RB, "
     ]
    }
   ],
   "source": [
    "# Pos Tag Spacy English\n",
    "#from spacy.lang.en import English\n",
    "#nlp_en = English()\n",
    "import spacy\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "tokens = nlp_en(T)\n",
    "for tok in tokens:\n",
    "    print(\"{}_{}\".format(tok, tok.tag_), end = ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conjunction, coordinating'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spacy tidak perlu tabel pos tag ...  bisa pakai perintah \"explain\"\n",
    "spacy.explain('CC')\n",
    "# Daftar Lengkap: https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saat \n",
      "bepergian \n",
      "ke \n",
      "Jogjakarta \n",
      "jangan \n",
      "lupa \n",
      "membeli \n",
      "oleh-oleh \n"
     ]
    }
   ],
   "source": [
    "# Pos Tags in Spacy - Bahasa Indonesia?\n",
    "from spacy.lang.id import Indonesian\n",
    "nlp_id = Indonesian()  # Language Model\n",
    "\n",
    "Ti = \"Saat bepergian ke Jogjakarta jangan lupa membeli oleh-oleh\"\n",
    "Teks = nlp_id(Ti)\n",
    "for token in Teks:\n",
    "    print(token.lemma_, token.tag_)\n",
    "# Fungsi pos-tags belum tersedia untuk bahasa indonesia .. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saat',\n",
       " 'bepergian',\n",
       " 'ke',\n",
       " 'Jogjakarta',\n",
       " 'jangan',\n",
       " 'lupa',\n",
       " 'membeli',\n",
       " 'oleh-oleh']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ti.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Saat', 'NN'), ('bepergian', 'NN'), ('ke', 'IN'), ('Jogjakarta', 'NNP'), ('jangan', 'NEG'), ('lupa', 'VB'), ('membeli', 'VB'), ('oleh-oleh', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "# Pos Tag Bahasa Indonesia lewat NLTK\n",
    "# https://yudiwbs.wordpress.com/2018/02/20/pos-tagger-bahasa-indonesia-dengan-pytho/\n",
    "from nltk.tag import CRFTagger\n",
    "ct = CRFTagger()\n",
    "ct.set_model_file('data/all_indo_man_tag_corpus_model.crf.tagger')\n",
    "\n",
    "hasil = ct.tag_sents([Ti.split()]) # Hati-hati ... Stuktur Data ini adalah \"List-of-Lists\"!!!...\n",
    "hasil = hasil[0]\n",
    "print(hasil)\n",
    "# Hati-hati dengan struktur data inputnya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><u><strong>Beberapa istilah penting:</strong></u></p>\n",
    "\n",
    "<ol>\n",
    "\t<li><strong>Semantics </strong>: Makna/arti ekspresi manusia melalui bahasa<br />\n",
    "\tNLP hanya bisa berusaha mendapatkan makna <strong>denotasi </strong>dari suatu bahasa.<br />\n",
    "\tKata-kata (<strong>Lexical </strong>terms) yang di olah hanyalah kata baku (dictionary words)</li>\n",
    "\t<li>Dalam Semantic terdapat beberapa konsep:</li>\n",
    "</ol>\n",
    "\n",
    "<ul>\n",
    "\t<li><strong>Polysemy </strong>: Kata dengan makna &gt;1. Contoh: Apple&nbsp;(buah dan merk)</li>\n",
    "\t<li><strong>Synonymy </strong>: Persamaan kata</li>\n",
    "\t<li><strong>Antonymy </strong>: Lawan kata</li>\n",
    "\t<li><strong>Hyponymy </strong>: Khusus ==&gt; Umum, contoh : &quot;merah&quot; adalah hyponym dari &quot;warna&quot;<br />\n",
    "\tHubungan ini di dapat dari taxonomy (hierarchical structure) kata .</li>\n",
    "\t<li><strong>Hypernym </strong>: Umum ==&gt; Khusus, contoh: &quot;warna&quot; adalah hypernymnya &quot;merah&quot;</li>\n",
    "\t<li><strong>Idiom </strong>: istilah yang makna berbeda secara signifikan dibandingkan kata penyusunnya.<br />\n",
    "\tContoh: buah tangan, meja hijau, dll.</li>\n",
    "\t<li><strong>Meronym </strong>: Hubungan sematik karena bagian dari sesuatu. Misal jari meronym tangan dan roda meronym mobil.</li>\n",
    "\t<li><strong>Holonym </strong>: kebalikan meronym, contoh: tangan holonym Jari, dan mobil holonym roda.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bank.n.01', 'depository_financial_institution.n.01', 'bank.n.03', 'bank.n.04', 'bank.n.05', 'bank.n.06', 'bank.n.07', 'savings_bank.n.02', 'bank.n.09', 'bank.n.10', 'bank.v.01', 'bank.v.02', 'bank.v.03', 'bank.v.04', 'bank.v.05', 'deposit.v.02', 'bank.v.07', 'trust.v.01']\n"
     ]
    }
   ],
   "source": [
    "#WordNet Interface - Synonym sets\n",
    "from nltk.corpus import wordnet as wn # Load English WordNet\n",
    "\n",
    "print([sinonim.name() for sinonim in wn.synsets(\"bank\")])\n",
    "\n",
    "# Hasilnya adalah sebuah triplets (<lemma>.<pos>.<number>)\n",
    "# <Lemma> : word’s morphological stem (kata baku/dasar)\n",
    "# <pos ~ part-of-speech> : Atribut kata n-NOUN, v-VERB, a-ADJECTIVE, s-ADJECTIVE SATELLITE, r-ADVERB \n",
    "# <number> : Sense number, index integer (bilangan bulat) \"terurut dari penggunaan yang paling populer\"\n",
    "# http://www.nltk.org/api/nltk.corpus.reader.html?highlight=wordnet#nltk.corpus.reader.wordnet.Lemma.synset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run.n.01', 'test.n.05', 'footrace.n.01', 'streak.n.01', 'run.n.05']\n",
      "['run.v.01', 'scat.v.01', 'run.v.03', 'operate.v.01', 'run.v.05']\n"
     ]
    }
   ],
   "source": [
    "# Sinonim bergantung pada Jenis kata # NOUN, VERB, ADJ, ADV\n",
    "print( [s.name() for s in wn.synsets(\"run\", pos=wn.NOUN)[:5]] ) \n",
    "print( [s.name() for s in wn.synsets(\"run\", pos=wn.VERB)[:5]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "the 9th letter of the Roman alphabet\n"
     ]
    }
   ],
   "source": [
    "print( [s.name() for s in wn.synsets(\"you\", pos=wn.NOUN)[:5]] ) \n",
    "print(wn.synset('i.n.03').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a container (usually with a slot in the top) for keeping money at home\n",
      "a score in baseball made by a runner touching all four bases safely\n",
      "move fast by using one's feet, with one foot off the ground at any given time\n"
     ]
    }
   ],
   "source": [
    "# Definisi suatu kata (membutuhkan parameter input yang spesifik)\n",
    "print(wn.synset('bank.n.08').definition())\n",
    "print(wn.synset('run.n.01').definition())\n",
    "print(wn.synset('run.v.01').definition())\n",
    "# Hati-hati \"synset\" not \"synsets\" \n",
    "# butuh attribut POS (i.e. n,v,a,r, atau s) dan index (00,01,02,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the coin bank was empty']\n",
      "['the Yankees scored 3 runs in the bottom of the 9th', 'their first tally came in the 3rd inning']\n",
      "[\"Don't run--you'll be out of breath\", 'The children ran to the store']\n"
     ]
    }
   ],
   "source": [
    "# Contoh kalimat untuk suatu kata (butuh triplets sebagai input)\n",
    "print(wn.synset('bank.n.08').examples())\n",
    "print(wn.synset('run.n.01').examples())\n",
    "print(wn.synset('run.v.01').examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aplikasi WordNet : Jarak antar Kata\n",
    "<p>WordNet memiliki beberapa&nbsp;<strong>Words Similarities</strong>, contoh (<strong>thesaurus-based</strong>):</p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Path similarity</li>\n",
    "\t<li>Leacock-Chodorow Similarity</li>\n",
    "\t<li>Wu-Palmer Similarity</li>\n",
    "</ul>\n",
    "\n",
    "<ol>\n",
    "\t<li>Thesaurus based similarity menggunakan hirarki (tingkatan) hypernym/hyponym (is-a or subsumption). Dalam hal ini di struktur WordNet.</li>\n",
    "\t<li>Hanya noun-noun (thesaurus-based) similarity bisa dilakukan di wordnet, karena noun dan verb berada di 2 hirarki yg berbeda</li>\n",
    "\t<li>&nbsp;2 kata similar jika &quot;hampir sinonim&quot; atau setidaknya dapat tergantikan dalam konteks yang sama. <strong>Word Relatedness</strong>&nbsp;(WR) != WS, Contoh <em>Love </em>dan <em>hate </em>memiliki relatedness yang besar (sebagai bentuk perasaan), tapi similarity yang kecil.</li>\n",
    "    <li>Similarity != Jarak</li>\n",
    "\t<li>WS berguna untuk aplikasi yang membutuhkan semantic kata, misal sistem QA, IR, Summariztion, &amp; machine translation.</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Distributional based</strong> simmilarities (tidak dibahas):</p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Resnik Similarity</li>\n",
    "\t<li>Jiang-Conrath Similarity</li>\n",
    "\t<li>Lin Similarity</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Reference</strong>:&nbsp;Dan Jurafsky and James H. Martin&#39;s ubiquitous&nbsp;<a href=\"http://rads.stackoverflow.com/amzn/click/0131873210\" target=\"_blank\">Speech and Language Processing 2nd Edition</a>. Halaman: 652-667 di bab 20 (Computational Lexical Semantics).&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.14285714285714285 0.2 0.3333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07692307692307693"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path Similarity : shortest path\n",
    "# Path similarity menghitung jumlah edges minimal dari suatu word sense ke word sense lainnya, \n",
    "# Menggunakan struktur data hirarki (graph) seperti WordNet\n",
    "man = wn.synset(\"man.n.01\")\n",
    "boy = wn.synset(\"boy.n.01\")\n",
    "woman = wn.synset(\"woman.n.01\")\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "dime = wn.synset('dime.n.00')\n",
    "\n",
    "a = man.path_similarity(man) \n",
    "b = man.path_similarity(dog)\n",
    "c = dog.path_similarity(cat)\n",
    "d = man.path_similarity(boy)\n",
    "print(a,b,c,d)\n",
    "man.path_similarity(dime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.538973871058276 1.6916760106710724 1.55814461804655\n"
     ]
    }
   ],
   "source": [
    "# Leacock-Chodorow Similarity (Leacock and Chodorow 1998) : -log(shortest_path_w1_w2)\n",
    "# Warning, slow!. \n",
    "man = wn.synset(\"man.n.01\")\n",
    "woman = wn.synset(\"woman.n.01\")\n",
    "dog = wn.synset('dog.n.01')\n",
    "tree = wn.synset('tree.n.01')\n",
    "\n",
    "a = man.lch_similarity(woman) \n",
    "b = man.lch_similarity(dog)\n",
    "c = dog.lch_similarity(tree)\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666 0.15384615384615385 0.15384615384615385 0.8571428571428571 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Word Similarity by semantics: wupsimilarity\n",
    "# need try-catch\n",
    "man = wn.synset(\"man.n.01\")\n",
    "woman = wn.synset(\"woman.n.01\")\n",
    "boy = wn.synset('boy.n.01')\n",
    "hate = wn.synset('hate.n.01')\n",
    "love = wn.synset('love.n.01')\n",
    "a = man.wup_similarity(boy) \n",
    "b = man.wup_similarity(hate)\n",
    "c = boy.wup_similarity(love)  \n",
    "d = hate.wup_similarity(love)\n",
    "w = man.wup_similarity(woman)\n",
    "print(a,b,c,d,w)\n",
    "# Measured by semantics, d close to 1 (because they are antonyms)\n",
    "# Explanation summary of this distance: https://linguistics.stackexchange.com/a/9164\n",
    "# More theoritical Reference: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3864022/\n",
    "# We will discuss word2vec latter as an alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('ambitious.a.01.ambisius')]\n",
      "[Synset('cocky.s.01'), Synset('daredevil.s.01'), Synset('cavalier.s.01'), Synset('grandiloquent.s.02'), Synset('bootless.s.01'), Synset('proud.a.01'), Synset('arrogant.s.01'), Synset('bigheaded.s.01'), Synset('disdainful.s.02'), Synset('conceited.s.01'), Synset('file_allocation_table.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# WordNet Indonesia?\n",
    "print(wn.lemmas(\"ambisius\", lang='ind'))\n",
    "print(wn.synsets('sombong', lang='ind'))\n",
    "# This is not the expected behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambisius {'def': ['mempunyai cita-cita tinggi'], 'pos': ['a']} \n",
      " ambigu {'def': ['ambigu (khususnya dalam hal negatif)'], 'pos': ['a']}\n"
     ]
    }
   ],
   "source": [
    "# Load secara manual [sangat terbatas & kurang akurat, namun bisa dikembangkan]\n",
    "import taudataNlpTm as tau\n",
    "\n",
    "wn_id = tau.WordNet_id()\n",
    "w1 = 'ambisius'\n",
    "w2 = 'ambigu'\n",
    "print(w1, wn_id[w1], '\\n', w2, wn_id[w2])\n",
    "# Masih load sebagai dictionary belum sebagai class/object\n",
    "# WordNet seluruh Bahasa: http://compling.hss.ntu.edu.sg/omw/  - MIT license\n",
    "# https://stackoverflow.com/questions/31478152/how-to-use-the-language-option-in-synsets-nltk-if-you-load-a-wordnet-manually\n",
    "# http://nullege.com/codes/search/nltk.corpus.reader.wordnet.WordNetCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('plant.n.01'),\n",
       " Synset('plant.n.02'),\n",
       " Synset('plant.n.03'),\n",
       " Synset('plant.n.04'),\n",
       " Synset('plant.v.01')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WordNet from textBlob\n",
    "from textblob import Word\n",
    "word = Word(\"plant\")\n",
    "word.synsets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buildings for carrying on industrial labor',\n",
       " '(botany) a living organism lacking the power of locomotion',\n",
       " 'an actor situated in the audience whose acting is rehearsed but seems spontaneous to the audience',\n",
       " 'something planted secretly for discovery by another',\n",
       " 'put or set (seeds, seedlings, or plants) into the ground']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# berbagai definisi \"plant\"\n",
    "word.definitions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plant', 'flora', 'plant_life']\n"
     ]
    }
   ],
   "source": [
    "# related Lemma\n",
    "plant = word.synsets[1]\n",
    "print(plant.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('organism.n.01')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hypernyms : Umum ==> khusus\n",
    "# e.g. contoh organisme adalah plant\n",
    "plant.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('acrogen.n.01'),\n",
       " Synset('air_plant.n.01'),\n",
       " Synset('annual.n.01'),\n",
       " Synset('apomict.n.01'),\n",
       " Synset('aquatic.n.01')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyponyms : khusus ==> umum\n",
    "# e.g. contoh plant adalah aquatic (plant)\n",
    "plant.hyponyms()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('plantae.n.01')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hubungan semantic bagian dari\n",
    "plant.member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('hood.n.02'), Synset('plant_part.n.01')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kebalikan Holonym\n",
    "plant.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.3333333333333333 0.1111111111111111 0.06666666666666667 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Contoh similarity menggunakan TextBlob\n",
    "from textblob.wordnet import Synset\n",
    "octopus = Synset(\"octopus.n.02\")\n",
    "nautilus = Synset('paper_nautilus.n.01')\n",
    "shrimp = Synset('shrimp.n.03')\n",
    "pearl = Synset('pearl.n.01')\n",
    "hate = wn.synset('hate.n.01')\n",
    "love = wn.synset('love.n.01')\n",
    "\n",
    "a = octopus.path_similarity(octopus)  # 1.0\n",
    "b = octopus.path_similarity(nautilus)  # 0.33\n",
    "c = octopus.path_similarity(shrimp)  # 0.11\n",
    "d = octopus.path_similarity(pearl)  \n",
    "e = hate.path_similarity(love)\n",
    "print(a,b,c,d,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Word Sense Disambiguation</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Bertujuan untuk mendapatkan word sense (makna kata) yang tepat &quot;sesuai dengan konteksnya&quot;.&nbsp;</li>\n",
    "\t<li>Contoh aplikasinya penterjemah (machine translation), named entity recognition, Question-Answering system, IR, klasifikasi text, dll.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please UH, book VB, me PRP, a DT, ticket NN, to IN, Jogjakarta NNP, \n",
      "I PRP, am VBP, going VBG, to TO, read VB, this DT, book NN, in IN, the DT, flight NN, "
     ]
    }
   ],
   "source": [
    "# Word sense disambiguation : \"book\" - buku dan memesan tiket\n",
    "T1 = 'Please book me a ticket to Jogjakarta'\n",
    "T2 = 'I am going to read this book in the flight'\n",
    "\n",
    "for token in nlp_en(T1):\n",
    "    print(token,token.tag_, end =', ')\n",
    "print()\n",
    "\n",
    "for token in nlp_en(T2):\n",
    "    print(token,token.tag_, end =', ')\n",
    "# perbedaan yang jelas antara VB dan NN pada kata \"book\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: I went to the bank to deposit my money\n",
      "Sense: depository_financial_institution.n.01\n",
      "Definition: a financial institution that accepts deposits and channels the money into lending activities \n",
      "\n",
      "Context: The river bank was full of dead fishes\n",
      "Sense: bank.n.01\n",
      "Definition: sloping land (especially the slope beside a body of water) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A more proper way = Lesk Algorithm\n",
    "# https://en.wikipedia.org/wiki/Lesk_algorithm\n",
    "# Minor Modified from from https://stackoverflow.com/questions/20896278/word-sense-disambiguation-algorithm-in-python\n",
    "\n",
    "bank_1 = 'I went to the bank to deposit my money'\n",
    "bank_2 = 'The river bank was full of dead fishes'\n",
    "\n",
    "# lesk_wsd(sentence, ambiguous_word, pos=None, stem=True, hyperhypo=True)\n",
    "print(\"Context:\", bank_1)\n",
    "answer = tau.lesk_wsd(bank_1,'bank')\n",
    "print(\"Sense:\", answer)\n",
    "print(\"Definition:\",wn.synset(answer).definition(),'\\n')\n",
    "\n",
    "print(\"Context:\", bank_2)\n",
    "answer = tau.lesk_wsd(bank_2,'bank')\n",
    "print(\"Sense:\", answer)\n",
    "print(\"Definition:\",wn.synset(answer).definition(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Please book me a ticket to Jogjakarta\n",
      "Sense: book.n.11\n",
      "Definition: a number of sheets (ticket or stamps etc.) bound together on one edge \n",
      "\n",
      "Context: I am going to read this book in the flight\n",
      "Sense: record.n.05\n",
      "Definition: a compilation of the known facts regarding something or someone \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Context:\", T1)\n",
    "answer = tau.lesk_wsd(T1,'book')\n",
    "print(\"Sense:\", answer)\n",
    "print(\"Definition:\",wn.synset(answer).definition(),'\\n')\n",
    "\n",
    "print(\"Context:\", T2)\n",
    "answer = tau.lesk_wsd(bank_2,'book')\n",
    "print(\"Sense:\", answer)\n",
    "print(\"Definition:\",wn.synset(answer).definition(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More alternatives in Python for WSD\n",
    "http://meta-guide.com/software-meta-guide/100-best-github-word-sense-disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>End of Module</h1>\n",
    "<hr />\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
